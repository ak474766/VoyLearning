<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pattern Recognition Principles - Lecture 2: Basic Probability Theory & Statistical Properties</title>
    <style>
        body {
            font-family: 'Georgia', serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
            color: #2c3e50;
        }
        
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #1a365d;
            text-align: center;
            border-bottom: 3px solid #3182ce;
            padding-bottom: 20px;
            margin-bottom: 30px;
            font-size: 2.2em;
        }
        
        h2 {
            color: #2b6cb0;
            border-left: 5px solid #3182ce;
            padding-left: 15px;
            margin-top: 35px;
            margin-bottom: 20px;
            font-size: 1.5em;
        }
        
        h3 {
            color: #2c5aa0;
            margin-top: 25px;
            margin-bottom: 15px;
            font-size: 1.3em;
        }
        
        .toc {
            background: #e6f3ff;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        
        .toc h2 {
            margin-top: 0;
            color: #1a365d;
        }
        
        .toc ul {
            list-style-type: none;
            padding-left: 0;
        }
        
        .toc li {
            margin: 8px 0;
        }
        
        .toc a {
            text-decoration: none;
            color: #2b6cb0;
            padding: 5px 10px;
            border-radius: 4px;
            transition: background-color 0.3s;
        }
        
        .toc a:hover {
            background-color: #3182ce;
            color: white;
        }
        
        .key-term {
            background-color: #fff3cd;
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: bold;
            color: #856404;
        }
        
        .formula {
            background: #f8f9fa;
            padding: 15px;
            border-left: 4px solid #17a2b8;
            margin: 20px 0;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
        }
        
        .example {
            background: #e8f5e8;
            padding: 15px;
            border-left: 4px solid #28a745;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .professor-note {
            background: #fff3cd;
            padding: 15px;
            border-left: 4px solid #ffc107;
            margin: 20px 0;
            border-radius: 4px;
            font-style: italic;
        }
        
        .hinglish-summary {
            background: #f0f8ff;
            padding: 20px;
            border: 2px solid #87ceeb;
            border-radius: 8px;
            margin: 25px 0;
            font-style: italic;
        }
        
        .hinglish-summary h4 {
            color: #4682b4;
            margin-top: 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
        }
        
        th, td {
            border: 1px solid #dee2e6;
            padding: 12px;
            text-align: left;
        }
        
        th {
            background-color: #e9ecef;
            font-weight: bold;
            color: #495057;
        }
        
        .practice-questions {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 25px 0;
            border-left: 4px solid #6f42c1;
        }
        
        .key-takeaways {
            background: #e6fffa;
            padding: 20px;
            border-radius: 8px;
            margin: 25px 0;
            border-left: 4px solid #38b2ac;
        }
        
        .diagram-placeholder {
            background: #f1f3f4;
            border: 2px dashed #9aa0a6;
            padding: 40px;
            text-align: center;
            margin: 20px 0;
            border-radius: 8px;
            color: #5f6368;
            font-style: italic;
        }
        
        .mind-map {
            background: #fafafa;
            padding: 30px;
            border-radius: 10px;
            margin: 30px 0;
            border: 2px solid #e0e0e0;
        }
        
        .mind-map-node {
            display: inline-block;
            background: #e3f2fd;
            padding: 10px 15px;
            margin: 5px;
            border-radius: 20px;
            border: 2px solid #1976d2;
        }
        
        .mind-map-subnode {
            display: inline-block;
            background: #f3e5f5;
            padding: 8px 12px;
            margin: 3px;
            border-radius: 15px;
            border: 1px solid #7b1fa2;
            font-size: 0.9em;
        }
        
        .math {
            font-family: 'Times New Roman', serif;
            font-style: italic;
            font-size: 1.1em;
        }
        
        ol, ul {
            padding-left: 25px;
        }
        
        li {
            margin: 8px 0;
        }
        
        .answer {
            color: #28a745;
            font-weight: bold;
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Pattern Recognition Principles<br>
        <small style="font-size: 0.6em; color: #666;">BS./BSc. in Applied AI and Data Science</small><br>
        <small style="font-size: 0.5em; color: #888;">Lecture 2: Basic Probability Theory & Statistical Properties of Data</small></h1>
        
        <div class="toc">
            <h2>üìö Table of Contents</h2>
            <ul>
                <li><a href="#intro">1. Introduction</a></li>
                <li><a href="#experiments">2. Experiments and Sample Space</a></li>
                <li><a href="#probability">3. Basic Probability Theory</a></li>
                <li><a href="#random-variables">4. Random Variables</a></li>
                <li><a href="#pmf-pdf">5. Probability Mass Function (PMF) and Probability Density Function (PDF)</a></li>
                <li><a href="#cdf">6. Cumulative Distribution Function (CDF)</a></li>
                <li><a href="#statistical-properties">7. Statistical Properties of Data</a></li>
                <li><a href="#mean">8. Mean (Expected Value)</a></li>
                <li><a href="#variance">9. Variance</a></li>
                <li><a href="#covariance">10. Covariance Matrix</a></li>
                <li><a href="#correlation">11. Correlation Coefficient</a></li>
                <li><a href="#mind-map">12. Comprehensive Mind Map</a></li>
            </ul>
        </div>

        <section id="intro">
            <h2>1. Introduction</h2>
            <p>In this second lecture on <span class="key-term">Pattern Recognition Principles</span>, we focus on two fundamental concepts that form the backbone of pattern recognition systems:</p>
            <ol>
                <li><strong>Basic Probability Theory</strong> - Essential mathematical foundation for understanding uncertainty in pattern recognition</li>
                <li><strong>Statistical Properties of Data</strong> - How to analyze and understand data patterns</li>
            </ol>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "In pattern recognition, we often have to perform experiments and deal with data that contains random variables and their measurements. Understanding these concepts is crucial for any task we take in pattern recognition."
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary</h4>
                <p>Aaj hum probability theory aur data ke statistical properties ke bare mein seekhenge. Ye dono cheezein pattern recognition mein bahut important hain kyunki hume hamesha uncertain data ke saath kaam karna padta hai. Jab bhi hum koi measurement karte hain ya experiment perform karte hain, tab ye concepts kaam aate hain.</p>
            </div>
        </section>

        <section id="experiments">
            <h2>2. Experiments and Sample Space</h2>
            
            <h3>2.1 What is an Experiment?</h3>
            <div class="formula">
                <strong>Definition:</strong> An experiment or trial is any procedure that can be repeated infinite times and has a well-defined set of outcomes.
            </div>
            
            <p>In pattern recognition, <span class="key-term">experiments</span> are fundamental operations where we perform some task and observe outcomes. The key characteristics are:</p>
            <ul>
                <li><strong>Repeatability:</strong> Can be performed infinite times (theoretically)</li>
                <li><strong>Well-defined outcomes:</strong> Results must be clearly measurable</li>
            </ul>
            
            <h3>2.2 Examples of Experiments</h3>
            
            <div class="example">
                <h4>Example 1: Apple vs Orange Classification</h4>
                <p><strong>Experiment 1:</strong> Choose a fruit randomly</p>
                <p><strong>Outcomes:</strong> Fruit‚ÇÅ, Fruit‚ÇÇ, Fruit‚ÇÉ, ... (where each could be apple or orange)</p>
                
                <p><strong>Experiment 2:</strong> Measure the weight of chosen fruit</p>
                <p><strong>Outcomes:</strong> 70g, 75g, 80g, 90g, 100g, ... (weights in grams)</p>
            </div>
            
            <div class="example">
                <h4>Example 2: Digit Classification</h4>
                <p><strong>Experiment:</strong> Analyze a handwritten digit as a grid</p>
                <p><strong>Process:</strong> Treat digit as m√ón grid, measure if each cell contains black pixel or white pixel</p>
                <p><strong>Outcomes:</strong> For each cell: 1 (black pixel) or 0 (white pixel)</p>
                <p><strong>Total possibilities:</strong> 2^(m√ón) combinations</p>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "If there are m√ón cells, you have 2 to the power mn possibilities, either 0 or 1 in each of these mn possible cells. This is a classic example of how we convert visual patterns into measurable data."
            </div>
            
            <h3>2.3 Sample Space (Œ©)</h3>
            <div class="formula">
                <strong>Sample Space (Œ©):</strong> The set of all possible outcomes of an experiment
                <br><br>
                <strong>Properties:</strong>
                <br>‚Ä¢ Elements are mutually exclusive (only one outcome occurs at a time)
                <br>‚Ä¢ Elements are collectively exhaustive (all possibilities covered)
            </div>
            
            <div class="example">
                <h4>Sample Space Examples</h4>
                <table>
                    <tr>
                        <th>Experiment</th>
                        <th>Sample Space (Œ©)</th>
                        <th>Size</th>
                    </tr>
                    <tr>
                        <td>Tossing one coin</td>
                        <td>Œ© = {Head, Tail}</td>
                        <td>2</td>
                    </tr>
                    <tr>
                        <td>Tossing two coins</td>
                        <td>Œ© = {HH, HT, TH, TT}</td>
                        <td>4</td>
                    </tr>
                    <tr>
                        <td>Rolling a dice</td>
                        <td>Œ© = {1, 2, 3, 4, 5, 6}</td>
                        <td>6</td>
                    </tr>
                    <tr>
                        <td>Choose fruit</td>
                        <td>Œ© = {Apple‚ÇÅ, Apple‚ÇÇ, ..., Orange‚ÇÅ, Orange‚ÇÇ, ...}</td>
                        <td>Variable</td>
                    </tr>
                </table>
            </div>
            
            <h3>2.4 What is an Event?</h3>
            <div class="formula">
                <strong>Event:</strong> A set of outcomes of an experiment. This set is a subset of the sample space.
                <br><br>
                Event A ‚äÜ Œ©
            </div>
            
            <div class="example">
                <h4>Event Examples (Throwing Two Dice)</h4>
                <p><strong>Sample Space:</strong> 36 possibilities (1,1), (1,2), ..., (6,6)</p>
                <p><strong>Event A:</strong> Sum of numbers is even</p>
                <p><strong>Event B:</strong> Sum of numbers is greater than 10</p>
                <p><strong>Possible Events:</strong> 2¬≥‚Å∂ different events (power set of sample space)</p>
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary</h4>
                <p>Experiment matlab koi bhi kaam jo hum baar baar kar sakte hain aur jiska result well-defined ho. Sample space mein saare possible outcomes hote hain, aur event sample space ka koi subset hota hai. Jaise coin toss mein sample space {H,T} hai aur "getting head" ek event hai.</p>
            </div>
            
            <div class="practice-questions">
                <h4>ü§î Practice Questions</h4>
                <ol>
                    <li>What is the sample space when rolling two dice?</li>
                    <li>In apple vs orange classification, if we measure both weight and color, what type of sample space do we get?</li>
                    <li>How many events are possible for a sample space with 4 outcomes?</li>
                </ol>
                
                <div class="answer">
                    <strong>Answers:</strong>
                    <ol>
                        <li>Œ© = {(1,1), (1,2), ..., (6,6)} with 36 outcomes</li>
                        <li>Two-dimensional sample space with weight and color measurements</li>
                        <li>2‚Å¥ = 16 possible events</li>
                    </ol>
                </div>
            </div>
        </section>

        <section id="probability">
            <h2>3. Basic Probability Theory</h2>
            
            <h3>3.1 What is Probability?</h3>
            <div class="formula">
                <strong>Probability Measure P:</strong> A function P: F ‚Üí [0,1] on sample space that satisfies the axioms of probability
                <br><br>
                P(A) = ? (where A is an event)
            </div>
            
            <h3>3.2 Axioms of Probability</h3>
            <p>For any function P to be a valid <span class="key-term">probability function</span>, it must satisfy three fundamental axioms:</p>
            
            <div class="formula">
                <strong>Axiom 1 - Non-negativity:</strong>
                <br>For all events A: P(A) ‚â• 0
                <br><br>
                <strong>Axiom 2 - Normalization:</strong>
                <br>P(Œ©) = 1
                <br><br>
                <strong>Axiom 3 - Additivity (Sum Rule):</strong>
                <br>If A‚ÇÅ, A‚ÇÇ, ..., A‚Çô are mutually exclusive events:
                <br>P(A‚ÇÅ ‚à™ A‚ÇÇ ‚à™ ... ‚à™ A‚Çô) = P(A‚ÇÅ) + P(A‚ÇÇ) + ... + P(A‚Çô)
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "Mutually exclusive events means only one of them can occur at a time. If A‚ÇÅ is occurring, A‚ÇÇ and others will not occur. This is crucial for understanding the sum rule."
            </div>
            
            <h3>3.3 Mapping Sample Space to Real Numbers</h3>
            <p>In pattern recognition, we often map our <span class="key-term">sample space</span> to real numbers to make mathematical analysis possible.</p>
            
            <div class="diagram-placeholder">
                [Insert diagram: Sample Space Œ© mapping to Real Numbers via Weight and Redness measurements]
            </div>
            
            <div class="example">
                <h4>Mapping Examples</h4>
                <table>
                    <tr>
                        <th>Sample Space Element</th>
                        <th>Weight Mapping</th>
                        <th>Redness Mapping</th>
                    </tr>
                    <tr>
                        <td>Apple‚ÇÅ</td>
                        <td>80 grams</td>
                        <td>0.7</td>
                    </tr>
                    <tr>
                        <td>Apple‚ÇÇ</td>
                        <td>75 grams</td>
                        <td>0.8</td>
                    </tr>
                    <tr>
                        <td>Orange‚ÇÅ</td>
                        <td>95 grams</td>
                        <td>0.3</td>
                    </tr>
                </table>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "Once we have such mapping, we can ask questions like 'What is the probability that an apple has weight less than 75 grams?' This type of question can be answered with such mappings."
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary</h4>
                <p>Probability ek function hai jo 0 se 1 tak ki value deta hai aur teen axioms follow karta hai. Non-negative hona chahiye, total sample space ka probability 1 hona chahiye, aur mutually exclusive events ka probability add hota hai. Hum sample space ko real numbers mein map karte hain taaki mathematical analysis kar sakein.</p>
            </div>
        </section>

        <section id="random-variables">
            <h2>4. Random Variables</h2>
            
            <div class="formula">
                <strong>Random Variable:</strong> A function from a set of possible outcomes (sample space) to the set of real numbers
                <br><br>
                X: Œ© ‚Üí ‚Ñù
            </div>
            
            <p>A <span class="key-term">random variable</span> is essentially a mathematical way to assign numerical values to the outcomes of experiments.</p>
            
            <h3>4.1 Types of Random Variables</h3>
            
            <div class="example">
                <h4>Discrete Random Variables</h4>
                <p>Take discrete (countable) values:</p>
                <ul>
                    <li><strong>Age in years:</strong> 1, 2, 3, ... (integers)</li>
                    <li><strong>Binary variable:</strong> Black/White pixels ‚Üí 1/0</li>
                    <li><strong>Dice outcomes:</strong> 1, 2, 3, 4, 5, 6</li>
                </ul>
            </div>
            
            <div class="example">
                <h4>Continuous Random Variables</h4>
                <p>Take continuous (uncountable) values:</p>
                <ul>
                    <li><strong>Rainfall in Jodhpur:</strong> Can be any decimal value in millimeters</li>
                    <li><strong>Weight measurement:</strong> Can be 75.234 grams, 80.1567 grams, etc.</li>
                    <li><strong>Temperature:</strong> Any real number in the valid range</li>
                </ul>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "The distinction between discrete and continuous random variables is crucial because they require different mathematical treatments - PMF for discrete and PDF for continuous variables."
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary</h4>
                <p>Random variable ek function hai jo sample space ko real numbers mein convert karta hai. Do types hain - discrete (countable values jaise 1,2,3) aur continuous (decimal values jaise 75.23, 80.156). Ye concept pattern recognition mein bahut important hai kyunki hume data ko numbers mein represent karna padta hai.</p>
            </div>
        </section>

        <section id="pmf-pdf">
            <h2>5. Probability Mass Function (PMF) and Probability Density Function (PDF)</h2>
            
            <h3>5.1 Probability Mass Function (PMF)</h3>
            <div class="formula">
                <strong>PMF for Discrete Random Variables:</strong>
                <br>P(X = x) = probability that discrete random variable X equals exactly x
                <br><br>
                <strong>Properties:</strong>
                <br>‚Ä¢ P(X = x) ‚â• 0 for all x
                <br>‚Ä¢ Œ£ P(X = x) = 1 (sum over all possible values of x)
                <br>‚Ä¢ PMF values are never greater than 1
            </div>
            
            <div class="example">
                <h4>PMF Examples</h4>
                <p><strong>Fair Coin:</strong> X = {0(Tail), 1(Head)}</p>
                <p>P(X = 0) = 0.5, P(X = 1) = 0.5</p>
                
                <p><strong>Fair Dice:</strong> X = {1, 2, 3, 4, 5, 6}</p>
                <p>P(X = k) = 1/6 for k = 1, 2, 3, 4, 5, 6</p>
            </div>
            
            <div class="diagram-placeholder">
                [Insert diagram: PMF bar chart showing discrete probabilities for coin toss and dice]
            </div>
            
            <h3>5.2 Probability Density Function (PDF)</h3>
            <div class="formula">
                <strong>PDF for Continuous Random Variables:</strong>
                <br>f(X = x) = likelihood of continuous random variable taking values in a given range
                <br><br>
                <strong>Properties:</strong>
                <br>‚Ä¢ f(X = x) ‚â• 0 for all x
                <br>‚Ä¢ ‚à´‚Çã‚àû^‚àû f(X = x)dx = 1 (area under entire curve = 1)
                <br>‚Ä¢ P(a < X < b) = ‚à´‚Çê·µá f(X = x)dx (probability as area under curve)
                <br>‚Ä¢ f(X = x) can be > 1 (unlike PMF)
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "The key difference is that for continuous variables, we cannot ask 'what is the probability of exactly 10.110 mm rainfall?' That probability is zero. Instead, we ask 'what is the probability of rainfall between 5mm and 10mm?'"
            </div>
            
            <div class="example">
                <h4>PDF Example - Uniform Distribution</h4>
                <p><strong>Uniform distribution between 0 and 0.5:</strong></p>
                <p>f(x) = 2 for 0 ‚â§ x ‚â§ 0.5, and f(x) = 0 elsewhere</p>
                <p><strong>Why height = 2?</strong> Because area = base √ó height = 0.5 √ó 2 = 1 ‚úì</p>
                <p><strong>Note:</strong> f(x) = 2 > 1, which is allowed for PDF but not PMF</p>
            </div>
            
            <div class="diagram-placeholder">
                [Insert diagram: PDF curve showing continuous probability density with area under curve representing probabilities]
            </div>
            
            <h3>5.3 Key Differences: PMF vs PDF</h3>
            <table>
                <tr>
                    <th>Aspect</th>
                    <th>PMF (Discrete)</th>
                    <th>PDF (Continuous)</th>
                </tr>
                <tr>
                    <td>Y-axis represents</td>
                    <td>Actual probability</td>
                    <td>Probability density (likelihood)</td>
                </tr>
                <tr>
                    <td>Maximum value</td>
                    <td>‚â§ 1</td>
                    <td>Can be > 1</td>
                </tr>
                <tr>
                    <td>Probability calculation</td>
                    <td>Direct value P(X = x)</td>
                    <td>Area under curve P(a < X < b)</td>
                </tr>
                <tr>
                    <td>Summation/Integration</td>
                    <td>Œ£ P(X = x) = 1</td>
                    <td>‚à´ f(x)dx = 1</td>
                </tr>
            </table>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary</h4>
                <p>PMF discrete random variables ke liye use hota hai aur actual probability deta hai jo kabhi 1 se zyada nahi ho sakti. PDF continuous variables ke liye hai aur density deta hai jo 1 se zyada ho sakti hai. PMF mein direct value mil jaati hai, PDF mein area calculate karna padta hai probability ke liye.</p>
            </div>
            
            <div class="practice-questions">
                <h4>ü§î Practice Questions</h4>
                <ol>
                    <li>Can a PMF value be 1.5? Why or why not?</li>
                    <li>Can a PDF value be 1.5? Why or why not?</li>
                    <li>What is P(X = 5.0) for a continuous random variable?</li>
                </ol>
                
                <div class="answer">
                    <strong>Answers:</strong>
                    <ol>
                        <li>No, PMF represents actual probability which must be ‚â§ 1</li>
                        <li>Yes, PDF represents density, not probability, so it can exceed 1</li>
                        <li>0 (probability of any exact value for continuous variable is 0)</li>
                    </ol>
                </div>
            </div>
        </section>

        <section id="cdf">
            <h2>6. Cumulative Distribution Function (CDF)</h2>
            
            <div class="formula">
                <strong>CDF Definition:</strong>
                <br>F(x) = P(X ‚â§ x) = Probability that random variable X takes a value less than or equal to x
                <br><br>
                <strong>Properties:</strong>
                <br>‚Ä¢ Non-decreasing function: F(a) ‚â§ F(b) if a ‚â§ b
                <br>‚Ä¢ 0 ‚â§ F(x) ‚â§ 1 for all x
                <br>‚Ä¢ F(-‚àû) = 0, F(+‚àû) = 1
            </div>
            
            <div class="example">
                <h4>CDF Construction from PDF</h4>
                <p>For a uniform distribution PDF with height h between [a, b]:</p>
                <ol>
                    <li><strong>Before a:</strong> F(x) = 0</li>
                    <li><strong>Between a and b:</strong> F(x) increases linearly</li>
                    <li><strong>After b:</strong> F(x) = 1</li>
                </ol>
                <p>The CDF is always a cumulative (non-decreasing) function that represents the accumulated probability up to any point.</p>
            </div>
            
            <div class="diagram-placeholder">
                [Insert diagram: PDF to CDF transformation showing how cumulative probability builds up]
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "CDF is always a non-decreasing function. It will be always increasing when we move from left to right, and this is the cumulative distribution function."
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary</h4>
                <p>CDF matlab cumulative distribution function jo batata hai ki X ki value x se kam ya barabar hone ka probability kya hai. Ye hamesha increasing function hota hai aur 0 se 1 tak ki values leta hai. PDF se CDF banane ke liye hum cumulative sum ya integration karte hain.</p>
            </div>
        </section>

        <section id="statistical-properties">
            <h2>7. Statistical Properties of Data</h2>
            
            <p>In pattern recognition, we deal with data containing <span class="key-term">random variables</span> and their measurements. This data always has statistical properties that are crucial for understanding what patterns the data follows.</p>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "In pattern recognition, we often have to deal with data. For example, when we are doing apple versus orange classification, the data was two-dimensional and used two random variables: weight and redness (color). Understanding statistical properties helps us analyze what pattern the data is following."
            </div>
            
            <p>The key statistical properties we'll study are:</p>
            <ul>
                <li><strong>Mean (Expected Value)</strong> - Central tendency</li>
                <li><strong>Variance</strong> - Spread in individual dimensions</li>
                <li><strong>Covariance Matrix</strong> - Relationships between dimensions</li>
                <li><strong>Correlation Coefficient</strong> - Normalized relationships</li>
            </ul>
        </section>

        <section id="mean">
            <h2>8. Mean (Expected Value)</h2>
            
            <h3>8.1 Mean for Single Variable</h3>
            <div class="formula">
                <strong>Expected Value (Mean):</strong>
                <br>Œº‚Çì = E[X]
                <br><br>
                <strong>For Discrete Variables:</strong>
                <br>E[X] = Œ£ x ¬∑ P(X = x)
                <br><br>
                <strong>For Continuous Variables:</strong>
                <br>E[X] = ‚à´ x ¬∑ f(x) dx
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "This looks different at first glance, but it is not actually different. It is the same as what you understand as mean or average. Let me show you how."
            </div>
            
            <div class="example">
                <h4>Mean Calculation Example</h4>
                <p><strong>Given:</strong> X takes values {1, 2, 3} with equal probability</p>
                <p><strong>Traditional Average:</strong> (1 + 2 + 3)/3 = 6/3 = 2</p>
                <p><strong>Expected Value Method:</strong></p>
                <p>E[X] = 1 √ó (1/3) + 2 √ó (1/3) + 3 √ó (1/3) = 6/3 = 2 ‚úì</p>
                <p><em>Both methods give the same result!</em></p>
            </div>
            
            <h3>8.2 Mean for Multivariate Data</h3>
            <div class="formula">
                <strong>For d-dimensional random vector X = [X‚ÇÅ, X‚ÇÇ, ..., X‚Çê]:</strong>
                <br><br>
                E[X] = [E[X‚ÇÅ], E[X‚ÇÇ], ..., E[X‚Çê]]·µÄ
                <br><br>
                <strong>Result:</strong> Mean becomes a vector (not a scalar)
            </div>
            
            <div class="example">
                <h4>Multivariate Mean Example</h4>
                <p><strong>Apple vs Orange with 2 features:</strong></p>
                <p>X‚ÇÅ = Weight, X‚ÇÇ = Redness</p>
                <p>E[X] = [E[Weight], E[Redness]]·µÄ</p>
                <p>E[X] = [Œº_weight, Œº_redness]·µÄ</p>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "When we have multivariate data, the expected value becomes a vector because we compute the average for each random variable separately - average of weight, average of color, etc."
            </div>
            
            <h3>8.3 Limitations of Mean</h3>
            <p>Mean only tells us <span class="key-term">where the center</span> of the data is, but doesn't provide information about:</p>
            <ul>
                <li>How spread out the data is</li>
                <li>The shape of the distribution</li>
                <li>Relationships between variables</li>
            </ul>
            
            <div class="example">
                <h4>Same Mean, Different Distributions</h4>
                <p><strong>Dataset 1:</strong> X‚ÇÅ = {-1, 0, 1} ‚Üí E[X‚ÇÅ] = 0</p>
                <p><strong>Dataset 2:</strong> X‚ÇÇ = {-100, 0, 100} ‚Üí E[X‚ÇÇ] = 0</p>
                <p><strong>Problem:</strong> Both have same mean but very different spreads!</p>
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary</h4>
                <p>Mean ya expected value data ka center batata hai. Discrete variables ke liye sum karte hain, continuous ke liye integrate karte hain. Multivariate data mein mean ek vector ban jata hai. Lekin sirf mean se data distribution ka complete picture nahi milta, isliye variance bhi chahiye.</p>
            </div>
            
            <div class="practice-questions">
                <h4>ü§î Practice Questions</h4>
                <ol>
                    <li>Calculate E[X] for X = {-100, 0, 100} with equal probabilities</li>
                    <li>Why is mean insufficient to describe a distribution completely?</li>
                    <li>What happens to the dimensionality of mean for d-dimensional data?</li>
                </ol>
                
                <div class="answer">
                    <strong>Answers:</strong>
                    <ol>
                        <li>E[X] = (-100 + 0 + 100)/3 = 0</li>
                        <li>Mean only shows center, not spread or shape of distribution</li>
                        <li>Mean becomes a d-dimensional vector</li>
                    </ol>
                </div>
            </div>
        </section>

        <section id="variance">
            <h2>9. Variance</h2>
            
            <h3>9.1 Understanding Variance</h3>
            <p><span class="key-term">Variance</span> measures how spread out the data points are from the mean. It addresses the limitation of mean by quantifying data dispersion.</p>
            
            <div class="formula">
                <strong>Variance of Random Variable X:</strong>
                <br>œÉ¬≤‚Çì = Var(X) = E[(X - E[X])¬≤]
                <br><br>
                <strong>Alternative form:</strong>
                <br>Var(X) = E[X¬≤] - (E[X])¬≤
            </div>
            
            <div class="example">
                <h4>Variance Calculation Example</h4>
                <p><strong>Given:</strong> X‚ÇÅ = {-1, 0, 1} with equal probabilities</p>
                <p><strong>Step 1:</strong> E[X‚ÇÅ] = 0 (calculated earlier)</p>
                <p><strong>Step 2:</strong> Calculate variance</p>
                <p>Var(X‚ÇÅ) = E[(X‚ÇÅ - 0)¬≤] = E[X‚ÇÅ¬≤]</p>
                <p>= ((-1)¬≤ + 0¬≤ + 1¬≤)/3 = (1 + 0 + 1)/3 = 2/3</p>
                
                <p><strong>Homework:</strong> Calculate variance for X‚ÇÇ = {-100, 0, 100}</p>
                <p><em>You'll see the variance is much higher, indicating greater spread!</em></p>
            </div>
            
            <h3>9.2 Sample vs Population Variance</h3>
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "There's a slight difference when computing variance. We divide by n for population variance, but by (n-1) for sample variance to get proper estimation. Many times in practice, we compute sample variance."
            </div>
            
            <div class="formula">
                <strong>Population Variance:</strong> Divide by n
                <br><strong>Sample Variance:</strong> Divide by (n-1) for unbiased estimation
            </div>
            
            <h3>9.3 Variance for Multivariate Data</h3>
            <p>For d-dimensional data, variance can be computed for each dimension separately:</p>
            
            <div class="formula">
                <strong>Multivariate Variance:</strong>
                <br>Var(X) = [Var(X‚ÇÅ), Var(X‚ÇÇ), ..., Var(X‚Çê)]·µÄ
            </div>
            
            <h3>9.4 Problem with Individual Variances</h3>
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "The problem with variance is it only captures the spread in respective dimensions. But if you want to understand relationships across dimensions, we need to compute covariance matrix."
            </div>
            
            <div class="diagram-placeholder">
                [Insert diagram: Two 2D distributions with same individual variances but different orientations - one positive correlation, one negative correlation]
            </div>
            
            <div class="example">
                <h4>Limitation of Individual Variance</h4>
                <p>Consider two 2D datasets:</p>
                <p><strong>Dataset A:</strong> Points form positive diagonal line</p>
                <p><strong>Dataset B:</strong> Points form negative diagonal line</p>
                <p><strong>Problem:</strong> Both have same œÉ¬≤‚Çì‚ÇÅ and œÉ¬≤‚Çì‚ÇÇ, but very different relationships!</p>
                <p><strong>Solution:</strong> Need covariance matrix to capture cross-dimensional relationships</p>
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary</h4>
                <p>Variance batata hai ki data points mean se kitne spread hain. Zyada variance matlab zyada spread. Multivariate data mein har dimension ka alag variance calculate kar sakte hain, lekin ye cross-dimension relationships nahi batata. Isliye covariance matrix ki zarurat padti hai.</p>
            </div>
        </section>

        <section id="covariance">
            <h2>10. Covariance Matrix</h2>
            
            <h3>10.1 Need for Covariance Matrix</h3>
            <p>Individual variances cannot capture how data is distributed <span class="key-term">across dimensions</span>. The covariance matrix solves this by measuring relationships between all pairs of variables.</p>
            
            <div class="formula">
                <strong>Covariance Matrix for d-dimensional random vector:</strong>
                <br>C = d √ó d matrix where C[i,j] = Cov(X·µ¢, X‚±º)
                <br><br>
                <strong>Covariance between X·µ¢ and X‚±º:</strong>
                <br>Cov(X·µ¢, X‚±º) = E[(X·µ¢ - E[X·µ¢])(X‚±º - E[X‚±º])]
                <br><br>
                <strong>When i = j:</strong> Cov(X·µ¢, X·µ¢) = Var(X·µ¢) = œÉ¬≤·µ¢
            </div>
            
            <h3>10.2 Worked Example: 2D Covariance Matrix</h3>
            <div class="example">
                <h4>Dataset: X = [[1, 4], [2, 3], [3, 7], [4, 9]]</h4>
                
                <p><strong>Step 1:</strong> Calculate means</p>
                <p>E[X‚ÇÅ] = (1 + 2 + 3 + 4)/4 = 2.5</p>
                <p>E[X‚ÇÇ] = (4 + 3 + 7 + 9)/4 = 5.75</p>
                
                <p><strong>Step 2:</strong> Calculate covariance matrix entries</p>
                <p><strong>Cov(1,1) = Var(X‚ÇÅ):</strong></p>
                <p>= [(1-2.5)¬≤ + (2-2.5)¬≤ + (3-2.5)¬≤ + (4-2.5)¬≤]/(n-1)</p>
                <p>= [(-1.5)¬≤ + (-0.5)¬≤ + (0.5)¬≤ + (1.5)¬≤]/3</p>
                <p>= [2.25 + 0.25 + 0.25 + 2.25]/3 = 5/3</p>
                
                <p><strong>Cov(1,2) = Cov(X‚ÇÅ, X‚ÇÇ):</strong></p>
                <p>= [(1-2.5)(4-5.75) + (2-2.5)(3-5.75) + (3-2.5)(7-5.75) + (4-2.5)(9-5.75)]/3</p>
                <p>= [(-1.5)(-1.75) + (-0.5)(-2.75) + (0.5)(1.25) + (1.5)(3.25)]/3</p>
                <p>= [2.625 + 1.375 + 0.625 + 4.875]/3 = 9.5/3</p>
                
                <p><strong>Final Covariance Matrix:</strong></p>
                <div class="formula">
                    C = [5/3    9.5/3]
                        [9.5/3    œÉ¬≤‚ÇÇ]
                </div>
                
                <p><em>Homework: Calculate Cov(2,2) = Var(X‚ÇÇ)</em></p>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "We divide by (n-1) = 3 instead of n = 4 because we're computing sample covariance. This gives us unbiased estimation."
            </div>
            
            <h3>10.3 Interpreting Covariance Matrices</h3>
            <div class="example">
                <h4>Four Different Data Distributions</h4>
                <table>
                    <tr>
                        <th>Distribution Type</th>
                        <th>Covariance Matrix</th>
                        <th>Interpretation</th>
                    </tr>
                    <tr>
                        <td>Independent Features</td>
                        <td>[œÉ‚ÇÅ¬≤  0  ]<br>[0   œÉ‚ÇÇ¬≤]</td>
                        <td>Diagonal matrix - no correlation</td>
                    </tr>
                    <tr>
                        <td>Positive Correlation</td>
                        <td>[œÉ‚ÇÅ¬≤  0.8]<br>[0.8  œÉ‚ÇÇ¬≤]</td>
                        <td>Positive off-diagonal - features increase together</td>
                    </tr>
                    <tr>
                        <td>Different Variances</td>
                        <td>[œÉ‚ÇÅ¬≤  0  ]<br>[0   œÉ‚ÇÉ¬≤]</td>
                        <td>œÉ‚ÇÅ¬≤ ‚â† œÉ‚ÇÉ¬≤ - different spreads in each dimension</td>
                    </tr>
                    <tr>
                        <td>Negative Correlation</td>
                        <td>[œÉ‚ÇÅ¬≤  -0.7]<br>[-0.7  œÉ‚ÇÇ¬≤]</td>
                        <td>Negative off-diagonal - one increases, other decreases</td>
                    </tr>
                </table>
            </div>
            
            <div class="diagram-placeholder">
                [Insert diagram: Four 2D scatter plots showing different covariance matrix patterns - independent, positive correlation, different variances, negative correlation]
            </div>
            
            <h3>10.4 Properties of Covariance Matrix</h3>
            <div class="formula">
                <strong>Key Properties:</strong>
                <br>1. <strong>Symmetric:</strong> C[i,j] = C[j,i]
                <br>2. <strong>Diagonal entries are variances:</strong> C[i,i] = Var(X·µ¢)
                <br>3. <strong>Off-diagonal entries are covariances:</strong> C[i,j] = Cov(X·µ¢, X‚±º)
                <br>4. <strong>Positive Semi-Definite (PSD):</strong> v·µÄCv ‚â• 0 for any vector v
                <br>5. <strong>If variables independent:</strong> C is diagonal matrix
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "Positive semi-definite means for any vector v, the quantity v·µÄCv ‚â• 0. This is always true for covariance matrices and has important implications in optimization and machine learning."
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary</h4>
                <p>Covariance matrix d√ód size ka hota hai jo batata hai ki variables ke beech kya relationship hai. Diagonal mein variance hoti hai, off-diagonal mein covariance. Positive covariance matlab dono saath badhte hain, negative matlab ek badhta hai to dusra ghatta hai. Ye matrix symmetric aur positive semi-definite hoti hai.</p>
            </div>
            
            <div class="practice-questions">
                <h4>ü§î Practice Questions</h4>
                <ol>
                    <li>What does a diagonal covariance matrix indicate?</li>
                    <li>Calculate Cov(2,2) for the worked example above</li>
                    <li>What does negative covariance between X‚ÇÅ and X‚ÇÇ mean?</li>
                </ol>
                
                <div class="answer">
                    <strong>Answers:</strong>
                    <ol>
                        <li>Variables are independent (no correlation between features)</li>
                        <li>Var(X‚ÇÇ) = [(4-5.75)¬≤ + (3-5.75)¬≤ + (7-5.75)¬≤ + (9-5.75)¬≤]/3</li>
                        <li>When X‚ÇÅ increases, X‚ÇÇ tends to decrease (negative relationship)</li>
                    </ol>
                </div>
            </div>
        </section>

        <section id="correlation">
            <h2>11. Correlation Coefficient</h2>
            
            <div class="formula">
                <strong>Correlation Coefficient:</strong>
                <br>œÅ(X·µ¢, X‚±º) = Cov(X·µ¢, X‚±º) / (œÉ·µ¢ √ó œÉ‚±º)
                <br><br>
                <strong>Properties:</strong>
                <br>‚Ä¢ -1 ‚â§ œÅ ‚â§ 1 (normalized measure)
                <br>‚Ä¢ œÅ = 1: Perfect positive correlation
                <br>‚Ä¢ œÅ = -1: Perfect negative correlation  
                <br>‚Ä¢ œÅ = 0: No linear correlation
            </div>
            
            <p>The <span class="key-term">correlation coefficient</span> is a normalized version of covariance that makes it easier to interpret the strength of relationships between variables.</p>
            
            <div class="example">
                <h4>Correlation Interpretation</h4>
                <table>
                    <tr>
                        <th>œÅ Value</th>
                        <th>Relationship</th>
                        <th>Interpretation</th>
                    </tr>
                    <tr>
                        <td>œÅ = 1</td>
                        <td>Perfect positive</td>
                        <td>X‚ÇÇ = a√óX‚ÇÅ + b (a > 0)</td>
                    </tr>
                    <tr>
                        <td>0.7 ‚â§ œÅ < 1</td>
                        <td>Strong positive</td>
                        <td>Variables increase together strongly</td>
                    </tr>
                    <tr>
                        <td>0.3 ‚â§ œÅ < 0.7</td>
                        <td>Moderate positive</td>
                        <td>Some positive relationship</td>
                    </tr>
                    <tr>
                        <td>-0.3 < œÅ < 0.3</td>
                        <td>Weak/No correlation</td>
                        <td>Little to no linear relationship</td>
                    </tr>
                    <tr>
                        <td>-0.7 < œÅ ‚â§ -0.3</td>
                        <td>Moderate negative</td>
                        <td>Some negative relationship</td>
                    </tr>
                    <tr>
                        <td>-1 < œÅ ‚â§ -0.7</td>
                        <td>Strong negative</td>
                        <td>Variables move in opposite directions strongly</td>
                    </tr>
                    <tr>
                        <td>œÅ = -1</td>
                        <td>Perfect negative</td>
                        <td>X‚ÇÇ = a√óX‚ÇÅ + b (a < 0)</td>
                    </tr>
                </table>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> "Correlation coefficient normalizes the covariance by dividing by the product of standard deviations. This gives us a value between -1 and 1, making it easy to compare relationships across different datasets and variables."
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary</h4>
                <p>Correlation coefficient covariance ka normalized version hai jo -1 se 1 tak ki value deta hai. 1 matlab perfect positive correlation, -1 matlab perfect negative, aur 0 matlab koi linear relationship nahi. Ye measure variables ke beech relationship ki strength batane ke liye use karte hain.</p>
            </div>
        </section>

        <section id="key-takeaways">
            <div class="key-takeaways">
                <h2>üéØ Key Takeaways from Today's Lecture</h2>
                <ul>
                    <li><strong>Experiments & Sample Space:</strong> Experiments are repeatable procedures with well-defined outcomes; sample space contains all possible outcomes</li>
                    <li><strong>Random Variables:</strong> Functions mapping sample space to real numbers - essential for mathematical analysis</li>
                    <li><strong>PMF vs PDF:</strong> PMF for discrete variables (actual probabilities ‚â§ 1), PDF for continuous variables (densities can > 1)</li>
                    <li><strong>Statistical Properties:</strong> Mean shows center, variance shows spread within dimensions, covariance matrix captures cross-dimensional relationships</li>
                    <li><strong>Covariance Matrix:</strong> d√ód matrix for d-dimensional data; diagonal = variances, off-diagonal = covariances</li>
                    <li><strong>Correlation:</strong> Normalized covariance (-1 to 1) for easier interpretation of relationships</li>
                </ul>
                
                <h3>üîÆ Next Lecture Preview</h3>
                <ul>
                    <li>Deeper dive into probability theory</li>
                    <li>Conditional probability, likelihood, and priors</li>
                    <li>Popular probability distributions (especially Gaussian)</li>
                    <li>Parameter estimation techniques</li>
                </ul>
            </div>
        </section>

        <section id="mind-map">
            <div class="mind-map">
                <h2>üìä Comprehensive Mind Map</h2>
                <div style="text-align: center;">
                    <div class="mind-map-node" style="font-size: 1.2em; background: #ffecb3; border-color: #ff8f00;">
                        <strong>PATTERN RECOGNITION PRINCIPLES</strong><br>
                        <small>Probability Theory & Statistical Properties</small>
                    </div>
                    
                    <br><br>
                    
                    <div class="mind-map-node">
                        <strong>EXPERIMENTS</strong>
                        <div style="margin-top: 10px;">
                            <div class="mind-map-subnode">Repeatable Procedures</div>
                            <div class="mind-map-subnode">Well-defined Outcomes</div>
                            <div class="mind-map-subnode">Sample Space (Œ©)</div>
                            <div class="mind-map-subnode">Events (Subsets)</div>
                        </div>
                    </div>
                    
                    <div class="mind-map-node">
                        <strong>PROBABILITY THEORY</strong>
                        <div style="margin-top: 10px;">
                            <div class="mind-map-subnode">3 Axioms</div>
                            <div class="mind-map-subnode">Non-negativity</div>
                            <div class="mind-map-subnode">Normalization</div>
                            <div class="mind-map-subnode">Additivity</div>
                        </div>
                    </div>
                    
                    <br><br>
                    
                    <div class="mind-map-node">
                        <strong>RANDOM VARIABLES</strong>
                        <div style="margin-top: 10px;">
                            <div class="mind-map-subnode">Œ© ‚Üí ‚Ñù Mapping</div>
                            <div class="mind-map-subnode">Discrete Types</div>
                            <div class="mind-map-subnode">Continuous Types</div>
                        </div>
                    </div>
                    
                    <div class="mind-map-node">
                        <strong>PROBABILITY FUNCTIONS</strong>
                        <div style="margin-top: 10px;">
                            <div class="mind-map-subnode">PMF (Discrete)</div>
                            <div class="mind-map-subnode">PDF (Continuous)</div>
                            <div class="mind-map-subnode">CDF (Cumulative)</div>
                        </div>
                    </div>
                    
                    <br><br>
                    
                    <div class="mind-map-node">
                        <strong>STATISTICAL PROPERTIES</strong>
                        <div style="margin-top: 10px;">
                            <div class="mind-map-subnode">Mean (Œº)</div>
                            <div class="mind-map-subnode">Variance (œÉ¬≤)</div>
                            <div class="mind-map-subnode">Covariance Matrix</div>
                            <div class="mind-map-subnode">Correlation (œÅ)</div>
                        </div>
                    </div>
                    
                    <br><br>
                    
                    <div style="font-size: 0.9em; color: #666; margin-top: 20px;">
                        <strong>Connections:</strong><br>
                        Experiments ‚Üí Sample Space ‚Üí Random Variables ‚Üí Probability Functions ‚Üí Statistical Analysis<br>
                        Mean (Center) ‚Üí Variance (Spread) ‚Üí Covariance (Relationships) ‚Üí Correlation (Normalized)
                    </div>
                </div>
            </div>
        </section>
        
        <footer style="text-align: center; margin-top: 50px; padding-top: 20px; border-top: 1px solid #ddd; color: #666;">
            <p><em>Pattern Recognition Principles - Lecture 2</em><br>
            <small>Generated comprehensive lecture notes combining PDF slides with detailed transcript explanations</small></p>
        </footer>
    </div>
</body>
</html>