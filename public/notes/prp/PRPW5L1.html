<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pattern Recognition Principles - Lecture Notes</title>
    
    <!-- MathJax for Mathematical Equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.8;
            color: #333;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 10px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1);
        }
        
        header {
            text-align: center;
            padding: 30px 0;
            border-bottom: 4px solid #3498db;
            margin-bottom: 40px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 8px;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        header p {
            font-size: 1.2em;
            opacity: 0.9;
        }
        
        .toc {
            background: #f8f9fa;
            padding: 25px;
            border-left: 5px solid #3498db;
            margin: 30px 0;
            border-radius: 5px;
        }
        
        .toc h2 {
            color: #2c3e50;
            margin-bottom: 15px;
            font-size: 1.8em;
        }
        
        .toc ul {
            list-style: none;
            padding-left: 0;
        }
        
        .toc ul li {
            margin: 10px 0;
            padding: 8px 0;
            border-bottom: 1px solid #ddd;
        }
        
        .toc ul li:last-child {
            border-bottom: none;
        }
        
        .toc a {
            color: #3498db;
            text-decoration: none;
            font-size: 1.1em;
            transition: all 0.3s;
            display: block;
            padding: 5px;
        }
        
        .toc a:hover {
            color: #2980b9;
            padding-left: 10px;
            background: rgba(52, 152, 219, 0.1);
        }
        
        h2 {
            color: #2c3e50;
            font-size: 2em;
            margin: 40px 0 20px 0;
            padding: 15px;
            background: linear-gradient(90deg, #3498db 0%, #2980b9 100%);
            color: white;
            border-radius: 5px;
        }
        
        h3 {
            color: #34495e;
            font-size: 1.6em;
            margin: 30px 0 15px 0;
            padding: 10px;
            background: #ecf0f1;
            border-left: 5px solid #3498db;
        }
        
        h4 {
            color: #555;
            font-size: 1.3em;
            margin: 20px 0 10px 0;
            padding: 8px;
            background: #f8f9fa;
            border-left: 3px solid #95a5a6;
        }
        
        p {
            margin: 15px 0;
            text-align: justify;
        }
        
        .key-term {
            font-weight: bold;
            color: #e74c3c;
            background: #ffe6e6;
            padding: 2px 6px;
            border-radius: 3px;
        }
        
        .equation {
            background: #f9f9f9;
            padding: 20px;
            margin: 20px 0;
            border-left: 4px solid #3498db;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 1.1em;
        }
        
        .professor-note {
            background: #fff3cd;
            border-left: 5px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }
        
        .professor-note::before {
            content: "üë®‚Äçüè´ Professor mentioned in class: ";
            font-weight: bold;
            color: #856404;
        }
        
        .hinglish-summary {
            background: linear-gradient(135deg, #667eea20 0%, #764ba220 100%);
            border: 2px solid #667eea;
            padding: 20px;
            margin: 25px 0;
            border-radius: 8px;
        }
        
        .hinglish-summary::before {
            content: "üìù Hinglish Summary / ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂:";
            display: block;
            font-weight: bold;
            color: #667eea;
            font-size: 1.2em;
            margin-bottom: 10px;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        table th {
            background: #3498db;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: bold;
        }
        
        table td {
            padding: 12px 15px;
            border-bottom: 1px solid #ddd;
        }
        
        table tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        table tr:hover {
            background: #e8f4f8;
        }
        
        .diagram-placeholder {
            background: #e8f4f8;
            border: 2px dashed #3498db;
            padding: 40px;
            margin: 25px 0;
            text-align: center;
            border-radius: 8px;
            color: #2980b9;
            font-style: italic;
        }
        
        .key-takeaways {
            background: #d4edda;
            border: 2px solid #28a745;
            padding: 20px;
            margin: 25px 0;
            border-radius: 8px;
        }
        
        .key-takeaways h4 {
            color: #155724;
            background: transparent;
            border: none;
            margin-top: 0;
        }
        
        .key-takeaways ul {
            padding-left: 25px;
            margin-top: 10px;
        }
        
        .key-takeaways li {
            margin: 8px 0;
            color: #155724;
        }
        
        .practice-questions {
            background: #fff;
            border: 2px solid #9b59b6;
            padding: 20px;
            margin: 25px 0;
            border-radius: 8px;
        }
        
        .practice-questions h4 {
            color: #9b59b6;
            background: transparent;
            border: none;
            margin-top: 0;
        }
        
        .question {
            background: #f8f9fa;
            padding: 15px;
            margin: 15px 0;
            border-left: 4px solid #9b59b6;
            border-radius: 5px;
        }
        
        .answer {
            background: #e8f5e9;
            padding: 15px;
            margin: 10px 0 15px 20px;
            border-left: 4px solid #4caf50;
            border-radius: 5px;
        }
        
        .answer::before {
            content: "‚úì Answer: ";
            font-weight: bold;
            color: #2e7d32;
        }
        
        .mind-map {
            margin: 40px 0;
            padding: 30px;
            background: white;
            border: 3px solid #3498db;
            border-radius: 10px;
        }
        
        .mind-map h2 {
            text-align: center;
            margin-bottom: 30px;
        }
        
        .mind-map-container {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        
        .main-topic {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            font-size: 1.5em;
            font-weight: bold;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }
        
        .subtopics {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        
        .subtopic {
            background: #3498db;
            color: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 3px 10px rgba(0,0,0,0.1);
        }
        
        .subtopic h4 {
            color: white;
            background: transparent;
            border: none;
            padding: 0;
            margin: 0 0 10px 0;
        }
        
        .subtopic ul {
            list-style: none;
            padding-left: 0;
        }
        
        .subtopic li {
            padding: 5px 0;
            padding-left: 15px;
            position: relative;
        }
        
        .subtopic li::before {
            content: "‚Üí";
            position: absolute;
            left: 0;
        }
        
        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #e74c3c;
        }
        
        .recap-section {
            background: #e3f2fd;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 5px solid #2196f3;
        }
        
        @media print {
            body {
                background: white;
            }
            .container {
                box-shadow: none;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Pattern Recognition Principles</h1>
            <h1>By Armaan Kachhawa</h1>
            <p style="font-size: 0.9em; margin-top: 10px;">Comprehensive Lecture Notes</p>
        </header>

        <!-- Table of Contents -->
        <nav class="toc">
            <h2>üìö Table of Contents</h2>
            <ul>
                <li><a href="#recap">1. Recap</a></li>
                <li><a href="#bayesian-decision">2. Bayesian Decision Theory</a></li>
                <li><a href="#error-bayesian">3. Error in Bayesian Classification</a></li>
                <li><a href="#risk-bayesian">4. Risk in Bayesian Classification</a></li>
                <li><a href="#discriminant">5. Discriminant Functions</a>
                    <ul style="padding-left: 20px; margin-top: 10px;">
                        <li><a href="#linear-discriminant">5.1 Linear Discriminant Functions</a></li>
                        <li><a href="#nonlinear-discriminant">5.2 Non-Linear Discriminant Functions</a></li>
                        <li><a href="#bayesian-discriminant">5.3 Bayesian Discriminant Functions</a></li>
                    </ul>
                </li>
                <li><a href="#mind-map">6. Mind Map</a></li>
            </ul>
        </nav>

        <!-- Recap Section -->
        <section id="recap">
            <h2>1. Recap</h2>
            
            <div class="recap-section">
                <h3>Previously Covered Topics</h3>
                <ul style="padding-left: 25px; margin-top: 15px;">
                    <li><strong>Conditional Probability:</strong> The probability of an event occurring given that another event has already occurred</li>
                    <li><strong>Bayes Theorem:</strong> A fundamental theorem in probability theory for updating beliefs based on evidence</li>
                    <li><strong>Gaussian Distribution:</strong> A continuous probability distribution that is symmetric about the mean</li>
                    <li><strong>Uniform Distribution:</strong> A probability distribution where all outcomes are equally likely</li>
                </ul>
            </div>

            <div class="hinglish-summary">
                Pichli class mein humne conditional probability, Bayes theorem, aur distributions (Gaussian aur Uniform) ke baare mein padha tha. Yeh sab concepts aaj ke lecture ke liye foundation hai. Bayes theorem basically yeh batata hai ki kaise hum naye evidence ke basis par apni probabilities ko update kar sakte hain.
            </div>
        </section>

        <!-- Bayesian Decision Theory -->
        <section id="bayesian-decision">
            <h2>2. Bayesian Decision Theory</h2>
            
            <h3>2.1 Introduction</h3>
            <p>
                <span class="key-term">Bayesian Decision Theory</span> is a fundamental statistical approach to pattern classification. It provides a systematic way to make optimal decisions in the presence of uncertainty using probability theory and the <span class="key-term">Bayes' Theorem</span>.
            </p>

            <div class="professor-note">
                The Bayesian approach is one of the most powerful methods in pattern recognition because it provides a probabilistic framework for decision-making under uncertainty.
            </div>

            <h3>2.2 Bayes' Theorem for Classification</h3>
            <p>
                The foundation of Bayesian Decision Theory is Bayes' Theorem. For a classification problem with class $\omega_i$ and feature vector $X$, Bayes' Theorem states:
            </p>

            <div class="equation">

                $$P(\omega_i | X) = \frac{P(X | \omega_i) \cdot P(\omega_i)}{P(X)}$$
            </div>

            <h4>Components of Bayes' Theorem:</h4>
            <table>
                <tr>
                    <th>Component</th>
                    <th>Symbol</th>
                    <th>Description</th>
                </tr>
                <tr>
                    <td><strong>Posterior Probability</strong></td>
                    <td>$P(\omega_i | X)$</td>
                    <td>The probability that the sample belongs to class $\omega_i$ given the observed features $X$</td>
                </tr>
                <tr>
                    <td><strong>Likelihood</strong></td>
                    <td>$P(X | \omega_i)$</td>
                    <td>The probability of observing features $X$ given that the sample belongs to class $\omega_i$</td>
                </tr>
                <tr>
                    <td><strong>Prior Probability</strong></td>
                    <td>$P(\omega_i)$</td>
                    <td>The prior probability of class $\omega_i$ (before observing any features)</td>
                </tr>
                <tr>
                    <td><strong>Evidence</strong></td>
                    <td>$P(X)$</td>
                    <td>The total probability of observing features $X$ across all classes (normalization factor)</td>
                </tr>
            </table>

            <h3>2.3 Decision Rule</h3>
            <p>
                In <span class="key-term">Bayesian Decision Theory</span>, we classify a sample $X$ to the class that has the <strong>maximum posterior probability</strong>:
            </p>

            <div class="equation">

                $$\text{Assign } X \text{ to class } \omega_i \text{ if } P(\omega_i | X) > P(\omega_j | X) \text{ for all } j \neq i$$
            </div>

            <div class="professor-note">
                The decision rule is very simple: for a given sample, calculate the posterior probability for each class, and whichever posterior probability is higher, we assign that particular class. This is the essence of Bayesian classification.
            </div>

            <h3>2.4 Simplifying the Decision Rule</h3>
            <p>
                Since $P(X)$ is common across all classes (it doesn't depend on the class $\omega_i$), we can ignore it for the purpose of finding the maximum. Therefore, the decision rule simplifies to:
            </p>

            <div class="equation">

                $$\text{Maximize: } P(X | \omega_i) \cdot P(\omega_i)$$
            </div>

            <p>
                This means we only need to compute the <span class="key-term">likelihood</span> multiplied by the <span class="key-term">prior probability</span> for each class and choose the class with the highest value.
            </p>

            <div class="hinglish-summary">
                Bayesian Decision Theory ek powerful method hai classification ke liye. Ismein hum Bayes' Theorem ka use karte hain. Basic idea yeh hai ki hum posterior probability calculate karte hain har class ke liye, aur jis class ki posterior probability sabse zyada hoti hai, hum sample ko us class mein assign kar dete hain. P(X) common hota hai sab classes ke liye, toh hum sirf P(X|œâi) √ó P(œâi) ko maximize karte hain.
            </div>

            <div class="key-takeaways">
                <h4>üéØ Key Takeaways</h4>
                <ul>
                    <li>Bayesian Decision Theory uses Bayes' Theorem for classification</li>
                    <li>We classify based on maximum posterior probability</li>
                    <li>Posterior = (Likelihood √ó Prior) / Evidence</li>
                    <li>Evidence term P(X) can be ignored when comparing classes</li>
                    <li>The approach provides an optimal decision-making framework under uncertainty</li>
                </ul>
            </div>
        </section>

        <!-- Error in Bayesian Classification -->
        <section id="error-bayesian">
            <h2>3. Error in Bayesian Classification</h2>
            
            <h3>3.1 Understanding Classification Error</h3>
            <p>
                Even with the optimal Bayesian classifier, there is always some probability of misclassification. This occurs because the probability distributions of different classes may overlap. The <span class="key-term">Bayesian Error</span> represents the minimum possible error rate that can be achieved for a given classification problem.
            </p>

            <div class="professor-note">
                No matter how good your classifier is, if the distributions of different classes overlap, you will have some error. The Bayesian approach gives us the minimum error that is theoretically possible for that particular problem.
            </div>

            <h3>3.2 Types of Classification Errors</h3>
            <p>
                In a binary classification problem with classes $\omega_1$ and $\omega_2$, there are two types of errors:
            </p>

            <table>
                <tr>
                    <th>Error Type</th>
                    <th>Description</th>
                    <th>Mathematical Expression</th>
                </tr>
                <tr>
                    <td><strong>Type I Error (False Positive)</strong></td>
                    <td>Classifying a sample from class $\omega_2$ as class $\omega_1$</td>
                    <td>$P(\text{decide } \omega_1 | \text{true class is } \omega_2)$</td>
                </tr>
                <tr>
                    <td><strong>Type II Error (False Negative)</strong></td>
                    <td>Classifying a sample from class $\omega_1$ as class $\omega_2$</td>
                    <td>$P(\text{decide } \omega_2 | \text{true class is } \omega_1)$</td>
                </tr>
            </table>

            <h3>3.3 Probability of Error</h3>
            <p>
                The overall <span class="key-term">probability of error</span> $P(error)$ in Bayesian classification is given by:
            </p>

            <div class="equation">

                $$P(error) = P(\omega_1) \cdot P(error | \omega_1) + P(\omega_2) \cdot P(error | \omega_2)$$
            </div>

            <p>
                Where:
                <ul style="padding-left: 25px; margin-top: 10px;">
                    <li>$P(error | \omega_1)$ is the probability of misclassifying a sample from class $\omega_1$</li>
                    <li>$P(error | \omega_2)$ is the probability of misclassifying a sample from class $\omega_2$</li>
                </ul>
            </p>

            <h3>3.4 Minimizing Error</h3>
            <p>
                The Bayesian classifier is <strong>optimal</strong> in the sense that it minimizes the probability of error. By choosing the class with the maximum posterior probability, we automatically minimize the classification error.
            </p>

            <div class="diagram-placeholder">
                [Insert diagram: Overlapping probability distributions showing decision boundary and error regions]
            </div>

            <div class="hinglish-summary">
                Classification mein error tab hota hai jab do classes ke probability distributions overlap karte hain. Bayesian classifier optimal hai kyunki yeh minimum possible error deta hai. Do types ke errors hote hain: Type I (false positive) aur Type II (false negative). Total error probability dono classes ke errors ka weighted sum hota hai, jahan weights unki prior probabilities hoti hain.
            </div>

            <div class="key-takeaways">
                <h4>üéØ Key Takeaways</h4>
                <ul>
                    <li>Bayesian Error is the minimum achievable error for a given problem</li>
                    <li>Error occurs due to overlapping class distributions</li>
                    <li>Two types of errors: False Positive (Type I) and False Negative (Type II)</li>
                    <li>Bayesian classifier minimizes the overall probability of error</li>
                    <li>Total error is a weighted sum of errors from all classes</li>
                </ul>
            </div>
        </section>

        <!-- Risk in Bayesian Classification -->
        <section id="risk-bayesian">
            <h2>4. Risk in Bayesian Classification</h2>
            
            <h3>4.1 Introduction to Risk</h3>
            <p>
                In many real-world applications, different types of errors have different costs or consequences. For example, in medical diagnosis, failing to detect a disease (false negative) might be more costly than a false alarm (false positive). The concept of <span class="key-term">Risk</span> in Bayesian classification incorporates these varying costs into the decision-making process.
            </p>

            <div class="professor-note">
                Risk is a generalization of error. Instead of just counting errors, we assign different costs to different types of misclassifications and try to minimize the expected cost.
            </div>

            <h3>4.2 Loss Function</h3>
            <p>
                A <span class="key-term">Loss Function</span> $L(\alpha_i | \omega_j)$ represents the cost or loss incurred when we take action $\alpha_i$ (decide class $i$) when the true state is $\omega_j$ (true class $j$).
            </p>

            <table>
                <tr>
                    <th>Action / True Class</th>
                    <th>$\omega_1$ (True)</th>
                    <th>$\omega_2$ (True)</th>
                </tr>
                <tr>
                    <td><strong>Decide $\omega_1$</strong></td>
                    <td>$L(\alpha_1 | \omega_1)$ = 0 (Correct)</td>
                    <td>$L(\alpha_1 | \omega_2)$ (False Positive)</td>
                </tr>
                <tr>
                    <td><strong>Decide $\omega_2$</strong></td>
                    <td>$L(\alpha_2 | \omega_1)$ (False Negative)</td>
                    <td>$L(\alpha_2 | \omega_2)$ = 0 (Correct)</td>
                </tr>
            </table>

            <p>
                Typically, the loss is zero for correct classifications: $L(\alpha_i | \omega_i) = 0$
            </p>

            <h3>4.3 Conditional Risk</h3>
            <p>
                The <span class="key-term">Conditional Risk</span> $R(\alpha_i | X)$ is the expected loss of taking action $\alpha_i$ given observation $X$:
            </p>

            <div class="equation">

                $$R(\alpha_i | X) = \sum_{j=1}^{c} L(\alpha_i | \omega_j) \cdot P(\omega_j | X)$$
            </div>

            <p>
                Where $c$ is the number of classes. This represents the weighted average of losses, weighted by the posterior probabilities.
            </p>

            <h3>4.4 Minimum Risk Decision Rule</h3>
            <p>
                The optimal decision rule is to choose the action that <strong>minimizes the conditional risk</strong>:
            </p>

            <div class="equation">

                $$\text{Choose action } \alpha_i \text{ if } R(\alpha_i | X) < R(\alpha_j | X) \text{ for all } j \neq i$$
            </div>

            <h3>4.5 Overall Risk (Bayes Risk)</h3>
            <p>
                The <span class="key-term">Bayes Risk</span> is the expected risk over all possible observations:
            </p>

            <div class="equation">

                $$R = \int R(\alpha(X) | X) \cdot P(X) \, dX$$
            </div>

            <p>
                Where $\alpha(X)$ is the decision rule (action chosen for observation $X$). The Bayes Risk represents the minimum expected loss achievable by any classifier.
            </p>

            <h3>4.6 Special Case: 0-1 Loss</h3>
            <p>
                When all misclassifications have equal cost (0-1 loss function), minimizing risk is equivalent to minimizing error. In this case:
            </p>

            <div class="equation">

                $$L(\alpha_i | \omega_j) = \begin{cases} 0 & \text{if } i = j \\ 1 & \text{if } i \neq j \end{cases}$$
            </div>

            <p>
                With 0-1 loss, the minimum risk decision reduces to the maximum posterior probability rule.
            </p>

            <div class="hinglish-summary">
                Risk ek advanced concept hai jo error ko generalize karta hai. Jab different types ki errors ki alag-alag costs hoti hain, tab hum risk minimize karte hain. Loss function define karta hai ki agar hum galat decision lete hain toh kitna loss hoga. Conditional risk ek expected loss hai given koi observation. Optimal decision rule yeh hai ki hum wo action choose karein jisse conditional risk minimum ho. Bayes Risk minimum possible expected loss hai jo koi bhi classifier achieve kar sakta hai.
            </div>

            <div class="key-takeaways">
                <h4>üéØ Key Takeaways</h4>
                <ul>
                    <li>Risk incorporates varying costs of different types of errors</li>
                    <li>Loss function quantifies the cost of each decision</li>
                    <li>Conditional risk is the expected loss given an observation</li>
                    <li>Optimal decision rule minimizes conditional risk</li>
                    <li>Bayes Risk is the minimum achievable expected loss</li>
                    <li>With equal error costs (0-1 loss), risk minimization = error minimization</li>
                </ul>
            </div>

            <div class="practice-questions">
                <h4>üìù Practice Questions</h4>
                
                <div class="question">
                    <strong>Q1:</strong> What is the difference between minimizing error and minimizing risk in Bayesian classification?
                </div>
                <div class="answer">
                    Minimizing error treats all misclassifications equally (0-1 loss), while minimizing risk allows different misclassifications to have different costs through the loss function. Risk is a more general framework that reduces to error minimization when all misclassification costs are equal.
                </div>

                <div class="question">
                    <strong>Q2:</strong> If the loss of misclassifying a disease patient as healthy is 100, and misclassifying a healthy person as diseased is 10, how should the decision threshold be adjusted compared to equal loss?
                </div>
                <div class="answer">
                    The threshold should be shifted to make the classifier more sensitive (lower threshold for disease detection). Since missing a disease patient is 10 times more costly, we should be willing to accept more false positives to reduce false negatives. This means we'll classify more borderline cases as diseased to minimize overall risk.
                </div>

                <div class="question">
                    <strong>Q3:</strong> What is Bayes Risk and why is it important?
                </div>
                <div class="answer">
                    Bayes Risk is the minimum expected loss achievable by any classifier for a given problem. It's important because it provides a theoretical lower bound on performance‚Äîno classifier can do better than the Bayes Risk for that problem. It helps us understand the inherent difficulty of a classification task and evaluate how close our classifiers are to optimal performance.
                </div>
            </div>
        </section>

        <!-- Discriminant Functions -->
        <section id="discriminant">
            <h2>5. Discriminant Functions</h2>
            
            <h3>5.1 Introduction to Discriminant Functions</h3>
            <p>
                From the training data, we need to develop some functions that, when evaluated for different classes, give meaningful outputs that help discriminate between classes. The idea is that for a test sample, the discriminant function should give a higher value for the class to which the sample belongs.
            </p>

            <div class="professor-note">
                The idea is: from a given training data, how do we come up with some function which, when evaluated for training data of different classes, gives something meaningful? The meaningful thing here is that it tries to discriminate between them. For a test sample, the test function should be higher for the class where the sample belongs to.
            </div>

            <h3>5.2 Formal Definition</h3>
            <p>
                A pattern classifier can be represented as a set of <span class="key-term">Discriminant Functions</span>:
            </p>

            <div class="equation">

                $$g_1(X), g_2(X), \ldots, g_c(X)$$
            </div>

            <p>
                Where:
                <ul style="padding-left: 25px; margin-top: 10px;">
                    <li>$c$ is the number of classes (for binary classification, $c = 2$)</li>
                    <li>$X$ is a $d$-dimensional feature vector representing the sample</li>
                    <li>Each $g_i(X)$ is a real-valued function</li>
                </ul>
            </p>

            <h3>5.3 Classification Rule</h3>
            <p>
                The classifier assigns feature vector $X$ to class $\omega_i$ if:
            </p>

            <div class="equation">

                $$g_i(X) > g_j(X) \quad \text{for all } j \neq i$$
            </div>

            <p>
                This means we evaluate all $c$ discriminant functions for the input sample $X$, and whichever function gives the maximum value, we assign that class to the sample.
            </p>

            <h3>5.4 Vector Representation</h3>
            <p>
                Discriminant functions can be represented vectorially:
            </p>

            <div class="diagram-placeholder">
                [Insert diagram: Vector representation showing input X (d-dimensional) going through C discriminant functions g1, g2, ..., gc, followed by a max operation to determine the class]
            </div>

            <p>
                The input is a $d$-dimensional feature vector $X = [x_1, x_2, \ldots, x_d]^T$. For this feature vector, we evaluate each discriminant function, and then take the maximum to determine the class.
            </p>

            <div class="hinglish-summary">
                Discriminant functions ek set of functions hote hain jo input sample ke liye evaluate kiye jaate hain. Har class ke liye ek discriminant function hota hai. Jis class ke liye function ki value sabse zyada hoti hai, sample ko us class mein assign kar dete hain. Yeh ek general framework hai classification ke liye‚Äîdifferent types ke discriminant functions ho sakte hain jaise linear, non-linear, ya Bayesian.
            </div>
        </section>

        <!-- Linear Discriminant Functions -->
        <section id="linear-discriminant">
            <h2>5.1 Linear Discriminant Functions</h2>
            
            <h3>5.1.1 Definition</h3>
            <p>
                A <span class="key-term">Linear Discriminant Function</span> is a discriminant function that is a linear combination of the feature vector components. For two classes $\omega_1$ and $\omega_2$, the linear discriminant functions are:
            </p>

            <div class="equation">

                $$g_1(X) = W_1^T X + b_1$$

                $$g_2(X) = W_2^T X + b_2$$
            </div>

            <p>
                Where:
                <ul style="padding-left: 25px; margin-top: 10px;">
                    <li>$W_1$ and $W_2$ are $d$-dimensional weight vectors</li>
                    <li>$b_1$ and $b_2$ are scalar bias terms</li>
                    <li>$X$ is the $d$-dimensional feature vector</li>
                    <li>$W^T X$ represents the dot product between weight vector and feature vector</li>
                </ul>
            </p>

            <h3>5.1.2 Expanded Form</h3>
            <p>
                If $X$ is 2-dimensional, i.e., $X = [x_1, x_2]^T$, then:
            </p>

            <div class="equation">

                $$g_1(X) = W_1^T X + b_1 = [w_{11}, w_{12}] \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} + b_1$$

                $$g_1(X) = w_{11} x_1 + w_{12} x_2 + b_1$$
            </div>

            <p>
                This is the <strong>equation of a hyperplane</strong> (or a line in 2D). It's similar to the familiar line equation $y = mx + c$, but generalized to multiple dimensions.
            </p>

            <div class="professor-note">
                This is the equation of a hyperplane if X is more than one dimension. If X is one dimension, this is the equation of a line‚Äîlike you remember y = mx + c. But here X could be more than one dimension, and in that case, if X is d-dimensional, W will also be d-dimensional, and b is a scalar bias value.
            </div>

            <h3>5.1.3 Decision Boundary</h3>
            <p>
                The <span class="key-term">Decision Boundary</span> is the set of points where both discriminant functions are equal:
            </p>

            <div class="equation">

                $$g_1(X) = g_2(X)$$

                $$W_1^T X + b_1 = W_2^T X + b_2$$

                $$(W_1 - W_2)^T X + (b_1 - b_2) = 0$$
            </div>

            <p>
                We can simplify this as:
            </p>

            <div class="equation">

                $$W^T X + b = 0$$
            </div>

            <p>
                Where $W = W_1 - W_2$ and $b = b_1 - b_2$. This is the equation of a hyperplane that separates the two classes.
            </p>

            <h3>5.1.4 Classification Regions</h3>
            <p>
                The decision boundary divides the feature space into regions:
            </p>

            <table>
                <tr>
                    <th>Region</th>
                    <th>Condition</th>
                    <th>Classification</th>
                </tr>
                <tr>
                    <td>One side of boundary</td>
                    <td>$g_1(X) > g_2(X)$</td>
                    <td>Class $\omega_1$</td>
                </tr>
                <tr>
                    <td>Other side of boundary</td>
                    <td>$g_1(X) < g_2(X)$</td>
                    <td>Class $\omega_2$</td>
                </tr>
                <tr>
                    <td>On the boundary</td>
                    <td>$g_1(X) = g_2(X)$</td>
                    <td>Ambiguous (on decision boundary)</td>
                </tr>
            </table>

            <div class="diagram-placeholder">
                [Insert diagram: 2D feature space showing linear decision boundary as a straight line separating two classes, with regions labeled as g1(X) > g2(X) on one side and g1(X) < g2(X) on the other]
            </div>

            <div class="professor-note">
                This is one of the lines which can classify positive and negative samples, or omega 1 and omega 2. At this line, both g1(X) and g2(X) (the discriminant functions) are equal for both classes. This line is called the decision boundary.
            </div>

            <div class="hinglish-summary">
                Linear discriminant function ek straight line ya hyperplane ki equation hoti hai. Ismein weights (W) aur bias (b) hote hain. Decision boundary wo line hai jahan dono classes ke discriminant functions equal hote hain. Ek side of line pe ek class hota hai, dusri side pe dusra class. Yeh linear boundary tab kaam karti hai jab data linearly separable ho‚Äîmatlab ek straight line se classes ko separate kiya ja sake.
            </div>

            <div class="key-takeaways">
                <h4>üéØ Key Takeaways</h4>
                <ul>
                    <li>Linear discriminant functions are equations of hyperplanes</li>
                    <li>Form: $g(X) = W^T X + b$</li>
                    <li>Decision boundary is where $g_1(X) = g_2(X)$</li>
                    <li>Boundary equation: $W^T X + b = 0$</li>
                    <li>Separates feature space into linear regions</li>
                    <li>Suitable for linearly separable data</li>
                </ul>
            </div>
        </section>

        <!-- Non-Linear Discriminant Functions -->
        <section id="nonlinear-discriminant">
            <h2>5.2 Non-Linear Discriminant Functions</h2>
            
            <h3>5.2.1 Introduction</h3>
            <p>
                When classes are not linearly separable, we need <span class="key-term">Non-Linear Discriminant Functions</span>. These functions can create curved decision boundaries that better separate complex distributions.
            </p>

            <h3>5.2.2 Quadratic Discriminant Function</h3>
            <p>
                A common form of non-linear discriminant function includes quadratic terms:
            </p>

            <div class="equation">

                $$g_1(X) = X^T A_1 X + B_1^T X + c_1$$

                $$g_2(X) = X^T A_2 X + B_2^T X + c_2$$
            </div>

            <p>
                Where:
                <ul style="padding-left: 25px; margin-top: 10px;">
                    <li>$A_1, A_2$ are $d \times d$ square matrices (quadratic term coefficients)</li>
                    <li>$B_1, B_2$ are $d$-dimensional vectors (linear term coefficients)</li>
                    <li>$c_1, c_2$ are scalar constants</li>
                    <li>$X$ is the $d$-dimensional feature vector</li>
                </ul>
            </p>

            <div class="professor-note">
                Here A is a d √ó d matrix, a square matrix. If you multiply, you get a square term here‚ÄîX square term‚Äîand then X term, and then c. This is a non-linear equation and non-linear discriminant function.
            </div>

            <h3>5.2.3 Non-Linear Decision Boundary</h3>
            <p>
                The decision boundary is found by equating the two discriminant functions:
            </p>

            <div class="equation">

                $$g_1(X) = g_2(X)$$

                $$X^T A_1 X + B_1^T X + c_1 = X^T A_2 X + B_2^T X + c_2$$

                $$X^T (A_1 - A_2) X + (B_1 - B_2)^T X + (c_1 - c_2) = 0$$
            </div>

            <p>
                Simplifying with $A = A_1 - A_2$, $B = B_1 - B_2$, and $c = c_1 - c_2$:
            </p>

            <div class="equation">

                $$X^T A X + B^T X + c = 0$$
            </div>

            <p>
                This is a <strong>quadratic equation</strong>, which can represent curves like ellipses, parabolas, or hyperbolas.
            </p>

            <h3>5.2.4 Example: 2D Case</h3>
            <p>
                Let's consider a concrete example where $X = [x_1, x_2]^T$ is 2-dimensional:
            </p>

            <div class="equation">

                $$A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad B = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad c = 0$$
            </div>

            <p>
                Then the decision boundary equation becomes:
            </p>

            <div class="equation">

                $$[x_1, x_2] \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} + [1, 1] \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} + 0 = 0$$
            </div>

            <div class="equation">

                $$x_1^2 + x_2^2 + x_1 + x_2 = 0$$
            </div>

            <p>
                This represents a circle (or ellipse) in 2D space, demonstrating how quadratic terms create curved boundaries.
            </p>

            <div class="professor-note">
                If I multiply this matrix [x1, x2], I get x1¬≤ + x2¬≤ + x1 + x2 + c = 0. So this is an equation which is non-linear in nature, and this is the decision boundary. At the decision boundary, both discriminant functions are going to be equal, while on one side of the decision boundary, one function will be higher than the other.
            </div>

            <h3>5.2.5 Types of Non-Linear Boundaries</h3>
            <p>
                Depending on the matrix $A$ and vectors $B$, the decision boundary can be:
            </p>

            <table>
                <tr>
                    <th>Boundary Type</th>
                    <th>Characteristics</th>
                    <th>Example Equation</th>
                </tr>
                <tr>
                    <td><strong>Circle/Ellipse</strong></td>
                    <td>Closed curve, both terms positive</td>
                    <td>$x_1^2 + x_2^2 = r^2$</td>
                </tr>
                <tr>
                    <td><strong>Hyperbola</strong></td>
                    <td>Open curve, opposite signs</td>
                    <td>$x_1^2 - x_2^2 = c$</td>
                </tr>
                <tr>
                    <td><strong>Parabola</strong></td>
                    <td>Open curve, one quadratic term</td>
                    <td>$x_1^2 + x_2 = 0$</td>
                </tr>
            </table>

            <div class="diagram-placeholder">
                [Insert diagram: 2D feature space showing non-linear (curved) decision boundaries - examples of circular, elliptical, and parabolic boundaries separating two classes]
            </div>

            <div class="hinglish-summary">
                Jab data linearly separable nahi hota, tab hum non-linear discriminant functions use karte hain. Inme quadratic terms (X¬≤) hote hain jo curved decision boundaries banate hain. Matrix A quadratic terms define karta hai, vector B linear terms define karta hai. Decision boundary circles, ellipses, parabolas jaise curves ban sakti hai. Yeh complex patterns ko better separate kar sakti hai compared to straight lines.
            </div>

            <div class="key-takeaways">
                <h4>üéØ Key Takeaways</h4>
                <ul>
                    <li>Non-linear discriminant functions handle non-linearly separable data</li>
                    <li>Quadratic form: $g(X) = X^T A X + B^T X + c$</li>
                    <li>Matrix A determines the quadratic terms (curvature)</li>
                    <li>Decision boundaries can be circles, ellipses, parabolas, or hyperbolas</li>
                    <li>More flexible than linear boundaries for complex patterns</li>
                    <li>Higher computational cost than linear discriminant functions</li>
                </ul>
            </div>
        </section>

        <!-- Bayesian Discriminant Functions -->
        <section id="bayesian-discriminant">
            <h2>5.3 Bayesian Discriminant Functions</h2>
            
            <h3>5.3.1 Connection to Bayesian Decision Theory</h3>
            <p>
                We can express Bayesian Decision Theory in terms of discriminant functions. The discriminant function for class $\omega_i$ in the Bayesian framework is:
            </p>

            <div class="equation">

                $$g_i(X) = P(\omega_i | X)$$
            </div>

            <p>
                The posterior probability itself serves as the discriminant function. Using Bayes' theorem:
            </p>

            <div class="equation">

                $$g_i(X) = P(\omega_i | X) = \frac{P(X | \omega_i) \cdot P(\omega_i)}{P(X)}$$
            </div>

            <h3>5.3.2 Simplified Discriminant Function</h3>
            <p>
                Since $P(X)$ is common across all classes and doesn't affect the comparison, we can use:
            </p>

            <div class="equation">

                $$g_i(X) = P(X | \omega_i) \cdot P(\omega_i)$$
            </div>

            <p>
                This is the product of the <span class="key-term">likelihood</span> and the <span class="key-term">prior probability</span>.
            </p>

            <div class="professor-note">
                In Bayesian decision theory, what we do is we actually try to find the maximum of the posterior. Since P(X) is not important for finding the maximum (because P(X) is common across all the terms for all classes), the discriminant function is just P(X|œâi) √ó P(œâi).
            </div>

            <h3>5.3.3 Log-Likelihood Discriminant Function</h3>
            <p>
                Taking the logarithm simplifies calculations without changing the decision (since log is monotonic):
            </p>

            <div class="equation">

                $$g_i(X) = \log P(X | \omega_i) + \log P(\omega_i)$$
            </div>

            <p>
                Using the logarithm property: $\log(A \cdot B) = \log A + \log B$
            </p>

            <h4>Advantages of Log-Likelihood:</h4>
            <ul style="padding-left: 25px; margin-top: 10px;">
                <li>Converts products into sums (computationally easier)</li>
                <li>Prevents numerical underflow with very small probabilities</li>
                <li>Simplifies derivatives for optimization</li>
                <li>Doesn't change the decision (log is monotonic)</li>
            </ul>

            <h3>5.3.4 Gaussian Discriminant Function</h3>
            <p>
                When the likelihood follows a <span class="key-term">Gaussian (Normal) Distribution</span>, we get a specific form. For a multivariate Gaussian:
            </p>

            <div class="equation">

                $$P(X | \omega_i) = \frac{1}{(2\pi)^{d/2} |\Sigma_i|^{1/2}} \exp\left(-\frac{1}{2}(X - \mu_i)^T \Sigma_i^{-1} (X - \mu_i)\right)$$
            </div>

            <p>
                Where:
                <ul style="padding-left: 25px; margin-top: 10px;">
                    <li>$\mu_i$ is the mean vector of class $\omega_i$</li>
                    <li>$\Sigma_i$ is the covariance matrix of class $\omega_i$</li>
                    <li>$d$ is the dimensionality of feature vector $X$</li>
                </ul>
            </p>

            <p>
                Taking the log:
            </p>

            <div class="equation">

                $$g_i(X) = -\frac{1}{2}(X - \mu_i)^T \Sigma_i^{-1} (X - \mu_i) - \frac{d}{2}\log(2\pi) - \frac{1}{2}\log|\Sigma_i| + \log P(\omega_i)$$
            </div>

            <p>
                The constant term $-\frac{d}{2}\log(2\pi)$ can be dropped as it's common to all classes.
            </p>

            <div class="professor-note">
                If this particular probability density function follows normal distribution, then what happens? We will see this in the next class. This is an area we will be covering‚Äîthe Gaussian discriminant function and how it relates to the decision boundaries.
            </div>

            <h3>5.3.5 Visualization of Bayesian Discriminant Functions</h3>
            <p>
                For Gaussian distributions, the discriminant functions create regions in the feature space. The decision boundaries are where the discriminant functions are equal.
            </p>

            <div class="diagram-placeholder">
                [Insert diagram: 3D plot showing two Gaussian posterior distributions P(œâ1|X) and P(œâ2|X) with decision boundaries marked where they intersect. Include projection showing regions R1 and R2 in 2D]
            </div>

            <p>
                The decision boundary divides the entire space into different regions:
                <ul style="padding-left: 25px; margin-top: 10px;">
                    <li><strong>Region R1:</strong> Points classified as class $\omega_1$</li>
                    <li><strong>Region R2:</strong> Points classified as class $\omega_2$</li>
                    <li><strong>Boundary:</strong> Points where $g_1(X) = g_2(X)$</li>
                </ul>
            </p>

            <div class="professor-note">
                If you plot this 3D structure to 2D, this is how the decision boundary looks like. This side of the boundary is going to be R2, this side is R1, and this is also R1; outside this is R2. Any point which lies in this region is going to be classified as R1, and any point which lies in this region is going to be classified as R2.
            </div>

            <h3>5.3.6 Properties of Gaussian Decision Boundaries</h3>
            <table>
                <tr>
                    <th>Covariance Structure</th>
                    <th>Decision Boundary Shape</th>
                    <th>Complexity</th>
                </tr>
                <tr>
                    <td>$\Sigma_1 = \Sigma_2 = \sigma^2 I$</td>
                    <td>Linear (hyperplane)</td>
                    <td>Low</td>
                </tr>
                <tr>
                    <td>$\Sigma_1 = \Sigma_2$ (equal, arbitrary)</td>
                    <td>Linear (hyperplane)</td>
                    <td>Medium</td>
                </tr>
                <tr>
                    <td>$\Sigma_1 \neq \Sigma_2$ (different)</td>
                    <td>Quadratic (curved)</td>
                    <td>High</td>
                </tr>
            </table>

            <div class="hinglish-summary">
                Bayesian discriminant function posterior probability hi hai. Hum P(X|œâi) √ó P(œâi) ko maximize karte hain, ya phir iska log le lete hain jo calculations ko easy banata hai. Jab likelihood Gaussian distribution follow karti hai, toh discriminant function ek specific form leta hai jisme mean (Œº) aur covariance (Œ£) involved hote hain. Decision boundaries tab banti hain jab do classes ke discriminant functions equal hote hain. Yeh boundaries linear ya quadratic ho sakti hain depending on covariance matrices pe.
            </div>

            <div class="key-takeaways">
                <h4>üéØ Key Takeaways</h4>
                <ul>
                    <li>Bayesian discriminant function = posterior probability $P(\omega_i | X)$</li>
                    <li>Can be simplified to $P(X | \omega_i) \cdot P(\omega_i)$ (ignoring P(X))</li>
                    <li>Log-likelihood form converts products to sums</li>
                    <li>For Gaussian distributions, get specific discriminant function form</li>
                    <li>Decision boundaries depend on mean and covariance of distributions</li>
                    <li>Equal covariances ‚Üí linear boundary; different covariances ‚Üí quadratic boundary</li>
                </ul>
            </div>

            <div class="practice-questions">
                <h4>üìù Practice Questions</h4>
                
                <div class="question">
                    <strong>Q1:</strong> What is a discriminant function and how is it used for classification?
                </div>
                <div class="answer">
                    A discriminant function is a real-valued function g(X) that takes a feature vector X as input and outputs a scalar value. For classification, we compute discriminant functions for all classes and assign the sample to the class with the highest discriminant function value. Mathematically, assign X to class œâi if gi(X) > gj(X) for all j ‚â† i.
                </div>

                <div class="question">
                    <strong>Q2:</strong> What is the difference between linear and non-linear discriminant functions?
                </div>
                <div class="answer">
                    Linear discriminant functions have the form g(X) = W^T X + b, creating straight line (2D) or hyperplane (higher dimensions) decision boundaries. Non-linear discriminant functions include higher-order terms like X^T A X, creating curved decision boundaries (circles, ellipses, parabolas). Linear functions work for linearly separable data, while non-linear functions can handle more complex, non-linearly separable patterns.
                </div>

                <div class="question">
                    <strong>Q3:</strong> Given two classes with equal covariance matrices, what shape will the decision boundary have and why?
                </div>
                <div class="answer">
                    The decision boundary will be linear (a hyperplane). When Œ£1 = Œ£2, the quadratic terms (X^T Œ£^(-1) X) in the discriminant functions cancel out when we set g1(X) = g2(X), leaving only linear terms. This results in a linear decision boundary regardless of the actual covariance structure, as long as both classes share the same covariance matrix.
                </div>

                <div class="question">
                    <strong>Q4:</strong> Why do we use log-likelihood instead of likelihood in Bayesian discriminant functions?
                </div>
                <div class="answer">
                    We use log-likelihood because: (1) It converts products into sums, which is computationally easier; (2) It prevents numerical underflow when dealing with very small probability values; (3) The logarithm is a monotonic function, so maximizing log P(X|œâi) is equivalent to maximizing P(X|œâi); (4) It simplifies taking derivatives for optimization algorithms.
                </div>

                <div class="question">
                    <strong>Q5:</strong> In a two-class problem, if g1(X) = 2X1 + 3X2 + 1 and g2(X) = X1 - X2 + 2, find the decision boundary equation.
                </div>
                <div class="answer">
                    The decision boundary is where g1(X) = g2(X):
                    2X1 + 3X2 + 1 = X1 - X2 + 2
                    2X1 - X1 + 3X2 + X2 = 2 - 1
                    X1 + 4X2 = 1
                    
                    So the decision boundary equation is: X1 + 4X2 - 1 = 0
                    
                    This is a straight line. Points satisfying X1 + 4X2 > 1 belong to class œâ1, and points with X1 + 4X2 < 1 belong to class œâ2.
                </div>
            </div>
        </section>

        <!-- Mind Map -->
        <section id="mind-map" class="mind-map">
            <h2>üß† Comprehensive Mind Map</h2>
            
            <div class="mind-map-container">
                <div class="main-topic">
                    PATTERN RECOGNITION PRINCIPLES
                </div>
                
                <div class="subtopics">
                    <div class="subtopic">
                        <h4>Bayesian Decision Theory</h4>
                        <ul>
                            <li>Bayes' Theorem</li>
                            <li>Posterior Probability</li>
                            <li>Likelihood √ó Prior</li>
                            <li>Maximum Posterior Rule</li>
                            <li>Optimal Classification</li>
                        </ul>
                    </div>
                    
                    <div class="subtopic">
                        <h4>Classification Error</h4>
                        <ul>
                            <li>Bayesian Error</li>
                            <li>Type I Error (FP)</li>
                            <li>Type II Error (FN)</li>
                            <li>Overlapping Distributions</li>
                            <li>Minimum Error Rate</li>
                        </ul>
                    </div>
                    
                    <div class="subtopic">
                        <h4>Risk Minimization</h4>
                        <ul>
                            <li>Loss Function</li>
                            <li>Conditional Risk</li>
                            <li>Bayes Risk</li>
                            <li>0-1 Loss</li>
                            <li>Cost-Sensitive Learning</li>
                        </ul>
                    </div>
                    
                    <div class="subtopic">
                        <h4>Discriminant Functions</h4>
                        <ul>
                            <li>Set of C functions</li>
                            <li>Real-valued output</li>
                            <li>Max function ‚Üí Class</li>
                            <li>d-dimensional input</li>
                            <li>Classification rule</li>
                        </ul>
                    </div>
                    
                    <div class="subtopic">
                        <h4>Linear Discriminants</h4>
                        <ul>
                            <li>g(X) = W^T X + b</li>
                            <li>Hyperplane equation</li>
                            <li>Linear boundary</li>
                            <li>Weight vector W</li>
                            <li>Bias term b</li>
                        </ul>
                    </div>
                    
                    <div class="subtopic">
                        <h4>Non-Linear Discriminants</h4>
                        <ul>
                            <li>Quadratic terms</li>
                            <li>g(X) = X^T A X + ...</li>
                            <li>Curved boundaries</li>
                            <li>Circles, ellipses</li>
                            <li>Complex patterns</li>
                        </ul>
                    </div>
                    
                    <div class="subtopic">
                        <h4>Bayesian Discriminants</h4>
                        <ul>
                            <li>g(X) = P(œâi|X)</li>
                            <li>Gaussian distributions</li>
                            <li>Log-likelihood</li>
                            <li>Mean & Covariance</li>
                            <li>Probabilistic framework</li>
                        </ul>
                    </div>
                    
                    <div class="subtopic">
                        <h4>Decision Boundaries</h4>
                        <ul>
                            <li>g1(X) = g2(X)</li>
                            <li>Separates classes</li>
                            <li>Linear or curved</li>
                            <li>Depends on functions</li>
                            <li>Classification regions</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- Summary Section -->
        <section style="margin-top: 50px; padding: 30px; background: linear-gradient(135deg, #667eea20 0%, #764ba220 100%); border-radius: 10px; border: 3px solid #667eea;">
            <h2>üìå Lecture Summary</h2>
            <p>
                In today's lecture, we covered two main topics: <strong>Bayesian Decision Theory</strong> and <strong>Discriminant Functions</strong>.
            </p>
            <p>
                <strong>Bayesian Decision Theory</strong> is based on Bayes' theorem. We assign a sample to a class based on the posterior probability P(œâi|X). The decision rule is simple: for a given sample, calculate the posterior for each class, and assign the class with the higher posterior. We also discussed Bayesian error (minimum achievable error) and risk (incorporating varying costs of different errors).
            </p>
            <p>
                We then moved to <strong>Discriminant Functions</strong>. For a C-class problem, we define C discriminant functions. For any sample, we evaluate these C functions, and whichever gives the maximum value, we assign that class. We discussed three types:
            </p>
            <ul style="padding-left: 25px; margin-top: 10px;">
                <li><strong>Linear Discriminant Functions:</strong> Create linear (straight line/hyperplane) decision boundaries</li>
                <li><strong>Non-Linear Discriminant Functions:</strong> Include quadratic terms, creating curved decision boundaries</li>
                <li><strong>Bayesian Discriminant Functions:</strong> Based on posterior probabilities, can be linear or quadratic depending on the covariance structure</li>
            </ul>
            <p>
                We also pictorially saw how discriminant functions and decision boundaries look in the case of Bayesian decision theory with Gaussian distributions.
            </p>
            <p style="margin-top: 20px; font-style: italic;">
                <strong>Next Lecture Preview:</strong> We will continue with discriminant functions in more detail, particularly Gaussian discriminant functions, and then move on to parameter estimation techniques.
            </p>
        </section>

        <footer style="text-align: center; margin-top: 50px; padding: 30px; background: #f8f9fa; border-radius: 10px;">
            <div class="footer-inner">
          <p class="footer-text">
            I created this knowledge during my Second semester of BSc in Applied
            AI and Data Science.
          </p>
          <p class="footer-author">~ Armaan Kachhawa</p>
        </div>
        </footer>
    </div>
</body>
</html>