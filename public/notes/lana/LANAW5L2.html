<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lecture 9: Symmetry, Determinants, and Inverse - Linear Algebra and Numerical Analysis</title>
    
    <!-- MathJax Configuration -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.8;
            color: #333;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.1);
        }
        
        .header {
            text-align: center;
            padding: 30px 0;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 10px;
            margin-bottom: 40px;
        }
        
        .header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        .header p {
            font-size: 1.2em;
            opacity: 0.95;
        }
        
        .toc {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
            border-left: 5px solid #667eea;
        }
        
        .toc h2 {
            color: #667eea;
            margin-bottom: 15px;
            font-size: 1.8em;
        }
        
        .toc ul {
            list-style: none;
        }
        
        .toc li {
            margin: 10px 0;
            padding-left: 20px;
        }
        
        .toc a {
            color: #495057;
            text-decoration: none;
            font-size: 1.1em;
            transition: all 0.3s ease;
        }
        
        .toc a:hover {
            color: #667eea;
            padding-left: 10px;
        }
        
        h2 {
            color: #667eea;
            margin: 40px 0 20px 0;
            font-size: 2em;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }
        
        h3 {
            color: #764ba2;
            margin: 30px 0 15px 0;
            font-size: 1.5em;
        }
        
        h4 {
            color: #495057;
            margin: 20px 0 10px 0;
            font-size: 1.2em;
        }
        
        p {
            margin: 15px 0;
            text-align: justify;
        }
        
        .key-term {
            background: linear-gradient(120deg, #ffd89b 0%, #19547b 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            font-weight: bold;
            font-size: 1.05em;
        }
        
        .professor-note {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        
        .professor-note strong {
            color: #856404;
        }
        
        .hinglish-summary {
            background: linear-gradient(135deg, #e0c3fc 0%, #8ec5fc 100%);
            padding: 20px;
            border-radius: 10px;
            margin: 25px 0;
            border-left: 5px solid #667eea;
        }
        
        .hinglish-summary h4 {
            color: #4a148c;
            margin-bottom: 10px;
            font-size: 1.3em;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            box-shadow: 0 2px 15px rgba(0,0,0,0.1);
            border-radius: 8px;
            overflow: hidden;
        }
        
        th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }
        
        td {
            padding: 12px 15px;
            border-bottom: 1px solid #dee2e6;
        }
        
        tr:hover {
            background: #f8f9fa;
        }
        
        .example-box {
            background: #e7f3ff;
            border: 2px solid #2196F3;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .example-box h4 {
            color: #1976D2;
            margin-bottom: 15px;
        }
        
        .solution-box {
            background: #e8f5e9;
            border: 2px solid #4CAF50;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .solution-box h4 {
            color: #2E7D32;
            margin-bottom: 15px;
        }
        
        .diagram-placeholder {
            background: #f1f3f5;
            border: 2px dashed #adb5bd;
            padding: 40px;
            text-align: center;
            margin: 20px 0;
            border-radius: 8px;
            color: #6c757d;
            font-style: italic;
        }
        
        .code-block {
            background: #282c34;
            color: #abb2bf;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            line-height: 1.6;
        }
        
        .code-block .keyword {
            color: #c678dd;
        }
        
        .code-block .function {
            color: #61afef;
        }
        
        .code-block .string {
            color: #98c379;
        }
        
        .code-block .comment {
            color: #5c6370;
            font-style: italic;
        }
        
        .practice-questions {
            background: #fff8e1;
            border: 2px solid #ffa726;
            padding: 25px;
            margin: 25px 0;
            border-radius: 10px;
        }
        
        .practice-questions h4 {
            color: #ef6c00;
            margin-bottom: 15px;
            font-size: 1.4em;
        }
        
        .practice-questions ol {
            padding-left: 25px;
        }
        
        .practice-questions li {
            margin: 15px 0;
        }
        
        .key-takeaways {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
        }
        
        .key-takeaways h4 {
            color: white;
            margin-bottom: 15px;
            font-size: 1.4em;
        }
        
        .key-takeaways ul {
            list-style-position: inside;
        }
        
        .key-takeaways li {
            margin: 10px 0;
            padding-left: 10px;
        }
        
        .mind-map {
            margin: 40px 0;
            padding: 30px;
            background: white;
            border-radius: 10px;
            box-shadow: 0 5px 25px rgba(0,0,0,0.1);
        }
        
        .mind-map h2 {
            text-align: center;
            color: #667eea;
            margin-bottom: 30px;
        }
        
        .mind-map-container {
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        
        .central-topic {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px 40px;
            border-radius: 50px;
            font-size: 1.5em;
            font-weight: bold;
            margin: 20px 0;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }
        
        .branches {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            width: 100%;
            margin-top: 30px;
        }
        
        .branch {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
            padding: 20px;
            border-radius: 15px;
            box-shadow: 0 3px 10px rgba(0,0,0,0.1);
        }
        
        .branch h4 {
            color: #5a2d0c;
            margin-bottom: 10px;
            font-size: 1.2em;
        }
        
        .branch ul {
            list-style: none;
            padding-left: 10px;
        }
        
        .branch li {
            margin: 8px 0;
            color: #333;
            position: relative;
            padding-left: 20px;
        }
        
        .branch li:before {
            content: "‚Üí";
            position: absolute;
            left: 0;
            color: #5a2d0c;
        }
        
        .matrix-notation {
            display: inline-block;
            padding: 5px 10px;
            background: #f8f9fa;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
        }
        
        @media print {
            body {
                background: white;
            }
            .container {
                box-shadow: none;
            }
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            h2 {
                font-size: 1.5em;
            }
            
            .branches {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Header Section -->
        <div class="header">
            <h1>Linear Algebra and Numerical Analysis notes</br>By Armaan Kachhawa</br>Week 5 Lecture 2 </h1>
        </div>
        
        <!-- Table of Contents -->
        <div class="toc">
            <h2>üìö Table of Contents</h2>
            <ul>
                <li><a href="#introduction">1. Introduction</a></li>
                <li><a href="#symmetric-square">2. Symmetric and Square Matrices</a>
                    <ul style="margin-left: 20px;">
                        <li><a href="#applications">2.1 Real-World Applications</a></li>
                        <li><a href="#definition">2.2 Definition of Symmetric Matrices</a></li>
                        <li><a href="#creating-symmetric">2.3 Creating Symmetric Matrices from Non-Symmetric Matrices</a></li>
                        <li><a href="#python-implementation">2.4 Python Implementation</a></li>
                    </ul>
                </li>
                <li><a href="#determinants">3. The Determinant of a Matrix</a>
                    <ul style="margin-left: 20px;">
                        <li><a href="#determinant-definition">3.1 Definition and Properties</a></li>
                        <li><a href="#computing-determinants">3.2 Computing Determinants</a></li>
                        <li><a href="#challenges">3.3 Computational Challenges</a></li>
                    </ul>
                </li>
                <li><a href="#inverse">4. Inverse of a Matrix (Introduction)</a></li>
                <li><a href="#recap">5. Lecture Recap</a></li>
                <li><a href="#practice">6. Practice Questions and Solutions</a></li>
                <li><a href="#mind-map">7. Comprehensive Mind Map</a></li>
            </ul>
        </div>
        
        <!-- Introduction Section -->
        <section id="introduction">
            <h2>1. Introduction</h2>
            <p>Welcome back to our lectures on <span class="key-term">Linear Algebra and Numerical Analysis</span>! In this module, we continue our exploration of matrix operations by focusing on three fundamental concepts that are crucial for advanced data science and machine learning applications.</p>
            
            <p>Today's lecture covers the following key topics:</p>
            <ul style="margin-left: 30px; margin-top: 15px;">
                <li><strong>Symmetric and Square Matrices</strong> - Understanding their properties and importance</li>
                <li><strong>Determinants</strong> - A scalar value that reveals important properties of matrices</li>
                <li><strong>Matrix Inverse</strong> - An introduction to the concept (detailed coverage in next module)</li>
                <li><strong>Rank of a Matrix</strong> - Brief introduction to this important property</li>
            </ul>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> These are very important properties of matrix operations and matrix algebra. They form the foundation for understanding more advanced concepts like eigenvalues, eigenvectors, and various decomposition techniques that we'll encounter later in the course.
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary (‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂)</h4>
                <p>Aaj hum matrix ke teen important concepts padhenge - symmetric matrices (jo apne transpose ke equal hote hain), determinants (ek scalar value jo matrix ke properties batati hai), aur matrix inverse ka introduction. Ye sab concepts data science aur machine learning mein bahut kaam aate hain, especially jab hum linear equations solve karte hain ya data analysis karte hain. In concepts ko samajhna bahut zaroori hai kyunki ye aage ke advanced topics ke liye foundation banate hain.</p>
            </div>
        </section>
        
        <!-- Symmetric and Square Matrices Section -->
        <section id="symmetric-square">
            <h2>2. Symmetric and Square Matrices</h2>
            
            <h3 id="applications">2.1 Real-World Applications of Symmetric Matrices</h3>
            <p>Before diving into the technical details, let's understand why <span class="key-term">symmetric matrices</span> are so important in practical applications. Symmetric matrices have many special properties that make them excellent tools for numerous real-world problems.</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Application Domain</th>
                        <th>How Symmetric Matrices Are Used</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Image Processing</strong></td>
                        <td>Covariance matrices (always symmetric) describe relationships between image pixel values</td>
                        <td>Image compression, feature extraction</td>
                    </tr>
                    <tr>
                        <td><strong>Machine Learning</strong></td>
                        <td>Covariance matrices describe relationships between variables in datasets</td>
                        <td>Principal Component Analysis (PCA), dimensionality reduction</td>
                    </tr>
                    <tr>
                        <td><strong>Kernel Methods</strong></td>
                        <td>Kernel matrices capture similarity between data points</td>
                        <td>Support Vector Machines (SVMs), kernel-based learning algorithms</td>
                    </tr>
                    <tr>
                        <td><strong>Network Analysis</strong></td>
                        <td>Adjacency matrices for undirected graphs are symmetric</td>
                        <td>Social networks, web graphs, railway/airport networks, communication systems</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> When dealing with undirected graphs (where there's no directionality), the adjacency matrices are symmetric. These can be utilized to analyze various interconnected systems like social networks, web graphs, railway networks, airport networks, communication networks, and so on. Symmetric matrices are fundamental and can be utilized in a variety of applications across data science and engineering.
            </div>
            
            <h3 id="definition">2.2 Definition of Symmetric Matrices</h3>
            <p>Now that we understand the importance, let's formally define what makes a matrix symmetric.</p>
            
            <h4>üìå Criteria for a Matrix to be Symmetric:</h4>
            <ol style="margin-left: 30px; margin-top: 15px;">
                <li><strong>Equal Rows and Columns:</strong> The matrix must be a <span class="key-term">square matrix</span>, meaning it has the same number of rows and columns ($m = n$).</li>
                <li><strong>Transpose Equality:</strong> The matrix must equal its transpose: $A = A^T$</li>
                <li><strong>Element Symmetry:</strong> Elements are mirrored across the main diagonal, i.e., $a_{ij} = a_{ji}$ for all $i, j$</li>
            </ol>
            
            <div class="example-box">
                <h4>üìù Example: 2√ó2 Symmetric Matrix</h4>
                <p>Consider a 2√ó2 matrix:</p>

                $$A = \begin{bmatrix} a & b \\ b & d \end{bmatrix}$$
                
                <p>Taking the transpose:</p>

                $$A^T = \begin{bmatrix} a & b \\ b & d \end{bmatrix}$$
                
                <p>Since $A = A^T$, this matrix is symmetric. Notice how:</p>
                <ul style="margin-left: 20px;">
                    <li>Element at position (1,2) = $b$ equals element at position (2,1) = $b$</li>
                    <li>The diagonal elements ($a$ and $d$) remain in the same position</li>
                    <li>We can "swap" rows and columns without changing the matrix</li>
                </ul>
            </div>
            
            <div class="example-box">
                <h4>üìù Example: General Symmetric Matrix</h4>
                <p>A general symmetric matrix looks like:</p>

                $$A = \begin{bmatrix} 
                a_{11} & a_{12} & a_{13} \\
                a_{12} & a_{22} & a_{23} \\
                a_{13} & a_{23} & a_{33}
                \end{bmatrix}$$
                
                <p>Notice the symmetry across the main diagonal (marked elements):</p>
                <ul style="margin-left: 20px;">
                    <li>$a_{12} = a_{21}$ (both are $a_{12}$)</li>
                    <li>$a_{13} = a_{31}$ (both are $a_{13}$)</li>
                    <li>$a_{23} = a_{32}$ (both are $a_{23}$)</li>
                </ul>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> When we have equal number of rows and columns, we call it a symmetric matrix. You can swap the rows with the columns. For example, if the first row is ABC, you can change it to ABC as an entry in the first column. When such kind of swapping is allowed without changing the matrix, then it is called a symmetric matrix.
            </div>
            
            <h3 id="creating-symmetric">2.3 Creating Symmetric Matrices from Non-Symmetric Matrices</h3>
            <p>This is a fascinating property: we can create <span class="key-term">symmetric matrices</span> from <span class="key-term">non-symmetric matrices</span>! Even from non-square matrices!</p>
            
            <h4>üéØ The Magic Formula</h4>
            <p>Given any matrix $A$ (even if it's not square or symmetric), we can create a symmetric matrix by:</p>
            <ul style="margin-left: 30px;">
                <li><strong>Method 1:</strong> $A^T A$ (transpose times original)</li>
                <li><strong>Method 2:</strong> $A A^T$ (original times transpose)</li>
            </ul>
            
            <h4>üìê Proof by Matrix Dimensions</h4>
            <p>Let's prove this works by examining the matrix sizes:</p>
            
            <div class="example-box">
                <h4>Step-by-Step Proof</h4>
                <p><strong>Given:</strong> Matrix $A$ is of size $M \times N$ (M rows, N columns)</p>
                
                <p><strong>For $A^T A$:</strong></p>
                <ul style="margin-left: 20px;">
                    <li>$A$ is $M \times N$</li>
                    <li>$A^T$ is $N \times M$ (transpose swaps dimensions)</li>
                    <li>$A^T A = (N \times M)(M \times N) = N \times N$ ‚úì (Square matrix!)</li>
                </ul>
                
                <p><strong>For $A A^T$:</strong></p>
                <ul style="margin-left: 20px;">
                    <li>$A$ is $M \times N$</li>
                    <li>$A^T$ is $N \times M$</li>
                    <li>$A A^T = (M \times N)(N \times M) = M \times M$ ‚úì (Square matrix!)</li>
                </ul>
                
                <p>So both products create square matrices, but are they symmetric?</p>
            </div>
            
            <h4>üìê Proof of Symmetry</h4>
            <p>Now let's prove these matrices are not just square, but also symmetric (equal to their transpose).</p>
            
            <div class="solution-box">
                <h4>Proving $(A^T A)$ is Symmetric</h4>
                <p>Recall that a symmetric matrix equals its transpose: $B = B^T$</p>
                
                <p>Let's transpose $A^T A$ and use the <span class="key-term">LIVE-EVIL rule</span> (palindrome rule for transposes):</p>

                
                $$\begin{align}
                (A^T A)^T &= A^T (A^T)^T \quad \text{(LIVE becomes EVIL)} \\
                &= A^T A \quad \text{(since } (A^T)^T = A\text{)}
                \end{align}$$
                
                <p>Since $(A^T A)^T = A^T A$, the matrix $A^T A$ is symmetric! ‚úì</p>
                
                <p><strong>The LIVE-EVIL Rule:</strong> When taking the transpose of a product, reverse the order and transpose each factor. Just like the palindrome LIVE ‚Üí EVIL!</p>

                $$(\text{LIVE})^T = (L \cdot I \cdot V \cdot E)^T = E^T \cdot V^T \cdot I^T \cdot L^T = \text{EVIL}$$
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> This is very nice because by using this principle, we can practically convert any non-symmetric matrices, or any non-square matrices, by simply taking the transpose and multiplying by that. We can use the same logic to create either $A^T A$ or $A A^T$. Both will give us square and symmetric matrices!
            </div>
            
            <div class="diagram-placeholder">
                [Insert diagram: Visual representation of converting a 2√ó3 non-square matrix to a 3√ó3 symmetric matrix via $A^T A$]
            </div>
            
            <h3 id="python-implementation">2.4 Python Implementation of Symmetric Matrices</h3>
            <p>Now let's see how we can work with symmetric matrices programmatically. We'll create Python functions to:</p>
            <ol style="margin-left: 30px;">
                <li>Generate a random symmetric matrix</li>
                <li>Check if a given matrix is symmetric</li>
            </ol>
            
            <h4>üéØ Problem Statement</h4>
            <p>Write a Python function that:</p>
            <ul style="margin-left: 30px;">
                <li><strong>Input:</strong> A matrix $A$ (as a NumPy array)</li>
                <li><strong>Output:</strong> A boolean value (True if symmetric, False otherwise)</li>
            </ul>
            
            <h4>üíª Code Implementation</h4>
            
            <div class="code-block">
<span class="keyword">import</span> numpy <span class="keyword">as</span> np</br>
</br>
<span class="comment"># Function to create a symmetric matrix</span></br>
<span class="keyword">def</span> <span class="function">create_symmetric_matrix</span>(size):</br>
    <span class="string">"""</br>
    Creates a random symmetric matrix of given size.</br>
    </br>
    Parameters:</br>
    size (int): The size of the square matrix (size √ó size)</br>
    </br>
    Returns:</br>
    numpy.ndarray: A symmetric matrix</br>
    """</span></br>
    <span class="comment"># Step 1: Initialize a square matrix with zeros</span></br>
    matrix = np.<span class="function">zeros</span>((size, size))</br>
    </br>
    <span class="comment"># Step 2: Iterate through upper triangle (including diagonal)</span></br>
    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="function">range</span>(size):</br>
        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="function">range</span>(i, size):  <span class="comment"># Start from i to ensure upper triangle</span></br>
            <span class="comment"># Step 3: Generate random integer</span></br>
            random_value = np.random.<span class="function">randint</span>(0, 10)</br>
            </br>
            <span class="comment"># Step 4: Assign to both (i,j) and (j,i) for symmetry</span></br>
            matrix[i, j] = random_value</br>
            matrix[j, i] = random_value  <span class="comment"># Mirror across diagonal</span></br>
    </br>
    <span class="keyword">return</span> matrix</br>
</br>
</br>
<span class="comment"># Function to check if a matrix is symmetric</span></br>
<span class="keyword">def</span> <span class="function">is_symmetric</span>(matrix):</br>
    <span class="string">"""</br>
    Checks whether a matrix is symmetric.</br>
    </br>
    Parameters:</br>
    matrix (numpy.ndarray): Input matrix to check</br>
    </br>
    Returns:</br>
    bool: True if symmetric (A = A^T), False otherwise</br>
    """</span></br>
    <span class="comment"># Compare matrix with its transpose using array_equal</span></br>
    <span class="keyword">return</span> np.<span class="function">array_equal</span>(matrix, matrix.T)</br>


<span class="comment"># Example usage</span></br>
<span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</br>
    <span class="comment"># Create a 4√ó4 symmetric matrix</span></br>
    sym_matrix = <span class="function">create_symmetric_matrix</span>(4)</br>
    <span class="function">print</span>(<span class="string">"Generated Symmetric Matrix:"</span>)</br>
    <span class="function">print</span>(sym_matrix)</br>
    </br>
    <span class="comment"># Verify it's symmetric</span></br>
    <span class="function">print</span>(<span class="string">f"\nIs symmetric? {<span class="function">is_symmetric</span>(sym_matrix)}"</span>)</br>
    </br>
    <span class="comment"># Test with a non-symmetric matrix</span></br>
    non_sym = np.<span class="function">array</span>([[1, 2], [3, 4]])</br>
    <span class="function">print</span>(<span class="string">f"\nIs [[1,2],[3,4]] symmetric? {<span class="function">is_symmetric</span>(non_sym)}"</span>)
            </div>
            
            <h4>üîç Code Explanation</h4>
            <table>
                <thead>
                    <tr>
                        <th>Function</th>
                        <th>What It Does</th>
                        <th>Key Steps</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>create_symmetric_matrix(size)</strong></td>
                        <td>Generates a random symmetric matrix</td>
                        <td>
                            1. Initialize with zeros<br>
                            2. Loop through upper triangle<br>
                            3. Generate random values<br>
                            4. Assign to both (i,j) and (j,i)<br>
                            5. Return symmetric matrix
                        </td>
                    </tr>
                    <tr>
                        <td><strong>is_symmetric(matrix)</strong></td>
                        <td>Checks if matrix equals its transpose</td>
                        <td>
                            1. Take transpose of matrix<br>
                            2. Compare with original using np.array_equal()<br>
                            3. Return True/False
                        </td>
                    </tr>
                </tbody>
            </table>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> When we operationalize this in Python, we create self-consistent steps. We iterate through the upper triangle of the matrix (including the main diagonal), generate random integers, and assign them to both $a_{ij}$ and $a_{ji}$ to ensure symmetry. The litmus test is checking if $A = A^T$ using NumPy's array_equal function.
            </div>
            
            <div class="diagram-placeholder">
                [Insert diagram: Visual representation of upper triangle, lower triangle, and main diagonal in a matrix]
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary (‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂)</h4>
                <p>Symmetric matrix woh hoti hai jismein rows aur columns equal hote hain, matlab square matrix honi chahiye. Iska matlab hai ki agar aap matrix ko transpose karo toh woh same hi rahegi ($A = A^T$). Sabse interesting baat yeh hai ki hum kisi bhi non-symmetric ya non-square matrix se symmetric matrix bana sakte hain, bas $A^T A$ ya $A A^T$ multiply kar do! Python mein hum upper triangle ke through loop karke aur same value ko (i,j) aur (j,i) dono positions pe assign karke symmetric matrix create kar sakte hain.</p>
            </div>
            
            <div class="practice-questions">
                <h4>üìù Practice Question 1</h4>
                <p><strong>Question:</strong> Show that the product of the following matrix and its transpose is symmetric.</p>

                $$A = \begin{bmatrix} 1 & -2 & 4 \\ 3 & 0 & -5 \end{bmatrix}$$
                
                <p style="margin-top: 20px;"><strong>Solution:</strong></p>
                <p>First, let's find the transpose of A:</p>

                $$A^T = \begin{bmatrix} 1 & 3 \\ -2 & 0 \\ 4 & -5 \end{bmatrix}$$
                
                <p><strong>Computing $A^T A$:</strong></p>

                $$A^T A = \begin{bmatrix} 1 & 3 \\ -2 & 0 \\ 4 & -5 \end{bmatrix} \begin{bmatrix} 1 & -2 & 4 \\ 3 & 0 & -5 \end{bmatrix}$$

                
                $$= \begin{bmatrix} 
                1(1)+3(3) & 1(-2)+3(0) & 1(4)+3(-5) \\
                -2(1)+0(3) & -2(-2)+0(0) & -2(4)+0(-5) \\
                4(1)+(-5)(3) & 4(-2)+(-5)(0) & 4(4)+(-5)(-5)
                \end{bmatrix}$$

                
                $$= \begin{bmatrix} 10 & -2 & -11 \\ -2 & 4 & -8 \\ -11 & -8 & 41 \end{bmatrix}$$
                
                <p>Notice this is a 3√ó3 matrix. Let's verify it's symmetric by checking $A^T A = (A^T A)^T$:</p>
                <ul style="margin-left: 20px;">
                    <li>Element at (1,2) = -2, Element at (2,1) = -2 ‚úì</li>
                    <li>Element at (1,3) = -11, Element at (3,1) = -11 ‚úì</li>
                    <li>Element at (2,3) = -8, Element at (3,2) = -8 ‚úì</li>
                </ul>
                
                <p><strong>Computing $A A^T$:</strong></p>

                $$A A^T = \begin{bmatrix} 1 & -2 & 4 \\ 3 & 0 & -5 \end{bmatrix} \begin{bmatrix} 1 & 3 \\ -2 & 0 \\ 4 & -5 \end{bmatrix}$$

                
                $$= \begin{bmatrix} 
                1(1)+(-2)(-2)+4(4) & 1(3)+(-2)(0)+4(-5) \\
                3(1)+0(-2)+(-5)(4) & 3(3)+0(0)+(-5)(-5)
                \end{bmatrix}$$

                
                $$= \begin{bmatrix} 21 & -17 \\ -17 & 34 \end{bmatrix}$$
                
                <p>This is a 2√ó2 symmetric matrix (verify: (1,2) = (2,1) = -17) ‚úì</p>
                
                <p><strong>Conclusion:</strong> Both $A^T A$ and $A A^T$ are symmetric as expected! Starting with a non-square 2√ó3 matrix, we created two different symmetric square matrices.</p>
            </div>
            
            <div class="key-takeaways">
                <h4>üéØ Key Takeaways: Symmetric and Square Matrices</h4>
                <ul>
                    <li><strong>Square matrices</strong> have equal numbers of rows and columns ($n \times n$)</li>
                    <li><strong>Symmetric matrices</strong> are square matrices that equal their transpose: $A = A^T$</li>
                    <li>Elements in symmetric matrices are <strong>mirrored across the main diagonal</strong>: $a_{ij} = a_{ji}$</li>
                    <li>All symmetric matrices are square, but <strong>not all square matrices are symmetric</strong></li>
                    <li>You can create symmetric matrices from any matrix using $A^T A$ or $A A^T$</li>
                    <li>Symmetric matrices have <strong>real eigenvalues</strong> and are crucial in PCA, covariance analysis, and kernel methods</li>
                    <li>The LIVE-EVIL rule helps prove transpose properties: $(AB)^T = B^T A^T$</li>
                </ul>
            </div>
        </section>
        
        <!-- Determinants Section -->
        <section id="determinants">
            <h2>3. The Determinant of a Matrix</h2>
            
            <h3 id="determinant-definition">3.1 Definition and Properties</h3>
            <p>Before we can compute the <span class="key-term">inverse of a matrix</span>, we need to understand another crucial concept: the <span class="key-term">determinant</span>. The determinant is intrinsically related to matrix inversion and tells us important properties about the matrix.</p>
            
            <h4>üìå What is a Determinant?</h4>
            <p>Each <span class="key-term">square matrix</span> $A$ has a unique <span class="key-term">scalar value</span> called its determinant, which we denote as:</p>
            <ul style="margin-left: 30px;">
                <li>$\det(A)$ (function notation)</li>
                <li>$|A|$ (determinant bars notation)</li>
            </ul>
            
            <p>For example, if we have a matrix:</p>

            $$A = \begin{bmatrix} 1 & 2 \\ 6 & 5 \end{bmatrix}$$
            
            <p>We can write its determinant as:</p>

            $$\det(A) = \begin{vmatrix} 1 & 2 \\ 6 & 5 \end{vmatrix}$$
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> The determinant is a unit scalar value associated with every square matrix. It's written with two vertical bars on each side, or sometimes we write it as det(A) in acronym form. This is a very important property because the determinant helps us understand whether a matrix is invertible or non-invertible.
            </div>
            
            <h4>üîë Key Properties of Determinants</h4>
            <table>
                <thead>
                    <tr>
                        <th>Property</th>
                        <th>Description</th>
                        <th>Implication</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Scalar Value</strong></td>
                        <td>The determinant is always a single number</td>
                        <td>Can be positive, negative, zero, integer, or decimal</td>
                    </tr>
                    <tr>
                        <td><strong>Square Matrices Only</strong></td>
                        <td>Only defined for square matrices ($n \times n$)</td>
                        <td>Cannot compute determinant of 2√ó3 or 3√ó5 matrices</td>
                    </tr>
                    <tr>
                        <td><strong>Invertibility Indicator</strong></td>
                        <td>If $\det(A) \neq 0$, matrix is invertible</td>
                        <td>Zero determinant means matrix is singular (non-invertible)</td>
                    </tr>
                    <tr>
                        <td><strong>Real Value</strong></td>
                        <td>For real-valued matrices, determinant is real</td>
                        <td>Output is in the same number system as matrix entries</td>
                    </tr>
                </tbody>
            </table>
            
            <h4>üìê Special Cases</h4>
            
            <div class="example-box">
                <h4>Case 1: 1√ó1 Matrix (Single Element)</h4>
                <p>If $A = [a_{11}]$ is a 1√ó1 matrix (just one element), then:</p>

                $$\det(A) = a_{11}$$
                
                <p><strong>Example:</strong> If $A = [7]$, then $\det(A) = 7$</p>
            </div>
            
            <div class="example-box">
                <h4>Case 2: General $n \times n$ Matrix</h4>
                <p>If $A$ is an $n \times n$ matrix, its determinant is defined in terms of smaller determinants of order $(n-1) \times (n-1)$ or even smaller.</p>
                
                <p>This recursive definition means:</p>
                <ul style="margin-left: 20px;">
                    <li>A 3√ó3 determinant is computed using three 2√ó2 determinants</li>
                    <li>A 4√ó4 determinant uses four 3√ó3 determinants</li>
                    <li>And so on...</li>
                </ul>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> If A is $n \times n$, its determinant may be defined not exceeding the maximum size $n$, but it has to be of the order $n-1$ or even lesser. This recursive property will become more clear as we move forward and see examples.
            </div>
            
            <h3 id="computing-determinants">3.2 Computing Determinants</h3>
            
            <h4>üìå The 2√ó2 Case (Easy!)</h4>
            <p>For a 2√ó2 matrix, computing the determinant is straightforward:</p>
            
            <div class="example-box">
                <h4>Formula for 2√ó2 Determinant</h4>
                <p>Given:</p>

                $$A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$$
                
                <p>The determinant is:</p>

                $$\det(A) = \begin{vmatrix} a & b \\ c & d \end{vmatrix} = ad - bc$$
                
                <p><strong>Method:</strong> Cross-multiply diagonally!</p>
                <ul style="margin-left: 20px;">
                    <li>Multiply main diagonal: $a \times d$</li>
                    <li>Multiply anti-diagonal: $b \times c$</li>
                    <li>Subtract: $ad - bc$</li>
                </ul>
            </div>
            
            <div class="diagram-placeholder">
                [Insert diagram: Visual representation of 2√ó2 determinant calculation showing diagonal multiplication arrows]
            </div>
            
            <div class="example-box">
                <h4>üìù Example: Computing a 2√ó2 Determinant</h4>
                <p>Find the determinant of:</p>

                $$A = \begin{bmatrix} 3 & 7 \\ 2 & 5 \end{bmatrix}$$
                
                <p><strong>Solution:</strong></p>

                $$\det(A) = (3)(5) - (7)(2) = 15 - 14 = 1$$
                
                <p>Since $\det(A) = 1 \neq 0$, this matrix is <strong>invertible</strong>! ‚úì</p>
            </div>
            
            <h4>üìå The 3√ó3 Case (More Complex)</h4>
            <p>For a 3√ó3 matrix, we use <span class="key-term">cofactor expansion</span> (also called Laplace expansion):</p>
            
            <div class="example-box">
                <h4>Formula for 3√ó3 Determinant</h4>
                <p>Given:</p>

                $$A = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix}$$
                
                <p>Expanding along the first row:</p>

                $$\det(A) = a\begin{vmatrix} e & f \\ h & i \end{vmatrix} - b\begin{vmatrix} d & f \\ g & i \end{vmatrix} + c\begin{vmatrix} d & e \\ g & h \end{vmatrix}$$

                
                $$= a(ei - fh) - b(di - fg) + c(dh - eg)$$
                
                <p><strong>Pattern:</strong> Notice the alternating signs (+, -, +)</p>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> We can calculate the determinant of a 3√ó3 matrix by taking the determinant of smaller 2√ó2 matrices. We take the first element, multiply it by the determinant of the remaining 2√ó2 matrix (after removing its row and column), then subtract the second element times its 2√ó2 determinant, and add the third element times its 2√ó2 determinant. The alternating plus-minus pattern is crucial!
            </div>
            
            <h3 id="challenges">3.3 Computational Challenges</h3>
            
            <h4>‚ö†Ô∏è The 4√ó4 Problem (and Beyond)</h4>
            <p>Once you get to 4√ó4 matrices, determinant calculations become extremely tedious and time-consuming, especially if the matrix has many non-zero entries and lacks strategically placed zeros.</p>
            
            <div class="example-box">
                <h4>4√ó4 Matrix Determinant</h4>
                <p>Consider a general 4√ó4 matrix:</p>

                $$A = \begin{bmatrix} 
                a & b & c & d \\
                e & f & g & h \\
                i & j & k & l \\
                m & n & o & p
                \end{bmatrix}$$
                
                <p>The determinant formula involves:</p>
                <ul style="margin-left: 20px;">
                    <li>Expanding along one row or column</li>
                    <li>Computing <strong>four 3√ó3 determinants</strong></li>
                    <li>Each 3√ó3 determinant requires <strong>three 2√ó2 determinants</strong></li>
                    <li>Total: <strong>12 different 2√ó2 calculations!</strong></li>
                </ul>
                
                <p style="color: #d32f2f; font-weight: bold;">Without zeros, this is an arduous manual task!</p>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> The equation below shows the determinant of a 4√ó4 matrix. Yeah, good luck with that! Computing these will take a huge amount of manual labor. If there are carefully placed zeros, computations may be possible, but if not, it becomes extremely difficult. That's why we use computational tools!
            </div>
            
            <h4>üíª Computational Solution: NumPy to the Rescue!</h4>
            <p>For large matrices, we <strong>always</strong> use computational libraries rather than manual calculation:</p>
            
            <div class="code-block">
<span class="keyword">import</span> numpy <span class="keyword">as</span> np</br>
</br>
<span class="comment"># Define a large matrix</span></br>
A = np.<span class="function">array</span>([</br>
    [1, 2, 3, 4],</br>
    [5, 6, 7, 8],</br>
    [9, 10, 11, 12],</br>
    [13, 14, 15, 16]</br>
])</br>
</br>
<span class="comment"># Compute determinant using NumPy</span></br>
det_A = np.linalg.<span class="function">det</span>(A)</br>
<span class="function">print</span>(<span class="string">f"Determinant of A: {det_A}"</span>)</br>
</br>
<span class="comment"># Or using SciPy</span></br>
<span class="keyword">from</span> scipy <span class="keyword">import</span> linalg</br>
det_A_scipy = linalg.<span class="function">det</span>(A)</br>
<span class="function">print</span>(<span class="string">f"Determinant (SciPy): {det_A_scipy}"</span>)
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> Estimating determinants and decompositions come under numerical analysis. As we move into the last module on numerical analysis and stability of matrices, it will be very interesting. But the point is, if you ever need to compute the determinant for any large matrix, you always have NumPy libraries. You can use linear algebra mathematical functions in NumPy and call the determinant function. That way you can save a lot of time and energy!
            </div>
            
            <h4>üéØ What Does the Determinant Tell Us?</h4>
            <p>The determinant value provides important information about the matrix:</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Determinant Value</th>
                        <th>Meaning</th>
                        <th>Implications</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>$\det(A) > 0$</td>
                        <td>Positive determinant</td>
                        <td>Matrix is invertible; preserves orientation in transformations</td>
                    </tr>
                    <tr>
                        <td>$\det(A) < 0$</td>
                        <td>Negative determinant</td>
                        <td>Matrix is invertible; reverses orientation in transformations</td>
                    </tr>
                    <tr>
                        <td>$\det(A) = 0$</td>
                        <td>Zero determinant (singular)</td>
                        <td><strong>Matrix is NOT invertible;</strong> columns/rows are linearly dependent</td>
                    </tr>
                    <tr>
                        <td>$|\det(A)| > 1$</td>
                        <td>Absolute value > 1</td>
                        <td>Transformation expands volumes</td>
                    </tr>
                    <tr>
                        <td>$|\det(A)| < 1$</td>
                        <td>Absolute value < 1</td>
                        <td>Transformation contracts volumes</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> You can see that the determinant is not limited to integers or positive values per se. Depending on the numerical values in the matrix, the determinant can be decimals, negative numbers, or any other number. The determinant will always be a real number for a real-valued matrix. There could be scenarios where you have zero determinant value, and scenarios where you have non-zero determinants. These properties are used in a variety of data science, ML, and data engineering applications.
            </div>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary (‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂)</h4>
                <p>Determinant ek scalar value hai jo sirf square matrices ke liye define hoti hai. Ye matrix ke bahut important properties batata hai, specially ye ki matrix invertible hai ya nahi. 2√ó2 matrix ka determinant nikalna easy hai - bas $ad - bc$ karo. Lekin jab matrix size badhta hai (3√ó3, 4√ó4, etc.), tab calculation bahut mushkil ho jati hai aur manual calculation mein bahut time lagta hai. Isliye hum NumPy library use karte hain jo instantly determinant calculate kar deti hai. Agar determinant zero hai, toh matrix non-invertible hai; agar non-zero hai toh invertible hai!</p>
            </div>
            
            <div class="practice-questions">
                <h4>üìù Practice Question 2</h4>
                <p><strong>Question:</strong> Find the determinant of the 3√ó3 matrix given below:</p>

                $$A = \begin{bmatrix} 1 & 2 & 1 \\ 0 & 3 & 4 \\ 3 & 1 & 4 \end{bmatrix}$$
                
                <p style="margin-top: 20px;"><strong>Solution:</strong></p>
                <p>We'll expand along the first row using cofactor expansion:</p>

                
                $$\det(A) = 1 \cdot \begin{vmatrix} 3 & 4 \\ 1 & 4 \end{vmatrix} - 2 \cdot \begin{vmatrix} 0 & 4 \\ 3 & 4 \end{vmatrix} + 1 \cdot \begin{vmatrix} 0 & 3 \\ 3 & 1 \end{vmatrix}$$
                
                <p>Now compute each 2√ó2 determinant:</p>
                
                <p><strong>First term:</strong></p>

                $$1 \cdot \begin{vmatrix} 3 & 4 \\ 1 & 4 \end{vmatrix} = 1 \cdot (3 \times 4 - 4 \times 1) = 1 \cdot (12 - 4) = 1 \cdot 8 = 8$$
                
                <p><strong>Second term:</strong></p>

                $$-2 \cdot \begin{vmatrix} 0 & 4 \\ 3 & 4 \end{vmatrix} = -2 \cdot (0 \times 4 - 4 \times 3) = -2 \cdot (0 - 12) = -2 \cdot (-12) = 24$$
                
                <p><strong>Third term:</strong></p>

                $$1 \cdot \begin{vmatrix} 0 & 3 \\ 3 & 1 \end{vmatrix} = 1 \cdot (0 \times 1 - 3 \times 3) = 1 \cdot (0 - 9) = 1 \cdot (-9) = -9$$
                
                <p><strong>Final answer:</strong></p>

                $$\det(A) = 8 + 24 + (-9) = 8 + 24 - 9 = 23$$
                
                <p style="color: #2E7D32; font-weight: bold;">Since $\det(A) = 23 \neq 0$, this matrix is invertible! ‚úì</p>
            </div>
            
            <div class="key-takeaways">
                <h4>üéØ Key Takeaways: Determinants</h4>
                <ul>
                    <li>The <strong>determinant</strong> is a scalar value associated with every square matrix</li>
                    <li>Notation: $\det(A)$ or $|A|$ (with vertical bars)</li>
                    <li>For 2√ó2 matrices: $\det(A) = ad - bc$ (cross-multiply diagonally)</li>
                    <li>For 3√ó3 and larger: Use <strong>cofactor expansion</strong> with alternating signs</li>
                    <li>Computing large determinants manually is <strong>extremely time-consuming</strong></li>
                    <li>Use <strong>NumPy or SciPy</strong> libraries for computational efficiency: <code>np.linalg.det()</code></li>
                    <li>Determinant tells us about <strong>invertibility</strong>: $\det(A) \neq 0$ means invertible</li>
                    <li>Zero determinant ($\det(A) = 0$) means matrix is <strong>singular</strong> (non-invertible)</li>
                    <li>Determinant values can be positive, negative, decimal, or zero - any real number</li>
                </ul>
            </div>
        </section>
        
        <!-- Inverse of a Matrix Section -->
        <section id="inverse">
            <h2>4. Inverse of a Matrix (Introduction)</h2>
            
            <h3>4.1 Real-World Applications of Matrix Inverse</h3>
            <p>Now that we understand determinants, let's briefly introduce the concept of <span class="key-term">matrix inverse</span>, which will be covered in more detail in the next module (Module 2.3).</p>
            
            <h4>üåç Why Matrix Inverse Matters</h4>
            <p>The inverse of a matrix has tremendous applications in solving real-world problems, particularly in data science and engineering:</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Application Area</th>
                        <th>How Matrix Inverse Is Used</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Solving Linear Systems</strong></td>
                        <td>Fundamental use: solving $Ax = b$ becomes $x = A^{-1}b$</td>
                        <td>Finding unknown parameters in linear models</td>
                    </tr>
                    <tr>
                        <td><strong>Statistical Modeling</strong></td>
                        <td>Computing regression coefficients, parameter estimation</td>
                        <td>Linear regression: $\beta = (X^T X)^{-1} X^T y$</td>
                    </tr>
                    <tr>
                        <td><strong>Data Engineering</strong></td>
                        <td>Transforming data, solving optimization problems</td>
                        <td>Feature transformation, dimensionality changes</td>
                    </tr>
                    <tr>
                        <td><strong>Control Systems</strong></td>
                        <td>System stability analysis, feedback control</td>
                        <td>Robotics, autonomous vehicles</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> Matrix inverse has a tremendous amount of real-world applications. Any kind of linear system or linear equations that we would like to solve eventually during this course requires a fundamental use of matrix inverses. In data engineering particularly, when we deal with linear models that represent relationships between different variables - for example, in certain types of statistical modeling - you might need to solve for unknown parameters in a linear system, and matrix inversion may be a go-to operational procedure in matrix algebra.
            </div>
            
            <h4>‚ö†Ô∏è Computational Considerations</h4>
            <p>While matrix inverse is theoretically elegant, there are practical challenges:</p>
            
            <div class="example-box">
                <h4>Challenges with Matrix Inversion</h4>
                <ul style="margin-left: 20px;">
                    <li><strong>Computational Cost:</strong> Calculating the inverse of large matrices can be computationally expensive</li>
                    <li><strong>Massive Datasets:</strong> In data engineering, datasets can be really massive, making direct inversion impractical</li>
                    <li><strong>Numerical Stability:</strong> Small errors can be amplified when computing inverses</li>
                    <li><strong>Alternative Methods:</strong> Decomposition techniques (LU, QR, SVD) are often preferred</li>
                </ul>
                
                <p style="margin-top: 15px;"><strong>Best Practice:</strong> Matrix inversion is most helpful when dealing with <strong>smaller matrices</strong>. For large-scale problems, use decomposition techniques!</p>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> Estimating inverse of large matrices can be computationally sometimes very expensive, particularly in data engineering where datasets can be really massive in real-world applications. Alternative methods for solving linear systems like some decomposition techniques - LU decomposition, SVD (singular value decomposition) - which we have not yet covered but eventually will, are often preferred. But matrix inversion is really helpful when you're dealing with smaller matrices, and that is probably the go-to method used in data engineering for a variety of applications.
            </div>
            
            <h4>üîó Connection to Determinants</h4>
            <p>Here's the crucial connection: <strong>The determinant is required to compute the inverse of a matrix!</strong></p>
            
            <div class="example-box">
                <h4>Invertibility Condition</h4>
                <p>A matrix $A$ has an inverse $A^{-1}$ if and only if:</p>

                $$\det(A) \neq 0$$
                
                <ul style="margin-left: 20px; margin-top: 15px;">
                    <li>If $\det(A) \neq 0$: Matrix is <strong>invertible</strong> (non-singular)</li>
                    <li>If $\det(A) = 0$: Matrix is <strong>non-invertible</strong> (singular)</li>
                </ul>
                
                <p style="margin-top: 15px;"><strong>This is why we studied determinants first!</strong></p>
            </div>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> Before we compute the inverse of a matrix, we need one more concept which is very important for us to know, which is called the determinant, because the determinant is intrinsically related to computing the inverse of a matrix. When we look at determinants, we need to know whether the matrix is invertible or non-invertible. There could be scenarios where you have zero determinant value, and scenarios where you have non-zero determinants.
            </div>
            
            <h4>üìù Preview: Rank of a Matrix</h4>
            <p>Another important concept related to invertibility is the <span class="key-term">rank of a matrix</span>, which we'll cover in the next module along with:</p>
            <ul style="margin-left: 30px;">
                <li><strong>Matrix Rank:</strong> The dimension of the column space (or row space)</li>
                <li><strong>Linear Independence:</strong> Understanding when vectors are independent</li>
                <li><strong>Linear Dependence:</strong> When vectors can be expressed as combinations of others</li>
            </ul>
            
            <div class="hinglish-summary">
                <h4>üéØ Hinglish Summary (‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂)</h4>
                <p>Matrix inverse bahut important hai linear equations solve karne ke liye. Jaise normal numbers mein $5 \times x = 10$ ko solve karne ke liye hum dono sides ko 5 se divide karte hain (ya $5^{-1}$ se multiply karte hain), waise hi matrices ke liye $Ax = b$ solve karne ke liye hum $A^{-1}$ use karte hain. Lekin inverse compute karna computationally expensive hai, especially large matrices ke liye. Isliye real-world applications mein hum LU decomposition ya SVD jaise alternative methods prefer karte hain. Ek matrix ka inverse tab hi exist karta hai jab uska determinant zero na ho!</p>
            </div>
            
            <div class="key-takeaways">
                <h4>üéØ Key Takeaways: Matrix Inverse (Introduction)</h4>
                <ul>
                    <li><strong>Matrix inverse</strong> is fundamental for solving linear systems: $Ax = b \Rightarrow x = A^{-1}b$</li>
                    <li>Used extensively in <strong>statistical modeling</strong> and data engineering</li>
                    <li><strong>Computational cost</strong> is high for large matrices</li>
                    <li>Alternative methods (LU, QR, SVD decompositions) often preferred for large-scale problems</li>
                    <li><strong>Invertibility condition:</strong> $\det(A) \neq 0$ (non-zero determinant)</li>
                    <li>If $\det(A) = 0$, matrix is <strong>singular</strong> (no inverse exists)</li>
                    <li>Best for <strong>smaller matrices</strong> in practical applications</li>
                    <li>Next module will cover detailed computation methods and rank of matrices</li>
                </ul>
            </div>
        </section>
        
        <!-- Recap Section -->
        <section id="recap">
            <h2>5. Lecture Recap</h2>
            
            <div class="key-takeaways" style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);">
                <h4>üìö What We Covered Today</h4>
                <ul>
                    <li><strong>Square Matrices:</strong> Matrices with equal rows and columns ($n \times n$) - essential for determinants and inverses</li>
                    <li><strong>Symmetric Matrices:</strong> Square matrices equal to their transpose ($A = A^T$) with elements mirrored across the main diagonal</li>
                    <li><strong>Creating Symmetric Matrices:</strong> Any matrix multiplied by its transpose produces a symmetric matrix ($A^T A$ or $A A^T$)</li>
                    <li><strong>Determinants:</strong> Scalar values computed only for square matrices that indicate invertibility (non-zero = invertible)</li>
                    <li><strong>Computational Reality:</strong> Manual calculation of large determinants is tedious - use NumPy/SciPy libraries</li>
                    <li><strong>Matrix Inverse Preview:</strong> Required for solving linear systems; needs non-zero determinant</li>
                    <li><strong>Real-World Applications:</strong> PCA, covariance analysis, network analysis, SVMs, kernel methods, statistical modeling</li>
                </ul>
            </div>
            
            <h3>üîú Coming Up Next: Module 2.3</h3>
            <p>In the next lecture, we'll dive deeper into:</p>
            <ul style="margin-left: 30px; margin-top: 15px;">
                <li><strong>Matrix Algebra:</strong> Complete coverage of matrix inverse computation</li>
                <li><strong>Rank of a Matrix:</strong> Understanding dimension and span</li>
                <li><strong>Linear Independence:</strong> When vectors are truly independent</li>
                <li><strong>Linear Dependence:</strong> Identifying redundancy in vector sets</li>
                <li><strong>Practical Applications:</strong> Solving systems of linear equations</li>
            </ul>
            
            <div class="professor-note">
                <strong>Professor mentioned in class:</strong> When we go into understanding concepts like eigenvectors and eigenvalues, and introduce those terms, we'll understand the utility of symmetric matrices even better. All symmetric matrices are square matrices. The determinant is a property of a square matrix, and we need to know determinants to understand whether the matrix is invertible or non-invertible. These concepts are used in a variety of data science, ML, and data engineering applications. Thank you for your attention, we'll return soon!
            </div>
        </section>
        
        <!-- Practice Questions Section -->
        <section id="practice">
            <h2>6. Practice Questions and Solutions</h2>
            
            <div class="practice-questions">
                <h4>üìù Additional Practice Questions</h4>
                
                <p><strong>Question 3:</strong> Verify that the following matrix is symmetric:</p>

                $$A = \begin{bmatrix} 5 & -2 & 3 \\ -2 & 8 & 1 \\ 3 & 1 & 4 \end{bmatrix}$$
                
                <div class="solution-box" style="margin-top: 15px;">
                    <h4>Solution:</h4>
                    <p>To verify symmetry, we check if $A = A^T$ (or equivalently, if $a_{ij} = a_{ji}$ for all $i, j$)</p>
                    
                    <p>First, let's write out the transpose:</p>

                    $$A^T = \begin{bmatrix} 5 & -2 & 3 \\ -2 & 8 & 1 \\ 3 & 1 & 4 \end{bmatrix}$$
                    
                    <p>Comparing elements:</p>
                    <ul style="margin-left: 20px;">
                        <li>$a_{12} = -2$ and $a_{21} = -2$ ‚úì</li>
                        <li>$a_{13} = 3$ and $a_{31} = 3$ ‚úì</li>
                        <li>$a_{23} = 1$ and $a_{32} = 1$ ‚úì</li>
                        <li>Diagonal elements remain the same: $a_{11} = 5, a_{22} = 8, a_{33} = 4$ ‚úì</li>
                    </ul>
                    
                    <p style="color: #2E7D32; font-weight: bold;">Since $A = A^T$, the matrix is symmetric! ‚úì</p>
                </div>
                
                <hr style="margin: 30px 0;">
                
                <p><strong>Question 4:</strong> Calculate the determinant of the following 2√ó2 matrix:</p>

                $$B = \begin{bmatrix} 4 & -3 \\ 2 & 1 \end{bmatrix}$$
                
                <div class="solution-box" style="margin-top: 15px;">
                    <h4>Solution:</h4>
                    <p>Using the formula for 2√ó2 determinants: $\det(B) = ad - bc$</p>
                    
                    <p>Where $a = 4, b = -3, c = 2, d = 1$</p>

                    
                    $$\det(B) = (4)(1) - (-3)(2) = 4 - (-6) = 4 + 6 = 10$$
                    
                    <p style="color: #2E7D32; font-weight: bold;">Answer: $\det(B) = 10$ (non-zero, so matrix is invertible) ‚úì</p>
                </div>
                
                <hr style="margin: 30px 0;">
                
                <p><strong>Question 5:</strong> Given matrix $C = \begin{bmatrix} 1 & 4 \\ 2 & 3 \end{bmatrix}$, compute $C^T C$ and verify it's symmetric.</p>
                
                <div class="solution-box" style="margin-top: 15px;">
                    <h4>Solution:</h4>
                    <p><strong>Step 1:</strong> Find $C^T$</p>

                    $$C^T = \begin{bmatrix} 1 & 2 \\ 4 & 3 \end{bmatrix}$$
                    
                    <p><strong>Step 2:</strong> Compute $C^T C$</p>

                    $$C^T C = \begin{bmatrix} 1 & 2 \\ 4 & 3 \end{bmatrix} \begin{bmatrix} 1 & 4 \\ 2 & 3 \end{bmatrix}$$

                    
                    $$= \begin{bmatrix} 
                    1(1)+2(2) & 1(4)+2(3) \\
                    4(1)+3(2) & 4(4)+3(3)
                    \end{bmatrix}$$

                    
                    $$= \begin{bmatrix} 5 & 10 \\ 10 & 25 \end{bmatrix}$$
                    
                    <p><strong>Step 3:</strong> Verify symmetry</p>
                    <ul style="margin-left: 20px;">
                        <li>Element (1,2) = 10, Element (2,1) = 10 ‚úì</li>
                        <li>Matrix equals its transpose ‚úì</li>
                    </ul>
                    
                    <p style="color: #2E7D32; font-weight: bold;">The product $C^T C$ is indeed symmetric! ‚úì</p>
                </div>
                
                <hr style="margin: 30px 0;">
                
                <p><strong>Question 6:</strong> Which of the following matrices are invertible based on their determinants?</p>
                <p>(a) $A = \begin{bmatrix} 2 & 4 \\ 1 & 2 \end{bmatrix}$ &nbsp;&nbsp;&nbsp; 
                   (b) $B = \begin{bmatrix} 3 & 1 \\ 6 & 4 \end{bmatrix}$</p>
                
                <div class="solution-box" style="margin-top: 15px;">
                    <h4>Solution:</h4>
                    
                    <p><strong>(a) Matrix A:</strong></p>

                    $$\det(A) = (2)(2) - (4)(1) = 4 - 4 = 0$$
                    <p style="color: #d32f2f;">Since $\det(A) = 0$, matrix A is <strong>NOT invertible</strong> (singular) ‚úó</p>
                    
                    <p><strong>(b) Matrix B:</strong></p>

                    $$\det(B) = (3)(4) - (1)(6) = 12 - 6 = 6$$
                    <p style="color: #2E7D32;">Since $\det(B) = 6 \neq 0$, matrix B <strong>IS invertible</strong> ‚úì</p>
                    
                    <p><strong>Conclusion:</strong> Only matrix B is invertible because it has a non-zero determinant.</p>
                </div>
            </div>
        </section>
        
        <!-- Mind Map Section -->
        <section id="mind-map">
            <div class="mind-map">
                <h2>7. üß† Comprehensive Mind Map</h2>
                
                <div class="mind-map-container">
                    <div class="central-topic">
                        Module 2.2: Symmetry, Determinants & Inverse
                    </div>
                    
                    <div class="branches">
                        <div class="branch" style="background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);">
                            <h4>üî≤ Symmetric Matrices</h4>
                            <ul>
                                <li>Definition: $A = A^T$</li>
                                <li>Square matrices only</li>
                                <li>Elements mirror across diagonal</li>
                                <li>$a_{ij} = a_{ji}$ for all $i, j$</li>
                                <li>Applications: PCA, covariance</li>
                                <li>Creating from non-symmetric: $A^T A$</li>
                                <li>Real eigenvalues property</li>
                            </ul>
                        </div>
                        
                        <div class="branch" style="background: linear-gradient(135deg, #30cfd0 0%, #330867 100%); color: white;">
                            <h4 style="color: white;">üî¢ Determinants</h4>
                            <ul style="color: white;">
                                <li>Scalar value from square matrices</li>
                                <li>Notation: $\det(A)$ or $|A|$</li>
                                <li>2√ó2: $ad - bc$</li>
                                <li>3√ó3: Cofactor expansion</li>
                                <li>$\det(A) \neq 0$ ‚Üí invertible</li>
                                <li>$\det(A) = 0$ ‚Üí singular</li>
                                <li>Use NumPy for large matrices</li>
                            </ul>
                        </div>
                        
                        <div class="branch" style="background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);">
                            <h4>üîÑ Matrix Inverse</h4>
                            <ul>
                                <li>Solves: $Ax = b \Rightarrow x = A^{-1}b$</li>
                                <li>Requires: $\det(A) \neq 0$</li>
                                <li>Computationally expensive</li>
                                <li>Alternatives: LU, QR, SVD</li>
                                <li>Best for small matrices</li>
                                <li>Applications: regression, modeling</li>
                                <li>Detailed coverage in Module 2.3</li>
                            </ul>
                        </div>
                        
                        <div class="branch" style="background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);">
                            <h4>üåç Applications</h4>
                            <ul>
                                <li>Image Processing (covariance)</li>
                                <li>Machine Learning (PCA)</li>
                                <li>Kernel Methods (SVMs)</li>
                                <li>Network Analysis (graphs)</li>
                                <li>Statistical Modeling</li>
                                <li>Data Engineering</li>
                                <li>Linear System Solving</li>
                            </ul>
                        </div>
                        
                        <div class="branch" style="background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);">
                            <h4>üíª Python Implementation</h4>
                            <ul>
                                <li>NumPy library essential</li>
                                <li>create_symmetric_matrix()</li>
                                <li>is_symmetric() checker</li>
                                <li>np.linalg.det() for determinants</li>
                                <li>Upper triangle iteration</li>
                                <li>Mirror assignment: (i,j) = (j,i)</li>
                                <li>Boolean output for checks</li>
                            </ul>
                        </div>
                        
                        <div class="branch" style="background: linear-gradient(135deg, #c471f5 0%, #fa71cd 100%); color: white;">
                            <h4 style="color: white;">üìê Key Properties</h4>
                            <ul style="color: white;">
                                <li>All symmetric ‚Üí square</li>
                                <li>Not all square ‚Üí symmetric</li>
                                <li>LIVE-EVIL transpose rule</li>
                                <li>$(A^T A)^T = A^T A$</li>
                                <li>Determinant recursive definition</li>
                                <li>Zero det = no inverse</li>
                                <li>Rank relates to independence</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <!-- Footer -->
        <footer style="margin-top: 50px; padding: 20px; background-color: #2c3e50; color: white; border-radius: 10px; text-align: center;">
            <h3 style= "color:#f8f9fa ">I created this knowledge during my first semester of BSc in Applied AI and Data Science.</h3>
            <h3 style= "color:#f8f9fa ">~ Armaan Kachhawa </h3>
        </footer>
    </div>
</body>
</html>