<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Foundation of AI - Comprehensive Lecture Notes</title>
    
    <!-- MathJax for rendering mathematical equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <style>
        /* ============================================
           GLOBAL STYLES & RESET
           ============================================ */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.8;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
        }
        
        /* ============================================
           MAIN CONTAINER
           ============================================ */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            padding: 40px;
        }
        
        /* ============================================
           HEADER SECTION
           ============================================ */
        header {
            text-align: center;
            padding: 30px 0;
            border-bottom: 4px solid #667eea;
            margin-bottom: 40px;
        }
        
        header h1 {
            font-size: 2.8em;
            color: #667eea;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
        }
        
        header p {
            font-size: 1.2em;
            color: #666;
            font-style: italic;
        }
        
        /* ============================================
           TABLE OF CONTENTS
           ============================================ */
        .toc {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 40px;
            border-left: 5px solid #667eea;
        }
        
        .toc h2 {
            color: #667eea;
            margin-bottom: 20px;
            font-size: 2em;
        }
        
        .toc ul {
            list-style: none;
        }
        
        .toc ul li {
            margin: 12px 0;
            padding-left: 20px;
            position: relative;
        }
        
        .toc ul li:before {
            content: "‚ñ∏";
            position: absolute;
            left: 0;
            color: #667eea;
            font-weight: bold;
        }
        
        .toc a {
            color: #333;
            text-decoration: none;
            font-size: 1.1em;
            transition: all 0.3s ease;
            display: inline-block;
        }
        
        .toc a:hover {
            color: #667eea;
            transform: translateX(5px);
        }
        
        /* ============================================
           SECTION STYLES
           ============================================ */
        section {
            margin: 50px 0;
            padding: 30px;
            background: #f9f9f9;
            border-radius: 10px;
            border-left: 5px solid #764ba2;
        }
        
        h2 {
            color: #764ba2;
            font-size: 2.2em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #764ba2;
        }
        
        h3 {
            color: #667eea;
            font-size: 1.8em;
            margin: 30px 0 15px 0;
        }
        
        h4 {
            color: #555;
            font-size: 1.4em;
            margin: 20px 0 10px 0;
        }
        
        /* ============================================
           TEXT ELEMENTS
           ============================================ */
        p {
            margin: 15px 0;
            text-align: justify;
        }
        
        strong {
            color: #764ba2;
            font-weight: 600;
        }
        
        em {
            color: #667eea;
            font-style: italic;
        }
        
        /* ============================================
           LISTS
           ============================================ */
        ul, ol {
            margin: 20px 0;
            padding-left: 40px;
        }
        
        li {
            margin: 10px 0;
        }
        
        /* ============================================
           DEFINITION BOX
           ============================================ */
        .definition {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 25px;
            border-radius: 10px;
            margin: 25px 0;
            border-left: 6px solid #ff6b6b;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .definition h4 {
            color: #c0392b;
            margin-bottom: 15px;
            font-size: 1.3em;
        }
        
        /* ============================================
           EXAMPLE BOX
           ============================================ */
        .example {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            padding: 25px;
            border-radius: 10px;
            margin: 25px 0;
            border-left: 6px solid #3498db;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .example h4 {
            color: #2980b9;
            margin-bottom: 15px;
            font-size: 1.3em;
        }
        
        /* ============================================
           PROFESSOR'S NOTE
           ============================================ */
        .professor-note {
            background: linear-gradient(135deg, #fff9e6 0%, #ffe5b4 100%);
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 6px solid #f39c12;
            font-style: italic;
        }
        
        .professor-note:before {
            content: "üë®‚Äçüè´ Professor mentioned in class: ";
            font-weight: bold;
            color: #e67e22;
            font-style: normal;
        }
        
        /* ============================================
           HINGLISH SUMMARY
           ============================================ */
        .hinglish-summary {
            background: linear-gradient(135deg, #e0f7fa 0%, #b2ebf2 100%);
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
            border-left: 6px solid #00bcd4;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .hinglish-summary h4 {
            color: #0097a7;
            margin-bottom: 15px;
            font-size: 1.3em;
        }
        
        .hinglish-summary p {
            color: #00695c;
            font-size: 1.05em;
            line-height: 1.7;
        }
        
        /* ============================================
           TABLE STYLES
           ============================================ */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            border-radius: 10px;
            overflow: hidden;
        }
        
        th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }
        
        td {
            padding: 12px 15px;
            border-bottom: 1px solid #ddd;
        }
        
        tr:nth-child(even) {
            background-color: #f8f9fa;
        }
        
        tr:hover {
            background-color: #e9ecef;
            transition: background-color 0.3s ease;
        }
        
        /* ============================================
           CODE & FORMULA BLOCKS
           ============================================ */
        .formula {
            background: #f4f4f4;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border: 2px solid #667eea;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
        }
        
        /* ============================================
           DIAGRAM PLACEHOLDER
           ============================================ */
        .diagram-placeholder {
            background: linear-gradient(135deg, #ffeaa7 0%, #fdcb6e 100%);
            padding: 60px;
            text-align: center;
            border-radius: 10px;
            margin: 25px 0;
            border: 3px dashed #e17055;
            color: #d63031;
            font-weight: bold;
            font-size: 1.2em;
        }
        
        /* ============================================
           KEY TAKEAWAYS
           ============================================ */
        .key-takeaways {
            background: linear-gradient(135deg, #c3ec52 0%, #0ba29d 100%);
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
            color: white;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .key-takeaways h4 {
            color: white;
            margin-bottom: 15px;
            font-size: 1.4em;
        }
        
        .key-takeaways ul {
            list-style: none;
            padding-left: 0;
        }
        
        .key-takeaways li {
            padding-left: 30px;
            position: relative;
            margin: 12px 0;
        }
        
        .key-takeaways li:before {
            content: "‚úì";
            position: absolute;
            left: 0;
            font-weight: bold;
            font-size: 1.3em;
        }
        
        /* ============================================
           PRACTICE QUESTIONS
           ============================================ */
        .practice-questions {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
            color: white;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .practice-questions h4 {
            color: white;
            margin-bottom: 20px;
            font-size: 1.4em;
        }
        
        .question {
            background: rgba(255,255,255,0.2);
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        
        .answer {
            background: rgba(255,255,255,0.3);
            padding: 15px;
            border-radius: 8px;
            margin-top: 10px;
            border-left: 4px solid white;
        }
        
        .answer:before {
            content: "Answer: ";
            font-weight: bold;
        }
        
        /* ============================================
           MIND MAP SECTION
           ============================================ */
        .mind-map {
            background: #f9f9f9;
            padding: 40px;
            border-radius: 15px;
            margin: 50px 0;
            border: 3px solid #667eea;
        }
        
        .mind-map h2 {
            text-align: center;
            color: #764ba2;
            margin-bottom: 40px;
        }
        
        .mind-map-container {
            display: flex;
            flex-direction: column;
            gap: 30px;
        }
        
        .main-topic {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            font-size: 1.5em;
            font-weight: bold;
            box-shadow: 0 6px 12px rgba(0,0,0,0.2);
        }
        
        .subtopics {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-left: 40px;
        }
        
        .subtopic {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            padding: 15px;
            border-radius: 8px;
            border-left: 5px solid #667eea;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        
        .subtopic h4 {
            color: #2980b9;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .subtopic ul {
            list-style: circle;
            padding-left: 20px;
            font-size: 0.95em;
        }
        
        /* ============================================
           FOOTER
           ============================================ */
        footer {
            text-align: center;
            padding: 30px 0;
            margin-top: 50px;
            border-top: 3px solid #667eea;
            color: #666;
        }
        
        /* ============================================
           RESPONSIVE DESIGN
           ============================================ */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            header h1 {
                font-size: 2em;
            }
            
            h2 {
                font-size: 1.8em;
            }
            
            h3 {
                font-size: 1.5em;
            }
            
            .subtopics {
                grid-template-columns: 1fr;
            }
        }
        
        /* ============================================
           SCROLL BEHAVIOR
           ============================================ */
        html {
            scroll-behavior: smooth;
        }
    </style>
</head>
<body>
    <!-- ============================================
         MAIN CONTAINER - Contains all lecture content
         ============================================ -->
    <div class="container">
        
        <!-- ============================================
             HEADER - Course title and information
             ============================================ -->
        <header>
            <h1>Foundation of AI</h1>
            <p>Created by Armaan Kachhawa </p>
            <p><em>Comprehensive Lecture Notes: Week 11</em></p>
        </header>
        
        <!-- ============================================
             TABLE OF CONTENTS - Navigation links
             ============================================ -->
        <div class="toc">
            <h2>üìö Table of Contents</h2>
            <ul>
                <li><a href="#random-variables">1. Introduction to Random Variables</a></li>
                <li><a href="#discrete-rv">2. Discrete Random Variables</a></li>
                <li><a href="#continuous-rv">3. Continuous Random Variables</a></li>
                <li><a href="#distributions-summary">4. Distributions Summary</a></li>
                <li><a href="#bayesian-networks">5. Bayesian Networks</a></li>
                <li><a href="#markov-chains">6. Markov Chains</a></li>
                <li><a href="#optimization">7. Introduction to Optimization</a></li>
                <li><a href="#hill-climbing">8. Hill Climbing Algorithm</a></li>
                <li><a href="#simulated-annealing">9. Simulated Annealing</a></li>
                <li><a href="#linear-programming">10. Linear Programming</a></li>
                <li><a href="#csp">11. Constraint Satisfaction Problems (CSP)</a></li>
                <li><a href="#backtracking">12. Backtracking Search</a></li>
                <li><a href="#game-theory">13. Introduction to Game Theory</a></li>
                <li><a href="#nash-equilibrium">14. Dominant Strategy & Nash Equilibrium</a></li>
                <li><a href="#multi-agent">15. Multi-Agent Planning</a></li>
                <li><a href="#ai-ethics">16. AI Ethics and Society</a></li>
                <li><a href="#mind-map">17. Comprehensive Mind Map</a></li>
            </ul>
        </div>
        
        <!-- ============================================
             SECTION 1: RANDOM VARIABLES
             ============================================ -->
        <section id="random-variables">
            <h2>1. Introduction to Random Variables</h2>
            
            <div class="definition">
                <h4>üìñ Definition: Random Variable</h4>
                <p>A <strong>random variable (RV)</strong> is a numerical quantity whose value depends on the outcome of a random experiment. It is a <em>deterministic function</em> that maps from the sample space \( \Omega \) to real numbers \( \mathbb{R} \).</p>
                <p class="formula">$$ X: \Omega \rightarrow \mathbb{R} $$</p>
                <p>Random variables are usually denoted by capital letters: \(X\), \(Y\), \(Z\).</p>
            </div>
            
            <div class="professor-note">
                There is nothing "random" about the random variable function itself‚Äîit is a deterministic function. The randomness comes from the randomness of the output values, which depend on the random experiment's outcome. Each outcome in the sample space is mapped to a specific number.
            </div>
            
            <div class="example">
                <h4>üí° Example 1: Coin Toss</h4>
                <p>Consider a simple coin toss experiment where the outcome is either Heads (H) or Tails (T).</p>
                <p>Let's define a random variable \(X\) that counts the number of heads:</p>
                <ul>
                    <li>\( X = 1 \) if we get Heads</li>
                    <li>\( X = 0 \) if we get Tails</li>
                </ul>
                <p>Sample space: \( \Omega = \{H, T\} \)</p>
                <p>The random variable function maps:</p>
                <ul>
                    <li>\( f(H) = 1 \)</li>
                    <li>\( f(T) = 0 \)</li>
                </ul>
                <p>Possible values: \( X \in \{0, 1\} \)</p>
            </div>
            
            <h3>Types of Random Variables</h3>
            
            <p>Random variables are classified into two main types based on the values they can take:</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Description</th>
                        <th>Examples</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Discrete</strong></td>
                        <td>Takes countable values (finite or countably infinite)</td>
                        <td>Coin tosses, number of goals in a match, dice rolls</td>
                    </tr>
                    <tr>
                        <td><strong>Continuous</strong></td>
                        <td>Takes any value in an interval (uncountable)</td>
                        <td>Time to finish a race, temperature, height</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="example">
                <h4>üí° Example 2: Discrete Random Variable</h4>
                <p><strong>Number of goals scored by a football team in a match</strong></p>
                <p>Let \(X\) be the number of goals. Then:</p>
                <p>\( X \in \{0, 1, 2, 3, \ldots\} \)</p>
                <p>The team can score 0 goals, 1 goal, 2 goals, and so on. These are discrete, countable values.</p>
            </div>
            
            <div class="example">
                <h4>üí° Example 3: Continuous Random Variable</h4>
                <p><strong>Time it takes a runner to finish a race</strong></p>
                <p>Let \(T\) be the time in seconds. Then:</p>
                <p>\( T \in [0, \infty) \)</p>
                <p>The time can take any value in a continuous interval (although practically, 0 time is not possible, but for mathematical completeness, we consider it).</p>
            </div>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Random variable</strong> ek aisa function hai jo sample space se real numbers tak mapping karta hai. Yeh khud random nahi hai, deterministic hai, lekin iske output values random experiment pe depend karti hain. <strong>Discrete RV</strong> countable values leta hai jaise dice roll, aur <strong>continuous RV</strong> ek interval mein koi bhi value le sakta hai jaise time ya temperature. Basically, random variable humein experiment ke outcomes ko numerical form mein represent karne mein help karta hai.</p>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>Random variables are deterministic functions that map outcomes to numbers</li>
                    <li>Denoted by capital letters (X, Y, Z)</li>
                    <li>Two types: Discrete (countable) and Continuous (any value in interval)</li>
                    <li>The randomness comes from the experiment, not the function itself</li>
                </ul>
            </div>
        </section>
        
        <!-- ============================================
             SECTION 2: DISCRETE RANDOM VARIABLES
             ============================================ -->
        <section id="discrete-rv">
            <h2>2. Discrete Random Variables</h2>
            
            <h3>Probability Mass Function (PMF)</h3>
            
            <div class="definition">
                <h4>üìñ Definition: Probability Mass Function</h4>
                <p>For a discrete random variable \(X\), the <strong>Probability Mass Function (PMF)</strong> is defined as:</p>
                <p class="formula">$$ p(x) = P(X = x) $$</p>
                <p>where \(x\) is a <em>realization</em> of the random variable (a specific value that \(X\) can take).</p>
            </div>
            
            <div class="professor-note">
                What is "realization"? When we say \(X = x\), we're asking about the probability that our random variable takes a specific value. For example, if we're rolling a die and \(X\) represents the outcome, then \(x = 3\) would be one realization‚Äîthe specific case where we rolled a 3.
            </div>
            
            <h4>Properties of PMF</h4>
            <ul>
                <li><strong>Non-negativity:</strong> \( p(x) \geq 0 \) for all \(x\)</li>
                <li><strong>Normalization:</strong> \( \sum_{x} p(x) = 1 \) (sum over all possible values)</li>
            </ul>
            
            <div class="example">
                <h4>üí° Example 4: Rolling a Fair Die</h4>
                <p>When rolling a fair six-sided die, the random variable \(X\) can take values:</p>
                <p>\( X \in \{1, 2, 3, 4, 5, 6\} \)</p>
                <p>The PMF is:</p>
                <p class="formula">$$ p(x) = \frac{1}{6} \quad \text{for } x = 1, 2, 3, 4, 5, 6 $$</p>
                <p>Verification:</p>
                <ul>
                    <li>\( P(X = 1) = \frac{1}{6} \)</li>
                    <li>\( P(X = 2) = \frac{1}{6} \)</li>
                    <li>... and so on</li>
                    <li>\( \sum_{i=1}^{6} p(i) = 6 \times \frac{1}{6} = 1 \) ‚úì</li>
                </ul>
            </div>
            
            <h3>Expected Value and Variance</h3>
            
            <div class="definition">
                <h4>üìñ Expected Value (Mean)</h4>
                <p>The <strong>expected value</strong> or <strong>mean</strong> of a discrete random variable \(X\) is:</p>
                <p class="formula">$$ E[X] = \mu = \sum_{x} x \cdot p(x) $$</p>
                <p>It represents the "average" value we expect to get from the random variable.</p>
            </div>
            
            <div class="definition">
                <h4>üìñ Variance</h4>
                <p>The <strong>variance</strong> measures the deviation from the mean:</p>
                <p class="formula">$$ \text{Var}(X) = E[(X - \mu)^2] = E[X^2] - (E[X])^2 $$</p>
                <p>It tells us how spread out the values are from the expected value.</p>
            </div>
            
            <div class="example">
                <h4>üí° Example 5: Expected Value and Variance for a Fair Die</h4>
                <p><strong>Expected Value:</strong></p>
                <p class="formula">

                    $$ E[X] = 1 \times \frac{1}{6} + 2 \times \frac{1}{6} + 3 \times \frac{1}{6} + 4 \times \frac{1}{6} + 5 \times \frac{1}{6} + 6 \times \frac{1}{6} $$

                    $$ = \frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \frac{21}{6} = 3.5 $$
                </p>
                
                <p><strong>Variance:</strong></p>
                <p>First, calculate \(E[X^2]\):</p>
                <p class="formula">

                    $$ E[X^2] = 1^2 \times \frac{1}{6} + 2^2 \times \frac{1}{6} + \cdots + 6^2 \times \frac{1}{6} $$

                    $$ = \frac{1 + 4 + 9 + 16 + 25 + 36}{6} = \frac{91}{6} $$
                </p>
                <p>Then:</p>
                <p class="formula">

                    $$ \text{Var}(X) = E[X^2] - (E[X])^2 = \frac{91}{6} - (3.5)^2 = \frac{91}{6} - \frac{49}{4} = \frac{35}{12} \approx 2.92 $$
                </p>
            </div>
            
            <h3>Common Discrete Distributions</h3>
            
            <h4>Bernoulli Distribution</h4>
            
            <div class="definition">
                <h4>üìñ Bernoulli Distribution</h4>
                <p>A <strong>Bernoulli distribution</strong> models a binary outcome (success/failure, yes/no, heads/tails).</p>
                <p>Notation: \( X \sim \text{Bernoulli}(p) \)</p>
                <p>PMF:</p>
                <ul>
                    <li>\( P(X = 1) = p \) (probability of success)</li>
                    <li>\( P(X = 0) = 1 - p \) (probability of failure)</li>
                </ul>
                <p><strong>Parameter:</strong> \(p\) (probability of success)</p>
                <p><strong>Mean:</strong> \( E[X] = p \)</p>
                <p><strong>Variance:</strong> \( \text{Var}(X) = p(1-p) \)</p>
            </div>
            
            <div class="example">
                <h4>üí° Example: Fair Coin Toss</h4>
                <p>For a fair coin:</p>
                <ul>
                    <li>\( p = 0.5 \) (probability of Heads)</li>
                    <li>\( P(X = H) = 0.5 \)</li>
                    <li>\( P(X = T) = 1 - 0.5 = 0.5 \)</li>
                </ul>
            </div>
            
            <h4>Binomial Distribution</h4>
            
            <div class="definition">
                <h4>üìñ Binomial Distribution</h4>
                <p>A <strong>Binomial distribution</strong> models the number of successes in \(n\) independent Bernoulli trials.</p>
                <p>Notation: \( X \sim \text{Binomial}(n, p) \)</p>
                <p>PMF:</p>
                <p class="formula">$$ P(X = k) = \binom{n}{k} p^k (1-p)^{n-k} $$</p>
                <p>where \( \binom{n}{k} = \frac{n!}{k!(n-k)!} \) is the binomial coefficient ("n choose k").</p>
                <p><strong>Parameters:</strong> \(n\) (number of trials), \(p\) (probability of success)</p>
                <p><strong>Mean:</strong> \( E[X] = np \)</p>
                <p><strong>Variance:</strong> \( \text{Var}(X) = np(1-p) \)</p>
            </div>
            
            <div class="professor-note">
                When we toss a coin \(n\) times, we're doing \(n\) independent Bernoulli trials. The binomial distribution tells us the probability of getting exactly \(k\) heads out of those \(n\) tosses. The possible values \(X\) can take are \(\{0, 1, 2, \ldots, n\}\)‚Äîyou can get 0 heads, 1 head, up to all \(n\) heads.
            </div>
            
            <div class="example">
                <h4>üí° Example 6: Flipping a Fair Coin 3 Times</h4>
                <p>Suppose we flip a fair coin 3 times. What is the probability of getting exactly 2 heads?</p>
                <p><strong>Given:</strong></p>
                <ul>
                    <li>\( n = 3 \) (number of tosses)</li>
                    <li>\( p = 0.5 \) (probability of heads on each toss)</li>
                    <li>\( k = 2 \) (we want exactly 2 heads)</li>
                </ul>
                <p><strong>Solution:</strong></p>
                <p class="formula">

                    $$ P(X = 2) = \binom{3}{2} (0.5)^2 (0.5)^{3-2} $$

                    $$ = 3 \times 0.25 \times 0.5 = 3 \times 0.125 = 0.375 $$
                </p>
                <p>So there's a 37.5% chance of getting exactly 2 heads in 3 coin flips.</p>
            </div>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Discrete random variables</strong> ke liye hum <strong>PMF (Probability Mass Function)</strong> use karte hain jo batata hai ki ek particular value ka probability kya hai. <strong>Expected value</strong> humein average outcome batata hai, aur <strong>variance</strong> batata hai ki values mean se kitni spread out hain. <strong>Bernoulli distribution</strong> ek binary outcome (success/failure) ko model karta hai with parameter \(p\), jabki <strong>Binomial distribution</strong> \(n\) independent Bernoulli trials mein successes ko count karta hai. Jaise agar hum 3 baar coin toss kare toh binomial distribution use karke hum calculate kar sakte hain ki exactly 2 heads aane ka probability kya hai.</p>
            </div>
            
            <div class="practice-questions">
                <h4>üìù Practice Questions</h4>
                
                <div class="question">
                    <p><strong>Q1:</strong> A biased coin has probability 0.6 of landing heads. If you flip it 5 times, what is the expected number of heads?</p>
                    <div class="answer">
                        For a binomial distribution \(X \sim \text{Binomial}(n=5, p=0.6)\): <br>
                        \( E[X] = np = 5 \times 0.6 = 3 \) heads
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>Q2:</strong> Calculate \(P(X = 3)\) for the same scenario (5 flips of a coin with \(p = 0.6\)).</p>
                    <div class="answer">
                        \( P(X = 3) = \binom{5}{3} (0.6)^3 (0.4)^2 = 10 \times 0.216 \times 0.16 = 0.3456 \)
                    </div>
                </div>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>PMF gives probability for each value: \(p(x) = P(X = x)\)</li>
                    <li>Expected value: \(E[X] = \sum x \cdot p(x)\) (weighted average)</li>
                    <li>Variance: \(\text{Var}(X) = E[X^2] - (E[X])^2\) (measures spread)</li>
                    <li>Bernoulli: Single binary trial with parameter \(p\)</li>
                    <li>Binomial: Number of successes in \(n\) independent trials</li>
                </ul>
            </div>
        </section>
        
        <!-- ============================================
             SECTION 3: CONTINUOUS RANDOM VARIABLES
             ============================================ -->
        <section id="continuous-rv">
            <h2>3. Continuous Random Variables</h2>
            
            <h3>Probability Density Function (PDF)</h3>
            
            <div class="definition">
                <h4>üìñ Definition: Probability Density Function</h4>
                <p>For a continuous random variable \(X\), we use a <strong>Probability Density Function (PDF)</strong> denoted by \(f(x)\).</p>
                <p>The probability that \(X\) falls in an interval \([a, b]\) is:</p>
                <p class="formula">$$ P(a \leq X \leq b) = \int_{a}^{b} f(x) \, dx $$</p>
                <p><strong>Important:</strong> For continuous RVs, \(P(X = x) = 0\) for any specific value \(x\). We can only calculate probabilities over intervals.</p>
            </div>
            
            <div class="professor-note">
                For continuous random variables, there's no point asking "What is the probability of getting exactly this value?" because there are infinitely many possible values. The probability of any single point is zero. Instead, we ask about intervals: "What's the probability that \(X\) is between \(a\) and \(b\)?" We integrate the PDF over that interval to find the probability.
            </div>
            
            <h4>Properties of PDF</h4>
            <ul>
                <li><strong>Non-negativity:</strong> \( f(x) \geq 0 \) for all \(x\)</li>
                <li><strong>Normalization:</strong> \( \int_{-\infty}^{\infty} f(x) \, dx = 1 \) (total area under curve is 1)</li>
            </ul>
            
            <div class="diagram-placeholder">
                [Insert diagram: Graph showing PDF curve with shaded area representing probability for interval [a,b]]
            </div>
            
            <h3>Uniform Distribution</h3>
            
            <div class="definition">
                <h4>üìñ Uniform Distribution</h4>
                <p>A <strong>continuous uniform distribution</strong> means all values in an interval \([a, b]\) are equally likely.</p>
                <p>Notation: \( X \sim \text{Uniform}(a, b) \)</p>
                <p>PDF:</p>
                <p class="formula">

                    $$ f(x) = \begin{cases} 
                    \frac{1}{b-a} & \text{if } a \leq x \leq b \\
                    0 & \text{otherwise}
                    \end{cases} $$
                </p>
                <p><strong>Mean:</strong> \( E[X] = \frac{a + b}{2} \)</p>
                <p><strong>Variance:</strong> \( \text{Var}(X) = \frac{(b-a)^2}{12} \)</p>
            </div>
            
            <div class="professor-note">
                In uniform distribution, the PDF is constant across the interval. Since every value is equally likely, the probability density is just \(\frac{1}{b-a}\), which ensures that when you integrate over the entire interval, you get 1 (the total probability).
            </div>
            
            <div class="example">
                <h4>üí° Example 7: Uniform Distribution on [2, 8]</h4>
                <p>Let \( X \sim \text{Uniform}(2, 8) \)</p>
                
                <p><strong>Mean:</strong></p>
                <p class="formula">$$ E[X] = \frac{2 + 8}{2} = 5 $$</p>
                
                <p><strong>Variance:</strong></p>
                <p class="formula">$$ \text{Var}(X) = \frac{(8-2)^2}{12} = \frac{36}{12} = 3 $$</p>
                
                <p><strong>Probability that \(3 \leq X \leq 5\):</strong></p>
                <p class="formula">

                    $$ P(3 \leq X \leq 5) = \int_{3}^{5} \frac{1}{6} dx = \frac{1}{6}(5-3) = \frac{2}{6} = \frac{1}{3} $$
                </p>
            </div>
            
            <h3>Normal (Gaussian) Distribution</h3>
            
            <div class="definition">
                <h4>üìñ Normal Distribution</h4>
                <p>The <strong>Normal distribution</strong> (also called <strong>Gaussian distribution</strong>) is one of the most important continuous distributions in statistics.</p>
                <p>Notation: \( X \sim N(\mu, \sigma^2) \)</p>
                <p>PDF:</p>
                <p class="formula">

                    $$ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$
                </p>
                <p><strong>Parameters:</strong></p>
                <ul>
                    <li>\(\mu\) = mean (center of distribution)</li>
                    <li>\(\sigma^2\) = variance (spread of distribution)</li>
                    <li>\(\sigma\) = standard deviation</li>
                </ul>
                <p><strong>Mean:</strong> \( E[X] = \mu \)</p>
                <p><strong>Variance:</strong> \( \text{Var}(X) = \sigma^2 \)</p>
            </div>
            
            <div class="professor-note">
                The normal distribution is extremely important‚Äîyou'll encounter it everywhere in probability, statistics, machine learning, and AI. It has a characteristic bell-shaped curve, symmetric around the mean. Many natural phenomena follow a normal distribution: heights, test scores, measurement errors, and much more.
            </div>
            
            <h4>Standard Normal Distribution</h4>
            
            <p>When \(\mu = 0\) and \(\sigma = 1\), we have the <strong>Standard Normal Distribution</strong>: \( Z \sim N(0, 1) \)</p>
            
            <p>We can convert any normal distribution to standard normal using the <strong>Z-transformation</strong>:</p>
            <p class="formula">$$ Z = \frac{X - \mu}{\sigma} $$</p>
            
            <div class="example">
                <h4>üí° Example 8: IQ Scores</h4>
                <p>Suppose IQ scores follow a normal distribution with mean \(\mu = 100\) and standard deviation \(\sigma = 15\). Find \(P(X < 115)\).</p>
                
                <p><strong>Step 1: Convert to standard normal</strong></p>
                <p class="formula">

                    $$ Z = \frac{115 - 100}{15} = \frac{15}{15} = 1 $$
                </p>
                
                <p><strong>Step 2: Look up in Z-table</strong></p>
                <p>From standard normal tables: \( P(Z < 1) = 0.8413 \)</p>
                
                <p><strong>Answer:</strong> There's approximately 84.13% probability that a randomly selected person has an IQ below 115.</p>
            </div>
            
            <div class="professor-note">
                When you have a normal distribution problem, you typically standardize it by transforming to \(Z\). This converts your distribution to \(N(0, 1)\), and then you can use standard Z-tables (or software) to find probabilities. Alternatively, you could directly integrate the PDF, but the Z-table method is much easier and is the standard approach.
            </div>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Continuous random variables</strong> ke liye hum <strong>PDF (Probability Density Function)</strong> use karte hain. Ek single point ka probability zero hota hai‚Äîhum sirf intervals ke liye probability calculate kar sakte hain by integrating PDF. <strong>Uniform distribution</strong> mein sabhi values equally likely hoti hain ek interval mein, aur PDF constant hota hai. <strong>Normal distribution</strong> sabse important distribution hai with bell-shaped curve, characterized by mean \(\mu\) aur variance \(\sigma^2\). Hum Z-transformation use karke kisi bhi normal distribution ko standard normal mein convert kar sakte hain probability calculations ke liye.</p>
            </div>
            
            <div class="practice-questions">
                <h4>üìù Practice Questions</h4>
                
                <div class="question">
                    <p><strong>Q1:</strong> For \( X \sim \text{Uniform}(0, 10) \), find \(P(X > 7)\).</p>
                    <div class="answer">
                        \( P(X > 7) = \int_{7}^{10} \frac{1}{10} dx = \frac{1}{10}(10-7) = 0.3 \) or 30%
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>Q2:</strong> If test scores follow \( N(70, 100) \) (mean=70, variance=100, so \(\sigma=10\)), what percentage of students score above 80?</p>
                    <div class="answer">
                        \( Z = \frac{80-70}{10} = 1 \). From Z-table, \(P(Z < 1) = 0.8413\). Therefore, \(P(Z > 1) = 1 - 0.8413 = 0.1587\) or about 15.87% of students.
                    </div>
                </div>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>PDF gives probability density; integrate over interval for probability</li>
                    <li>For continuous RVs, \(P(X = x) = 0\) for any specific \(x\)</li>
                    <li>Uniform distribution: Equal probability across interval \([a, b]\)</li>
                    <li>Normal distribution: Bell-shaped curve, most common in nature</li>
                    <li>Z-transformation: \(Z = \frac{X - \mu}{\sigma}\) standardizes any normal distribution</li>
                </ul>
            </div>
        </section>
        
        <!-- ============================================
             SECTION 4: DISTRIBUTIONS SUMMARY TABLE
             ============================================ -->
        <section id="distributions-summary">
            <h2>4. Summary of Common Distributions</h2>
            
            <table>
                <thead>
                    <tr>
                        <th>Distribution</th>
                        <th>Type</th>
                        <th>Parameters</th>
                        <th>Mean</th>
                        <th>Variance</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Bernoulli</strong></td>
                        <td>Discrete</td>
                        <td>\(p\)</td>
                        <td>\(p\)</td>
                        <td>\(p(1-p)\)</td>
                    </tr>
                    <tr>
                        <td><strong>Binomial</strong></td>
                        <td>Discrete</td>
                        <td>\(n, p\)</td>
                        <td>\(np\)</td>
                        <td>\(np(1-p)\)</td>
                    </tr>
                    <tr>
                        <td><strong>Uniform</strong></td>
                        <td>Continuous</td>
                        <td>\(a, b\)</td>
                        <td>\(\frac{a+b}{2}\)</td>
                        <td>\(\frac{(b-a)^2}{12}\)</td>
                    </tr>
                    <tr>
                        <td><strong>Normal (Gaussian)</strong></td>
                        <td>Continuous</td>
                        <td>\(\mu, \sigma^2\)</td>
                        <td>\(\mu\)</td>
                        <td>\(\sigma^2\)</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="professor-note">
                This summary table is crucial‚Äîit shows you at a glance the key characteristics of the distributions we've studied. For the normal distribution, the parameters \(\mu\) and \(\sigma^2\) actually define the distribution itself, which is why the mean equals \(\mu\) and the variance equals \(\sigma^2\). Notice that discrete distributions use \(p\) (probability) as a parameter, while continuous distributions use location and scale parameters.
            </div>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p>Yeh table humein saare important distributions ka quick reference deta hai. <strong>Discrete distributions</strong> (Bernoulli aur Binomial) countable outcomes model karte hain jaise coin tosses, jabki <strong>continuous distributions</strong> (Uniform aur Normal) intervals mein values model karte hain. Har distribution ke apne parameters hote hain jo distribution ke shape aur behavior ko define karte hain. Mean humein center batata hai aur variance spread ko represent karta hai.</p>
            </div>
        </section>
        
        <!-- ============================================
             SECTION 5: BAYESIAN NETWORKS
             ============================================ -->
        <section id="bayesian-networks">
            <h2>5. Bayesian Networks</h2>
            
            <div class="definition">
                <h4>üìñ Definition: Bayesian Network</h4>
                <p>A <strong>Bayesian Network (BN)</strong> is a <em>Directed Acyclic Graph (DAG)</em> where:</p>
                <ul>
                    <li>Each <strong>node</strong> represents a random variable</li>
                    <li>Each <strong>edge</strong> represents a conditional dependency</li>
                    <li>The <strong>joint probability</strong> is factored as a product of conditional probabilities</li>
                </ul>
                <p class="formula">

                    $$ P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i | \text{Parents}(X_i)) $$
                </p>
            </div>
            
            <h3>Understanding DAG (Directed Acyclic Graph)</h3>
            
            <div class="professor-note">
                What is a DAG? A <strong>Directed Acyclic Graph</strong> means:
                <ul>
                    <li><strong>Directed:</strong> All edges have a direction (arrows)</li>
                    <li><strong>Acyclic:</strong> No cycles‚Äîyou can't start at a node and follow edges to get back to the same node</li>
                </ul>
                For example, if you have 3 nodes forming a triangle with arrows going in a circle, that's NOT a DAG because it has a cycle. Bayesian Networks must be DAGs to represent proper causal or conditional relationships.
            </div>
            
            <h3>Key Concept: Conditional Independence</h3>
            
            <p>The special property of Bayesian Networks is that <strong>each variable depends only on its parents</strong>, not on all previous variables.</p>
            
            <p>In general, for \(n\) random variables, the chain rule gives us:</p>
            <p class="formula">

                $$ P(X_1, X_2, \ldots, X_n) = P(X_1) \cdot P(X_2|X_1) \cdot P(X_3|X_2, X_1) \cdots P(X_n|X_{n-1}, \ldots, X_1) $$
            </p>
            
            <p>But in a Bayesian Network, we simplify this because each variable only depends on its immediate parents:</p>
            <p class="formula">

                $$ P(X_3|X_2, X_1) = P(X_3|\text{Parents}(X_3)) $$
            </p>
            
            <div class="professor-note">
                This is a huge simplification! If \(X_3\) has only \(X_2\) as a parent (not \(X_1\)), then we can write \(P(X_3|X_2, X_1) = P(X_3|X_2)\). The variable \(X_3\) "forgets" about \(X_1\)‚Äîit only cares about its direct parent \(X_2\). This is the conditional independence property that makes Bayesian Networks so powerful and efficient.
            </div>
            
            <h3>Parent-Child Relationships</h3>
            
            <p>In a Bayesian Network:</p>
            <ul>
                <li><strong>Parent:</strong> A node from which an edge originates (points away)</li>
                <li><strong>Child:</strong> A node to which an edge points</li>
            </ul>
            
            <div class="diagram-placeholder">
                [Insert diagram: Simple Bayesian Network showing nodes A ‚Üí B ‚Üí C, with A as parent of B, B as parent of C]
            </div>
            
            <div class="example">
                <h4>üí° Example: Weather, Sprinkler, Grass Wet</h4>
                
                <p>Consider a scenario with three variables:</p>
                <ul>
                    <li><strong>W</strong>: Weather (Rain / No Rain)</li>
                    <li><strong>S</strong>: Sprinkler (On / Off)</li>
                    <li><strong>G</strong>: Grass Wet (Yes / No)</li>
                </ul>
                
                <p><strong>Causal relationships:</strong></p>
                <ul>
                    <li>Weather affects whether sprinkler is on: \(W \rightarrow S\)</li>
                    <li>Weather affects whether grass is wet: \(W \rightarrow G\)</li>
                    <li>Sprinkler affects whether grass is wet: \(S \rightarrow G\)</li>
                </ul>
                
                <div class="diagram-placeholder">
                    [Insert diagram: Bayesian Network with W at top, arrows to S and G, and S also has arrow to G]
                </div>
                
                <p><strong>Joint Probability Factorization:</strong></p>
                <p class="formula">

                    $$ P(W, S, G) = P(W) \cdot P(S|W) \cdot P(G|S, W) $$
                </p>
                
                <p><strong>Parents:</strong></p>
                <ul>
                    <li>\(W\) has no parents</li>
                    <li>Parents of \(S\): \(\{W\}\)</li>
                    <li>Parents of \(G\): \(\{W, S\}\)</li>
                </ul>
            </div>
            
            <h3>Numerical Example: Computing Probabilities</h3>
            
            <div class="example">
                <h4>üí° Computing P(G = wet)</h4>
                
                <p><strong>Given probabilities:</strong></p>
                <ul>
                    <li>\(P(W = \text{rain}) = 0.3\)</li>
                    <li>\(P(W = \text{no rain}) = 0.7\)</li>
                    <li>\(P(S = \text{on}|W = \text{rain}) = 0.1\)</li>
                    <li>\(P(S = \text{on}|W = \text{no rain}) = 0.5\)</li>
                    <li>\(P(G = \text{wet}|S = \text{on}, W = \text{rain}) = 0.99\)</li>
                    <li>\(P(G = \text{wet}|S = \text{off}, W = \text{rain}) = 0.8\)</li>
                    <li>\(P(G = \text{wet}|S = \text{on}, W = \text{no rain}) = 0.9\)</li>
                    <li>\(P(G = \text{wet}|S = \text{off}, W = \text{no rain}) = 0.0\)</li>
                </ul>
                
                <p><strong>To find:</strong> \(P(G = \text{wet})\)</p>
                
                <p><strong>Solution using marginalization:</strong></p>
                <p class="formula">

                    $$ P(G) = \sum_{W} \sum_{S} P(W) \cdot P(S|W) \cdot P(G|S, W) $$
                </p>
                
                <p><strong>Calculation:</strong></p>
                <p class="formula">

                    $$ P(G = \text{wet}) = P(W=R) [P(S=\text{on}|R) \cdot P(G=\text{wet}|S=\text{on}, R) + P(S=\text{off}|R) \cdot P(G=\text{wet}|S=\text{off}, R)] $$

                    $$ + P(W=\neg R) [P(S=\text{on}|\neg R) \cdot P(G=\text{wet}|S=\text{on}, \neg R) + P(S=\text{off}|\neg R) \cdot P(G=\text{wet}|S=\text{off}, \neg R)] $$
                </p>
                
                <p>Substituting values:</p>
                <p class="formula">

                    $$ = 0.3[(0.1)(0.99) + (0.9)(0.8)] + 0.7[(0.5)(0.9) + (0.5)(0.0)] $$

                    $$ = 0.3[0.099 + 0.72] + 0.7[0.45 + 0] $$

                    $$ = 0.3(0.819) + 0.7(0.45) $$

                    $$ = 0.2457 + 0.315 $$

                    $$ = 0.5607 \approx 0.56 $$
                </p>
                
                <p><strong>Note:</strong> The professor's calculation in the lecture gave 0.5823, which may use slightly different intermediate values. The methodology is what's important here.</p>
            </div>
            
            <div class="professor-note">
                When calculating probabilities in Bayesian Networks, we use the factorization formula and sum over all possible combinations of the other variables. This is called <em>marginalization</em>. We're essentially considering all possible scenarios (rain/no rain, sprinkler on/off) and weighing each by its probability. This is how we "infer" unknown probabilities from our knowledge base.
            </div>
            
            <h3>Inference in Bayesian Networks</h3>
            
            <div class="definition">
                <h4>üìñ Inference</h4>
                <p><strong>Inference</strong> in Bayesian Networks means computing the probability of unobserved variables given observed evidence.</p>
                <p>Example: Given that the grass is wet, what's the probability it rained?</p>
                <p class="formula">$$ P(W = \text{rain} | G = \text{wet}) $$</p>
            </div>
            
            <h4>Common Inference Methods:</h4>
            <ul>
                <li><strong>Variable Elimination:</strong> Systematically eliminate variables by summing them out</li>
                <li><strong>Belief Propagation:</strong> Message-passing algorithm for efficient computation</li>
                <li><strong>Monte Carlo Sampling:</strong> Use random sampling to approximate probabilities</li>
            </ul>
            
            <h3>Applications of Bayesian Networks</h3>
            
            <ul>
                <li><strong>Medical Diagnosis:</strong> Relating symptoms to diseases</li>
                <li><strong>Speech Recognition:</strong> Modeling relationships between acoustic signals and words</li>
                <li><strong>Risk Assessment:</strong> Financial risk modeling, insurance</li>
                <li><strong>AI Decision Systems:</strong> Expert systems, recommendation engines</li>
            </ul>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Bayesian Network</strong> ek directed acyclic graph (DAG) hai jismein har node ek random variable ko represent karta hai aur edges conditional dependencies ko show karte hain. Iska main advantage yeh hai ki <strong>har variable sirf apne parents pe depend karta hai</strong>, pure history pe nahi. Joint probability ko hum factorize kar sakte hain as product of conditional probabilities. For example, weather-sprinkler-grass example mein, grass being wet depends on whether it rained aur whether sprinkler was on. Hum <strong>inference</strong> use karke observed evidence se unknown variables ki probabilities calculate kar sakte hain. Medical diagnosis, speech recognition aur risk assessment mein Bayesian Networks ka wide application hai.</p>
            </div>
            
            <div class="practice-questions">
                <h4>üìù Practice Questions</h4>
                
                <div class="question">
                    <p><strong>Q1:</strong> In a Bayesian Network \(A \rightarrow B \rightarrow C\), what is the joint probability \(P(A, B, C)\)?</p>
                    <div class="answer">
                        \( P(A, B, C) = P(A) \cdot P(B|A) \cdot P(C|B) \) <br>
                        Note: \(C\) only depends on its parent \(B\), not on \(A\).
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>Q2:</strong> Why can't a Bayesian Network have cycles?</p>
                    <div class="answer">
                        Cycles would create circular dependencies, making it impossible to define a consistent probability distribution using the factorization formula. The acyclic property ensures we can compute probabilities in a well-defined order.
                    </div>
                </div>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>Bayesian Networks are DAGs representing probabilistic dependencies</li>
                    <li>Each node is a random variable; edges show conditional dependencies</li>
                    <li>Key property: Variables depend only on their parents (conditional independence)</li>
                    <li>Joint probability factors as: \(\prod P(X_i|\text{Parents}(X_i))\)</li>
                    <li>Used for inference: computing unknown probabilities from evidence</li>
                    <li>Applications: medical diagnosis, speech recognition, risk assessment</li>
                </ul>
            </div>
        </section>
        
        <!-- ============================================
             SECTION 6: MARKOV CHAINS
             ============================================ -->
        <section id="markov-chains">
            <h2>6. Markov Chains</h2>
            
            <div class="definition">
                <h4>üìñ Definition: Markov Chain</h4>
                <p>A <strong>Markov Chain</strong> is a stochastic process with the <strong>Markov Property</strong>:</p>
                <p class="formula">

                    $$ P(X_{n+1} = x_{n+1} | X_n = x_n, X_{n-1}, \ldots, X_0) = P(X_{n+1} = x_{n+1} | X_n = x_n) $$
                </p>
                <p><strong>The Markov Property:</strong> The next state depends <em>only</em> on the current state, not on the history of previous states.</p>
            </div>
            
            <div class="professor-note">
                Think of Markov Chains as a sequence of events occurring over time: \(X_0 \rightarrow X_1 \rightarrow X_2 \rightarrow X_3 \rightarrow \ldots\). When we're at state \(X_3\) and want to predict \(X_4\), we only need to know \(X_3\)‚Äîwe don't need to remember \(X_0, X_1, X_2\). This is why it's called "memoryless"‚Äîthe system has no memory of states before the current one. It's similar to Bayesian Networks where variables depend only on their parents, but here we're modeling a temporal sequence.
            </div>
            
            <h3>Transition Probability Matrix</h3>
            
            <p>Markov Chains are represented using a <strong>transition probability matrix</strong> \(P\), where:</p>
            <p class="formula">$$ P_{ij} = P(X_{t+1} = S_j | X_t = S_i) $$</p>
            <p>This gives the probability of transitioning from state \(S_i\) to state \(S_j\).</p>
            
            <h4>Properties of Transition Matrix:</h4>
            <ul>
                <li>Each row represents current state; each column represents next state</li>
                <li>All entries are probabilities: \(0 \leq P_{ij} \leq 1\)</li>
                <li>Each row sums to 1: \(\sum_j P_{ij} = 1\) (you must transition somewhere)</li>
            </ul>
            
            <div class="example">
                <h4>üí° Example: Weather Model</h4>
                
                <p>Consider a simple weather model with two states:</p>
                <ul>
                    <li>\(S_1\): Sunny</li>
                    <li>\(S_2\): Rainy</li>
                </ul>
                
                <p><strong>Transition Matrix:</strong></p>
                <p class="formula">

                    $$ P = \begin{bmatrix} 0.8 & 0.2 \\ 0.4 & 0.6 \end{bmatrix} $$
                </p>
                
                <table>
                    <thead>
                        <tr>
                            <th>Today ‚Üì / Tomorrow ‚Üí</th>
                            <th>Sunny</th>
                            <th>Rainy</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Sunny</strong></td>
                            <td>0.8</td>
                            <td>0.2</td>
                        </tr>
                        <tr>
                            <td><strong>Rainy</strong></td>
                            <td>0.4</td>
                            <td>0.6</td>
                        </tr>
                    </tbody>
                </table>
                
                <p><strong>Interpretation:</strong></p>
                <ul>
                    <li>\(P_{11} = 0.8\): If today is sunny, 80% chance tomorrow is sunny</li>
                    <li>\(P_{12} = 0.2\): If today is sunny, 20% chance tomorrow is rainy</li>
                    <li>\(P_{21} = 0.4\): If today is rainy, 40% chance tomorrow is sunny</li>
                    <li>\(P_{22} = 0.6\): If today is rainy, 60% chance tomorrow is rainy</li>
                </ul>
            </div>
            
            <div class="professor-note">
                When reading the transition matrix, think of rows as "today" and columns as "tomorrow." The entry \(P_{ij}\) tells you: "If I'm in state \(i\) today (row), what's the probability I'll be in state \(j\) tomorrow (column)?" Each row must sum to 1 because you must end up in some state tomorrow‚Äîthe probabilities of all possible next states must add up to 100%.
            </div>
            
            <h3>n-Step Transitions</h3>
            
            <p>What if we want to know the probability of states <em>n days</em> from now?</p>
            
            <p>The <strong>n-step transition matrix</strong> is simply:</p>
            <p class="formula">$$ P^{(n)} = P^n $$</p>
            <p>(Matrix \(P\) multiplied by itself \(n\) times)</p>
            
            <div class="example">
                <h4>üí° Example: Two Days Later</h4>
                <p>What's the probability of weather 2 days from now?</p>
                <p class="formula">

                    $$ P^{(2)} = P \times P = \begin{bmatrix} 0.8 & 0.2 \\ 0.4 & 0.6 \end{bmatrix} \times \begin{bmatrix} 0.8 & 0.2 \\ 0.4 & 0.6 \end{bmatrix} $$

                    $$ = \begin{bmatrix} 0.72 & 0.28 \\ 0.56 & 0.44 \end{bmatrix} $$
                </p>
                <p>So if today is sunny, there's a 72% chance it will be sunny in 2 days.</p>
            </div>
            
            <h3>Steady-State Distribution</h3>
            
            <div class="definition">
                <h4>üìñ Steady-State (Stationary Distribution)</h4>
                <p>A <strong>steady-state distribution</strong> \(\pi\) is a probability distribution that doesn't change over time:</p>
                <p class="formula">$$ \pi P = \pi $$</p>
                <p>with the constraint: \(\sum_i \pi_i = 1\)</p>
                <p>This represents the long-term behavior of the Markov Chain.</p>
            </div>
            
            <div class="example">
                <h4>üí° Finding Steady-State for Weather Model</h4>
                
                <p>Let \(\pi = [\pi_S, \pi_R]\) be the steady-state probabilities for Sunny and Rainy.</p>
                
                <p><strong>Equation:</strong></p>
                <p class="formula">

                    $$ [\pi_S, \pi_R] \begin{bmatrix} 0.8 & 0.2 \\ 0.4 & 0.6 \end{bmatrix} = [\pi_S, \pi_R] $$
                </p>
                
                <p><strong>This gives us:</strong></p>
                <ul>
                    <li>\(0.8\pi_S + 0.4\pi_R = \pi_S\)</li>
                    <li>\(0.2\pi_S + 0.6\pi_R = \pi_R\)</li>
                    <li>\(\pi_S + \pi_R = 1\) (normalization)</li>
                </ul>
                
                <p><strong>Simplifying first equation:</strong></p>
                <p class="formula">

                    $$ 0.4\pi_R = 0.2\pi_S $$

                    $$ \pi_R = 0.5\pi_S $$
                </p>
                
                <p><strong>Using normalization:</strong></p>
                <p class="formula">

                    $$ \pi_S + 0.5\pi_S = 1 $$

                    $$ 1.5\pi_S = 1 $$

                    $$ \pi_S = \frac{2}{3} \approx 0.667 $$

                    $$ \pi_R = \frac{1}{3} \approx 0.333 $$
                </p>
                
                <p><strong>Interpretation:</strong> In the long run, about 67% of days will be sunny and 33% will be rainy, regardless of the initial state.</p>
            </div>
            
            <div class="professor-note">
                The steady-state distribution is fascinating‚Äîno matter what state you start in, after many transitions, the probability distribution converges to \(\pi\). This is the equilibrium state of the system. It's like asking: "If I run this weather pattern for 100 years, what percentage of days will be sunny?" The answer is about 67%, regardless of whether year 1 started sunny or rainy.
            </div>
            
            <h3>Applications of Markov Chains</h3>
            
            <ul>
                <li><strong>PageRank Algorithm:</strong> Google's original web page ranking system uses Markov Chains to model random web surfing</li>
                <li><strong>Speech and Language Modeling:</strong> Predicting next word based on current word</li>
                <li><strong>Stock Market Modeling:</strong> Predicting market states (bull/bear markets)</li>
                <li><strong>Credit Risk:</strong> Modeling transitions between credit rating states</li>
                <li><strong>Game Theory:</strong> Modeling sequential decision processes</li>
                <li><strong>Queueing Systems:</strong> Modeling customer arrivals and service</li>
            </ul>
            
            <h3>Comparison: Bayesian Networks vs Markov Chains</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>Bayesian Network</th>
                        <th>Markov Chain</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Structure</strong></td>
                        <td>Directed Acyclic Graph (DAG)</td>
                        <td>Directed graph (can have cycles)</td>
                    </tr>
                    <tr>
                        <td><strong>Key Property</strong></td>
                        <td>Conditional independence (depends on parents)</td>
                        <td>Memoryless (depends only on current state)</td>
                    </tr>
                    <tr>
                        <td><strong>Time Dimension</strong></td>
                        <td>Not necessarily temporal</td>
                        <td>Sequential/temporal process</td>
                    </tr>
                    <tr>
                        <td><strong>Common Use</strong></td>
                        <td>Causal relationships, diagnosis</td>
                        <td>Sequential predictions, long-term behavior</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="professor-note">
                Both Bayesian Networks and Markov Chains model probabilistic dependencies, but in different ways. Bayesian Networks are great for modeling complex causal relationships among multiple variables (not necessarily in time). Markov Chains are specifically designed for modeling sequences over time where the future depends only on the present. Both can perform inference and prediction, making them powerful tools in probabilistic AI.
            </div>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Markov Chain</strong> ek sequential process hai jismein next state sirf current state pe depend karta hai, past states pe nahi. Ise <strong>memoryless property</strong> kehte hain kyunki system ko previous states ka koi memory nahi hai. Hum <strong>transition probability matrix</strong> use karte hain jo batata hai ek state se dusre state mein jane ki probability kya hai. \(P^n\) se hum \(n\) steps baad ki probabilities calculate kar sakte hain. <strong>Steady-state distribution</strong> long-term equilibrium ko represent karta hai jahan probabilities stable ho jaati hain. Applications include PageRank, speech modeling, aur financial risk assessment. Difference: Bayesian Networks general causal relationships model karte hain (DAG), jabki Markov Chains specifically temporal sequences handle karte hain with memoryless property.</p>
            </div>
            
            <div class="practice-questions">
                <h4>üìù Practice Questions</h4>
                
                <div class="question">
                    <p><strong>Q1:</strong> In a 2-state Markov Chain, if the transition matrix is \(P = \begin{bmatrix} 0.7 & 0.3 \\ 0.2 & 0.8 \end{bmatrix}\), verify that each row sums to 1.</p>
                    <div class="answer">
                        Row 1: \(0.7 + 0.3 = 1.0\) ‚úì<br>
                        Row 2: \(0.2 + 0.8 = 1.0\) ‚úì<br>
                        This confirms it's a valid transition matrix.
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>Q2:</strong> For the matrix in Q1, find the steady-state distribution \(\pi\).</p>
                    <div class="answer">
                        Let \(\pi = [\pi_1, \pi_2]\). Then:<br>
                        \(0.7\pi_1 + 0.2\pi_2 = \pi_1 \Rightarrow 0.2\pi_2 = 0.3\pi_1 \Rightarrow \pi_2 = 1.5\pi_1\)<br>
                        \(\pi_1 + \pi_2 = 1 \Rightarrow \pi_1 + 1.5\pi_1 = 1 \Rightarrow \pi_1 = 0.4, \pi_2 = 0.6\)<br>
                        Steady-state: \([0.4, 0.6]\)
                    </div>
                </div>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>Markov Chain: Sequential process where next state depends only on current state</li>
                    <li>Markov Property (memoryless): \(P(X_{n+1}|X_n, X_{n-1}, \ldots) = P(X_{n+1}|X_n)\)</li>
                    <li>Transition matrix \(P\): Rows sum to 1, entries are transition probabilities</li>
                    <li>n-step transitions: Compute using \(P^n\)</li>
                    <li>Steady-state: Long-term equilibrium where \(\pi P = \pi\)</li>
                    <li>Applications: PageRank, speech recognition, financial modeling</li>
                </ul>
            </div>
        </section>
        
        <!-- Due to length constraints, I'll continue with the remaining sections in the next part. The structure follows the same pattern for:
        - Section 7: Optimization
        - Section 8: Hill Climbing
        - Section 9: Simulated Annealing
        - Section 10: Linear Programming
        - Section 11: CSP
        - Section 12: Backtracking
        - Section 13: Game Theory
        - Section 14: Nash Equilibrium
        - Section 15: Multi-Agent Planning
        - Section 16: AI Ethics
        - Section 17: Mind Map
        
        Each follows the same formatting with definitions, examples, professor notes, Hinglish summaries, practice questions, and key takeaways. -->
        
        <!-- CONTINUING WITH REMAINING SECTIONS... -->
        
        <section id="optimization">
            <h2>7. Introduction to Optimization</h2>
            
            <div class="definition">
                <h4>üìñ Definition: Optimization</h4>
                <p><strong>Optimization</strong> is the process of finding the best solution from all feasible solutions, typically by minimizing or maximizing an objective function.</p>
                <p class="formula">

                    $$ \text{minimize/maximize } f(x) $$

                    $$ \text{subject to constraints} $$
                </p>
            </div>
            
            <div class="professor-note">
                Optimization is everywhere in AI! When we were doing DFS, BFS, or uniform cost search, we were already doing optimization‚Äîtrying to find the least-cost path. In machine learning, we optimize model parameters to maximize accuracy. In robotics, we optimize energy consumption. The general idea is: we have an objective function (what we want to optimize) and often some constraints (limits on what solutions are allowed).
            </div>
            
            <h3>Types of Optimization</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Characteristics</th>
                        <th>Examples</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Deterministic</strong></td>
                        <td>Follows a fixed, predictable pattern; no randomness</td>
                        <td>Linear Programming, Gradient Descent</td>
                    </tr>
                    <tr>
                        <td><strong>Stochastic</strong></td>
                        <td>Involves randomness; probabilistic decisions</td>
                        <td>Hill Climbing, Simulated Annealing, Genetic Algorithms</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Applications of Optimization in AI</h3>
            
            <ul>
                <li><strong>Path Finding:</strong> Finding shortest/lowest-cost routes</li>
                <li><strong>Machine Learning:</strong> Training models by minimizing loss functions</li>
                <li><strong>Resource Allocation:</strong> Distributing limited resources optimally</li>
                <li><strong>Scheduling:</strong> Optimizing task sequences and timings</li>
                <li><strong>Engineering Design:</strong> Minimizing cost/energy while meeting requirements</li>
            </ul>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Optimization</strong> ka matlab hai sabse best solution dhoondhna available solutions mein se. Hum ek objective function ko minimize ya maximize karte hain, usually kuch constraints ke sath. Do main types hain: <strong>deterministic</strong> (fixed pattern follow karta hai) aur <strong>stochastic</strong> (randomness involve karta hai). AI mein optimization har jagah use hota hai‚Äîpath finding se lekar machine learning model training tak. Basically, har baar jab hum "best" solution chahte hain, hum optimization techniques use karte hain.</p>
            </div>
        </section>
        
        <section id="hill-climbing">
            <h2>8. Hill Climbing Algorithm</h2>
            
            <div class="definition">
                <h4>üìñ Definition: Hill Climbing</h4>
                <p><strong>Hill Climbing</strong> is an iterative optimization algorithm that starts with an arbitrary solution and continuously moves toward better solutions by changing a single element at a time.</p>
                <p><strong>Intuition:</strong> Imagine climbing hills in a foggy landscape‚Äîalways take the step that goes upward. Stop when no neighbor has a higher value (local maximum reached).</p>
            </div>
            
            <h3>Algorithm Pseudocode</h3>
            
            <div class="formula">
                <strong>Hill Climbing Algorithm:</strong><br>
                1. Start with random initial solution S<br>
                2. Evaluate f(S) ‚Äî the objective function<br>
                3. Repeat until stopping condition:<br>
                &nbsp;&nbsp;&nbsp;- Select a neighbor S' of S<br>
                &nbsp;&nbsp;&nbsp;- If f(S') > f(S), move to S' (for maximization)<br>
                &nbsp;&nbsp;&nbsp;- Else, stop (local optimum reached)
            </div>
            
            <div class="diagram-placeholder">
                [Insert diagram: Hill climbing visualization showing local maxima, global maximum, and stuck points]
            </div>
            
            <div class="professor-note">
                Think about the graph showing objective function values (heights) versus state space. If you're at a current state and look at neighboring states, hill climbing always moves to the neighbor with the highest value (steepest ascent). The problem is: if you reach a local maximum (a peak that's not the highest), you stop there because all neighbors are lower. You never find the global maximum unless you happened to start on the right hill!
            </div>
            
            <h3>Problems with Basic Hill Climbing</h3>
            
            <ul>
                <li><strong>Local Maxima:</strong> Gets stuck at peaks that aren't the global maximum</li>
                <li><strong>Plateaus:</strong> Flat regions where all neighbors have same value‚Äîno direction to move</li>
                <li><strong>Ridges:</strong> Sequences of local maxima difficult to navigate</li>
            </ul>
            
            <h3>Variants of Hill Climbing</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Variant</th>
                        <th>Strategy</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Simple Hill Climbing</strong></td>
                        <td>Examines neighbors randomly until improvement found</td>
                    </tr>
                    <tr>
                        <td><strong>Steepest-Ascent</strong></td>
                        <td>Evaluates ALL neighbors and chooses the best one</td>
                    </tr>
                    <tr>
                        <td><strong>Stochastic</strong></td>
                        <td>Selects a random uphill move (not necessarily steepest)</td>
                    </tr>
                    <tr>
                        <td><strong>Random-Restart</strong></td>
                        <td>Runs multiple hill climbs from different random starts</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="example">
                <h4>üí° Example: Feature Selection in Machine Learning</h4>
                <p>Suppose you want to select the best subset of features for a ML model:</p>
                <ul>
                    <li><strong>State:</strong> A subset of features</li>
                    <li><strong>Objective function:</strong> Model accuracy with those features</li>
                    <li><strong>Neighbors:</strong> Subsets differing by adding/removing one feature</li>
                    <li><strong>Hill Climbing:</strong> Start with random features, iteratively add/remove features that improve accuracy</li>
                </ul>
            </div>
            
            <h3>Advantages and Disadvantages</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Advantages</th>
                        <th>Disadvantages</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Simple and easy to implement</td>
                        <td>Gets stuck in local optima</td>
                    </tr>
                    <tr>
                        <td>Works well for smooth, unimodal functions</td>
                        <td>Not effective for noisy/discontinuous landscapes</td>
                    </tr>
                    <tr>
                        <td>Fast convergence in good scenarios</td>
                        <td>No mechanism for escaping suboptimal regions</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Hill Climbing</strong> ek simple optimization technique hai jismein hum hamesha uphill direction mein move karte hain. Algorithm random solution se start karta hai aur iteratively better neighbors select karta hai jab tak local maximum reach nahi ho jata. Main problem yeh hai ki yeh <strong>local maxima</strong> mein stuck ho jata hai‚Äîagar aap galat hill pe start kare toh global maximum nahi mil payega. Solutions include <strong>random-restart</strong> (multiple starting points try karo) aur variants jaise stochastic hill climbing. Simple aur fast hai but guarantee nahi hai best solution milega.</p>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>Iterative algorithm: Always moves to better neighbor (steepest ascent)</li>
                    <li>Stops at local maxima when no neighbor is better</li>
                    <li>Fast and simple but no guarantee of global optimum</li>
                    <li>Variants help: random-restart, stochastic selection</li>
                    <li>Best for smooth, unimodal objective functions</li>
                </ul>
            </div>
        </section>
        
        <section id="simulated-annealing">
            <h2>9. Simulated Annealing</h2>
            
            <div class="definition">
                <h4>üìñ Definition: Simulated Annealing</h4>
                <p><strong>Simulated Annealing (SA)</strong> is a probabilistic optimization technique inspired by the process of metal annealing‚Äîheating and slowly cooling to reach a low-energy crystalline structure.</p>
                <p><strong>Key Idea:</strong> Occasionally accepts <em>worse</em> solutions to escape local optima. The probability of accepting worse moves decreases over time (temperature cooling).</p>
            </div>
            
            <div class="professor-note">
                The name comes from metallurgy! When you heat metal, atoms move freely (high energy, high "temperature"). As you slowly cool it, atoms settle into optimal positions (low energy). In optimization, "temperature" controls exploration: high T means we accept bad moves freely (explore widely), low T means we only accept improvements (exploit current region). This helps escape local optima!
            </div>
            
            <h3>Algorithm Pseudocode</h3>
            
            <div class="formula">
                <strong>Simulated Annealing Algorithm:</strong><br>
                1. Start with initial solution S and initial temperature T<br>
                2. Repeat until stopping condition:<br>
                &nbsp;&nbsp;&nbsp;- Generate neighbor S'<br>
                &nbsp;&nbsp;&nbsp;- Compute change: ŒîE = f(S') - f(S)<br>
                &nbsp;&nbsp;&nbsp;- If ŒîE > 0, accept S' (it's better)<br>
                &nbsp;&nbsp;&nbsp;- Else, accept S' with probability e^(ŒîE/T)<br>
                &nbsp;&nbsp;&nbsp;- Decrease temperature: T = Œ±¬∑T (0 < Œ± < 1)
            </div>
            
            <div class="professor-note">
                The crucial difference from hill climbing: When ŒîE < 0 (S' is worse), we don't immediately reject it! We accept it with probability \(e^{\Delta E/T}\). Since ŒîE is negative, this probability is less than 1. When T is high (early), this probability is relatively high‚Äîwe explore freely. As T decreases (cooling), the probability drops‚Äîwe become more selective. Eventually, at low T, we're basically doing hill climbing.
            </div>
            
            <h3>Cooling Schedule</h3>
            
            <p>Temperature T controls the exploration-exploitation tradeoff:</p>
            
            <ul>
                <li><strong>High T:</strong> Algorithm accepts worse solutions easily ‚Üí <em>Exploration</em></li>
                <li><strong>Low T:</strong> Algorithm focuses on improvement ‚Üí <em>Exploitation</em></li>
            </ul>
            
            <h4>Common Cooling Schedules:</h4>
            
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Formula</th>
                        <th>Characteristics</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Exponential</strong></td>
                        <td>\(T = T_0 \times \alpha^k\)</td>
                        <td>Most common; \(\alpha \approx 0.8-0.99\)</td>
                    </tr>
                    <tr>
                        <td><strong>Linear</strong></td>
                        <td>\(T = T_0 - k \times \beta\)</td>
                        <td>Simpler but less flexible</td>
                    </tr>
                    <tr>
                        <td><strong>Logarithmic</strong></td>
                        <td>\(T = \frac{T_0}{\log(k+1)}\)</td>
                        <td>Very slow cooling; theoretical guarantees</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="diagram-placeholder">
                [Insert diagram: Simulated annealing accepting worse solutions to escape local maxima]
            </div>
            
            <div class="example">
                <h4>üí° Example: Traveling Salesman Problem (TSP)</h4>
                <p>Find shortest route visiting all cities exactly once:</p>
                <ul>
                    <li><strong>State:</strong> A permutation of cities (route)</li>
                    <li><strong>Objective:</strong> Total distance (minimize)</li>
                    <li><strong>Neighbor:</strong> Swap two cities in the route</li>
                    <li><strong>SA Process:</strong> Start with random route, try swaps. Accept improvements always; accept worse routes with probability \(e^{-\Delta D/T}\) where ŒîD is distance increase. Cool temperature gradually.</li>
                </ul>
            </div>
            
            <h3>Comparison: Hill Climbing vs Simulated Annealing</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Hill Climbing</th>
                        <th>Simulated Annealing</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Exploration</strong></td>
                        <td>Low</td>
                        <td>High (probabilistic)</td>
                    </tr>
                    <tr>
                        <td><strong>Escapes Local Optima</strong></td>
                        <td>No</td>
                        <td>Yes</td>
                    </tr>
                    <tr>
                        <td><strong>Randomness</strong></td>
                        <td>Deterministic/Limited</td>
                        <td>Highly random</td>
                    </tr>
                    <tr>
                        <td><strong>Convergence Speed</strong></td>
                        <td>Fast</td>
                        <td>Slow</td>
                    </tr>
                    <tr>
                        <td><strong>Global Optimum Guarantee</strong></td>
                        <td>No</td>
                        <td>Theoretical (with slow cooling)</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Advantages and Disadvantages</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Advantages</th>
                        <th>Disadvantages</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Escapes local optima effectively</td>
                        <td>Requires tuning parameters (T‚ÇÄ, Œ±)</td>
                    </tr>
                    <tr>
                        <td>Works for complex, multimodal problems</td>
                        <td>Slow convergence if cooling too gradual</td>
                    </tr>
                    <tr>
                        <td>Handles discrete and continuous problems</td>
                        <td>No guarantee in finite time</td>
                    </tr>
                    <tr>
                        <td>Simple to implement</td>
                        <td>Performance depends heavily on cooling schedule</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Applications</h3>
            
            <ul>
                <li><strong>Traveling Salesman Problem (TSP)</strong></li>
                <li><strong>Circuit Design:</strong> VLSI layout optimization</li>
                <li><strong>Image Restoration:</strong> Noise reduction and pattern recognition</li>
                <li><strong>Scheduling Problems:</strong> Job shop scheduling</li>
                <li><strong>Neural Network Training:</strong> Avoiding local minima</li>
            </ul>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Simulated Annealing</strong> metal annealing process se inspired hai‚Äîmetal ko heat karke slowly cool karte hain for optimal structure. Optimization mein, yeh occasionally <strong>worse solutions</strong> ko accept karta hai to escape local optima. Acceptance probability \(e^{\Delta E/T}\) hai jahan T "temperature" hai. High temperature pe freely explore karta hai (bad moves accept karta hai), low temperature pe carefully exploit karta hai (mostly improvements accept karta hai). <strong>Cooling schedule</strong> se temperature gradually decrease hota hai. Hill climbing se better kyunki local optima se escape kar sakta hai, lekin slower hai aur parameter tuning chahiye. TSP, circuit design, aur image restoration jaise complex problems ke liye best hai.</p>
            </div>
            
            <div class="practice-questions">
                <h4>üìù Practice Questions</h4>
                
                <div class="question">
                    <p><strong>Q1:</strong> Why does simulated annealing accept worse solutions early on but not later?</p>
                    <div class="answer">
                        Early on, temperature T is high, so acceptance probability \(e^{\Delta E/T}\) is relatively high even for negative ŒîE. As T decreases (cooling), the probability drops, making the algorithm more selective. This allows early exploration and later exploitation.
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>Q2:</strong> If ŒîE = -5 and T = 10, what's the acceptance probability?</p>
                    <div class="answer">
                        \( P(\text{accept}) = e^{\Delta E/T} = e^{-5/10} = e^{-0.5} \approx 0.606 \) or about 61% chance of accepting the worse solution.
                    </div>
                </div>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>Probabilistic optimization inspired by metal annealing process</li>
                    <li>Accepts worse solutions with probability \(e^{\Delta E/T}\) to escape local optima</li>
                    <li>Temperature T controls exploration vs exploitation</li>
                    <li>Cooling schedule gradually decreases T over time</li>
                    <li>Better than hill climbing for complex, multimodal problems</li>
                    <li>Applications: TSP, circuit design, scheduling, image processing</li>
                </ul>
            </div>
        </section>
        
        <!-- Continuing with Linear Programming, CSP, Backtracking, Game Theory, Multi-Agent, Ethics, and Mind Map sections -->
        
        <section id="linear-programming">
            <h2>10. Linear Programming</h2>
            
            <div class="definition">
                <h4>üìñ Definition: Linear Programming (LP)</h4>
                <p><strong>Linear Programming</strong> is a mathematical technique for maximizing or minimizing a <em>linear</em> objective function subject to <em>linear</em> equality or inequality constraints.</p>
                <p class="formula">
                    <strong>Standard Form:</strong><br>
                    Maximize/Minimize: \( Z = c_1x_1 + c_2x_2 + \cdots + c_nx_n \)<br>
                    Subject to:<br>
                    \( a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \leq b_1 \)<br>
                    \( a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \leq b_2 \)<br>
                    \( \vdots \)<br>
                    \( x_i \geq 0 \quad \forall i \)
                </p>
            </div>
            
            <div class="professor-note">
                The key word here is "linear"‚Äîboth the objective function and all constraints must be linear. No squares, no exponentials, no products of variables. This linearity makes LP problems solvable efficiently using algorithms like the Simplex method. Linear programming is widely used because many real-world problems can be formulated as LP, or at least approximated as linear.
            </div>
            
            <div class="example">
                <h4>üí° Example: Resource Allocation Problem</h4>
                
                <p><strong>Problem:</strong> A company produces two products A and B using limited resources.</p>
                
                <p><strong>Objective:</strong> Maximize profit</p>
                <p class="formula">

                    $$ \text{Maximize } Z = 3x_1 + 5x_2 $$
                </p>
                
                <p>where:</p>
                <ul>
                    <li>\(x_1\) = quantity of product A</li>
                    <li>\(x_2\) = quantity of product B</li>
                    <li>Profit per unit A = $3, per unit B = $5</li>
                </ul>
                
                <p><strong>Constraints:</strong></p>
                <p class="formula">

                    $$ 2x_1 + x_2 \leq 100 \quad \text{(Resource 1)} $$

                    $$ x_1 + 3x_2 \leq 90 \quad \text{(Resource 2)} $$

                    $$ x_1, x_2 \geq 0 \quad \text{(Non-negativity)} $$
                </p>
            </div>
            
            <h3>Graphical Solution (2 Variables)</h3>
            
            <p>For problems with 2 variables, we can visualize the solution graphically:</p>
            
            <ol>
                <li><strong>Plot constraints:</strong> Each constraint is a line; feasible region is the intersection</li>
                <li><strong>Identify feasible region:</strong> The area satisfying all constraints</li>
                <li><strong>Find optimal vertex:</strong> The optimal solution lies at a corner point of the feasible region</li>
            </ol>
            
            <div class="diagram-placeholder">
                [Insert diagram: Graph showing constraint lines, feasible region (shaded), and optimal vertex]
            </div>
            
            <div class="professor-note">
                Why does the optimal solution lie at a vertex (corner point)? This is a fundamental property of linear programming. The feasible region is a convex polygon, and for a linear objective function, the maximum/minimum must occur at one of the vertices. This is why we only need to check the corner points‚Äîwe don't need to check interior points!
            </div>
            
            <h4>Finding the Optimal Solution:</h4>
            
            <p>For the example above, the corner points of the feasible region are:</p>
            <ul>
                <li>(0, 0): \(Z = 0\)</li>
                <li>(50, 0): \(Z = 3(50) = 150\)</li>
                <li>(30, 20): \(Z = 3(30) + 5(20) = 190\) ‚Üê Maximum!</li>
                <li>(0, 30): \(Z = 5(30) = 150\)</li>
            </ul>
            
            <p><strong>Optimal solution:</strong> Produce 30 units of A and 20 units of B for maximum profit of $190.</p>
            
            <h3>Solution Methods</h3>
            
            <ul>
                <li><strong>Simplex Method:</strong> Iterative algorithm that moves along vertices of feasible region</li>
                <li><strong>Interior Point Methods:</strong> Move through interior of feasible region</li>
                <li><strong>Graphical Method:</strong> Only for 2 variables; visualize solution</li>
            </ul>
            
            <h3>Applications of Linear Programming</h3>
            
            <ul>
                <li><strong>Supply Chain Optimization:</strong> Transportation, distribution, inventory</li>
                <li><strong>Financial Portfolio Optimization:</strong> Asset allocation</li>
                <li><strong>Energy Grid Load Balancing:</strong> Power distribution</li>
                <li><strong>Diet Planning:</strong> Nutrition optimization given cost constraints</li>
                <li><strong>AI Decision-Making:</strong> Resource allocation under constraints</li>
                <li><strong>Manufacturing:</strong> Production planning and scheduling</li>
            </ul>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Linear Programming</strong> ek deterministic optimization technique hai jahan objective function aur constraints dono <strong>linear</strong> hone chahiye. Goal hai ek linear function ko maximize ya minimize karna subject to linear constraints. <strong>Feasible region</strong> wo area hai jahan sabhi constraints satisfy hote hain, aur optimal solution hamesha ek <strong>corner point (vertex)</strong> pe milta hai. Graphical method 2 variables ke liye use hota hai; larger problems ke liye Simplex method use karte hain. Applications include supply chain, finance, energy systems, aur resource allocation. Linear constraint hone se problem efficiently solve ho jata hai.</p>
            </div>
            
            <div class="practice-questions">
                <h4>üìù Practice Questions</h4>
                
                <div class="question">
                    <p><strong>Q1:</strong> Why must the objective function be linear in LP?</p>
                    <div class="answer">
                        Linearity ensures the feasible region is a convex set and the objective function has a unique maximum/minimum at a vertex. This makes LP problems solvable efficiently using algorithms like Simplex. Non-linear objectives would require different (often more complex) methods.
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>Q2:</strong> Can LP handle "either-or" constraints like "either \(x_1 > 5\) or \(x_2 > 10\)"?</p>
                    <div class="answer">
                        No, standard LP cannot handle such disjunctive constraints directly. You would need Integer Programming (IP) or Mixed-Integer Linear Programming (MILP) with binary variables to model such constraints.
                    </div>
                </div>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>LP optimizes linear objective functions subject to linear constraints</li>
                    <li>Feasible region is the set of all solutions satisfying constraints</li>
                    <li>Optimal solution always at a vertex (corner point) of feasible region</li>
                    <li>Solved efficiently using Simplex or Interior Point methods</li>
                    <li>Applications: supply chain, finance, resource allocation, scheduling</li>
                    <li>Limitation: Can only model linear relationships</li>
                </ul>
            </div>
        </section>
        
        <!-- Due to the extensive length, I'll provide a condensed version of the remaining sections while maintaining all key content -->
        
        <section id="csp">
            <h2>11. Constraint Satisfaction Problems (CSP)</h2>
            
            <div class="definition">
                <h4>üìñ Definition: CSP</h4>
                <p>A <strong>Constraint Satisfaction Problem</strong> consists of:</p>
                <ul>
                    <li><strong>Variables:</strong> \(X = \{X_1, X_2, \ldots, X_n\}\)</li>
                    <li><strong>Domains:</strong> Each variable \(X_i\) has domain \(D_i\) of possible values</li>
                    <li><strong>Constraints:</strong> Rules specifying allowable combinations of values</li>
                </ul>
                <p><strong>Goal:</strong> Find an assignment of values to variables that satisfies all constraints.</p>
            </div>
            
            <div class="example">
                <h4>üí° Example: Map Coloring Problem</h4>
                <p><strong>Problem:</strong> Color regions of a map with 3 colors (Red, Green, Blue) such that no adjacent regions have the same color.</p>
                <ul>
                    <li><strong>Variables:</strong> Regions (WA, NT, Q, NSW, V, SA, T)</li>
                    <li><strong>Domains:</strong> {Red, Green, Blue} for each region</li>
                    <li><strong>Constraints:</strong> Adjacent regions ‚â† same color</li>
                </ul>
            </div>
            
            <div class="professor-note">
                CSP is about finding ANY solution that satisfies all constraints‚Äînot necessarily optimizing anything. This contrasts with Linear Programming where we optimize an objective. In map coloring, we just want a valid coloring; we don't care which specific colors are used as long as no adjacent regions match. The constraint "adjacent vertices cannot have the same color" is what defines the problem.
            </div>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>CSP</strong> mein humara goal hai variables ko values assign karna jo sabhi constraints satisfy kare. Optimization nahi hai focus‚Äîbas ek valid solution chahiye. Map coloring classic example hai: regions ko color karo such that adjacent regions ka color different ho. Variables hain regions, domain hai colors, aur constraint hai adjacency rule. CSP ko solve karne ke liye backtracking use karte hain.</p>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>CSP = Variables + Domains + Constraints</li>
                    <li>Goal: Find assignment satisfying ALL constraints</li>
                    <li>No optimization objective‚Äîjust find valid solution</li>
                    <li>Common example: Map coloring, Sudoku, scheduling</li>
                </ul>
            </div>
        </section>
        
        <section id="backtracking">
            <h2>12. Backtracking Search</h2>
            
            <div class="definition">
                <h4>üìñ Backtracking for CSP</h4>
                <p><strong>Backtracking</strong> is a depth-first search algorithm that incrementally builds candidates for the solution and abandons a candidate as soon as it violates any constraint.</p>
            </div>
            
            <div class="formula">
                <strong>Backtracking Algorithm:</strong><br>
                1. Choose an unassigned variable<br>
                2. Assign a value consistent with constraints<br>
                3. Recursively assign remaining variables<br>
                4. If no value is possible, backtrack (undo assignment and try different value)
            </div>
            
            <div class="professor-note">
                Backtracking is like depth-first search with constraint checking. We go deep (assign variables one by one), and if we hit a dead end (no valid value for current variable), we backtrack‚Äîundo the most recent assignment and try a different value. It's trial-and-error with intelligence: we immediately detect violations rather than completing the entire assignment first.
            </div>
            
            <h3>Improvements to Backtracking</h3>
            
            <ul>
                <li><strong>MRV (Minimum Remaining Values):</strong> Choose variable with fewest legal values</li>
                <li><strong>Degree Heuristic:</strong> Choose variable involved in most constraints</li>
                <li><strong>Least Constraining Value:</strong> Try value that rules out fewest choices for others</li>
                <li><strong>Forward Checking:</strong> Eliminate inconsistent values from domains after each assignment</li>
                <li><strong>Arc Consistency (AC-3):</strong> Enforce consistency among variables during search</li>
            </ul>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Backtracking</strong> ek depth-first search approach hai CSP solve karne ke liye. Hum variables ko ek-ek karke values assign karte hain. Agar kisi point pe koi valid value nahi milti (constraint violate ho rahi hai), toh hum <strong>backtrack</strong> karte hain‚Äîmatlab previous assignment ko undo karke different value try karte hain. Efficiency improve karne ke liye heuristics use karte hain jaise MRV (least legal values wala variable first choose karo) aur forward checking (invalid values ko early eliminate karo).</p>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>Backtracking = DFS + constraint checking</li>
                    <li>Abandon path immediately when constraint violated</li>
                    <li>Heuristics improve efficiency: MRV, degree heuristic, LCV</li>
                    <li>Forward checking and AC-3 reduce search space</li>
                </ul>
            </div>
        </section>
        
        <section id="game-theory">
            <h2>13. Introduction to Game Theory</h2>
            
            <div class="definition">
                <h4>üìñ Definition: Game Theory</h4>
                <p><strong>Game Theory</strong> is the study of mathematical models of strategic interactions among rational decision-makers.</p>
                <p><strong>Key Idea:</strong> Each player's payoff depends not only on their own actions but also on the actions of others.</p>
            </div>
            
            <h3>Elements of a Game</h3>
            
            <ul>
                <li><strong>Players:</strong> Decision-makers</li>
                <li><strong>Strategies:</strong> Actions each player can take</li>
                <li><strong>Payoffs:</strong> Rewards/costs for each combination of strategies</li>
            </ul>
            
            <div class="example">
                <h4>üí° Example: Prisoner's Dilemma</h4>
                <p>Two suspects are interrogated separately. Each can:</p>
                <ul>
                    <li><strong>Cooperate (C):</strong> Stay silent</li>
                    <li><strong>Defect (D):</strong> Betray the other</li>
                </ul>
                
                <table>
                    <thead>
                        <tr>
                            <th>Player 1 \ Player 2</th>
                            <th>Cooperate (C)</th>
                            <th>Defect (D)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Cooperate (C)</strong></td>
                            <td>(-1, -1)</td>
                            <td>(-3, 0)</td>
                        </tr>
                        <tr>
                            <td><strong>Defect (D)</strong></td>
                            <td>(0, -3)</td>
                            <td>(-2, -2)</td>
                        </tr>
                    </tbody>
                </table>
                
                <p>Numbers represent years in jail (lower is better).</p>
            </div>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Game Theory</strong> strategic interactions ko study karta hai jahan multiple players hote hain aur har player ka payoff not only apne actions pe but others ke actions pe bhi depend karta hai. Chess, poker jaise games se lekar economics aur politics tak game theory apply hota hai. <strong>Prisoner's Dilemma</strong> classic example hai jahan dono players ke liye betray karna dominant strategy hai even though cooperation se dono better off ho sakte hain. Yeh cooperation vs self-interest ka fundamental tension show karta hai.</p>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>Game theory models strategic interactions among rational players</li>
                    <li>Payoffs depend on all players' actions, not just one's own</li>
                    <li>Elements: Players, strategies, payoffs</li>
                    <li>Applications: Economics, politics, AI multi-agent systems</li>
                </ul>
            </div>
        </section>
        
        <section id="nash-equilibrium">
            <h2>14. Dominant Strategy & Nash Equilibrium</h2>
            
            <div class="definition">
                <h4>üìñ Dominant Strategy</h4>
                <p>A strategy is <strong>dominant</strong> if it provides a higher payoff than any other strategy, regardless of what others do.</p>
            </div>
            
            <div class="definition">
                <h4>üìñ Nash Equilibrium</h4>
                <p>A <strong>Nash Equilibrium</strong> is a set of strategies where no player can gain by unilaterally changing their strategy.</p>
                <p class="formula">

                    $$ u_i(s_i^*, s_{-i}^*) \geq u_i(s_i, s_{-i}^*) \quad \forall i, s_i $$
                </p>
                <p>Each player's strategy is the best response to the others.</p>
            </div>
            
            <div class="professor-note">
                In Prisoner's Dilemma, defection (D) is a dominant strategy for both players because no matter what the other does (cooperate or defect), defecting gives a better payoff. When both defect, we have (D, D) which is a Nash Equilibrium‚Äîneither can improve by unilaterally switching to cooperate. Interestingly, (C, C) would give both better outcomes (-1, -1) than (D, D) which gives (-2, -2), but it's not stable because each has incentive to defect!
            </div>
            
            <h3>Types of Games</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Description</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Zero-sum</strong></td>
                        <td>One player's gain = other's loss; \(u_1 + u_2 = 0\)</td>
                        <td>Chess, poker</td>
                    </tr>
                    <tr>
                        <td><strong>Non-zero-sum</strong></td>
                        <td>Players can both win or lose; \(u_1 + u_2 \neq 0\)</td>
                        <td>Prisoner's Dilemma, trade</td>
                    </tr>
                    <tr>
                        <td><strong>Cooperative</strong></td>
                        <td>Players can form binding agreements</td>
                        <td>Coalition formation</td>
                    </tr>
                    <tr>
                        <td><strong>Non-cooperative</strong></td>
                        <td>No binding agreements; independent action</td>
                        <td>Most competitive games</td>
                    </tr>
                </tbody>
            </table>
            
            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Dominant strategy</strong> wo strategy hai jo har situation mein best hai regardless of others ke actions. <strong>Nash Equilibrium</strong> wo state hai jahan koi bhi player unilaterally (akele) apni strategy change karke better off nahi ho sakta. Prisoner's Dilemma mein (D, D) Nash Equilibrium hai kyunki agar koi ek cooperate karta hai toh wo worse off ho jata hai. <strong>Zero-sum games</strong> mein ek ka gain dusre ka loss hai (chess), jabki <strong>non-zero-sum</strong> mein dono win ya lose kar sakte hain (trade negotiations).</p>
            </div>
            
            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>Dominant strategy: Best regardless of others' choices</li>
                    <li>Nash Equilibrium: No player benefits from unilateral change</li>
                    <li>Zero-sum: One's gain = other's loss</li>
                    <li>Non-zero-sum: Mutual benefit or loss possible</li>
                </ul>
            </div>
        </section>
        
        <section id="multi-agent">
            <h2>15. Multi-Agent Planning</h2>
            
            <div class="definition">
                <h4>üìñ Multi-Agent Planning</h4>
                <p><strong>Multi-Agent Planning (MAP)</strong> is the process of generating coordinated plans or strategies for multiple autonomous agents that interact in a shared environment.</p>
            </div>
            
            <h3>Key Components</h3>
            
            <ul>
                <li><strong>Agents:</strong> Autonomous entities with goals</li>
                <li><strong>Environment:</strong> Shared world where agents act</li>
                <li><strong>Actions:</strong> Operations each agent can perform</li>
                <li><strong>Goals:</strong> Desired outcomes</li>
                <li><strong>Communication:</strong> How agents share information</li>
            </ul>
            
            <h3>Types of Multi-Agent Planning</h3>
            
            <table>
                <thead>
                    <tr>
                        <th>Type</th>
                        <th>Description</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Cooperative</strong></td>
                        <td>Agents share common goal</td>
                        <td>Robots assembling product together</td>
                    </tr>
                    <tr>
                        <td><strong>Competitive</strong></td>
                        <td>Agents have conflicting objectives</td>
                        <td>AI agents playing games</td>
                        <!-- =========================================================
     SECTION 15  (CONTINUED)
     ========================================================= -->
                    <tr>
                        <td><strong>Mixed-Motive</strong></td>
                        <td>Cooperate on some tasks, compete on others</td>
                        <td>Companies collaborate on R&D but compete in sales</td>
                    </tr>
                </tbody>
            </table>

            <h3>Centralized vs Decentralized Planning</h3>
            <table>
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>Centralized</th>
                        <th>Decentralized</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Controller</td>
                        <td>Single global planner</td>
                        <td>Each agent plans locally</td>
                    </tr>
                    <tr>
                        <td>Knowledge</td>
                        <td>Full knowledge of all agents</td>
                        <td>Local knowledge + communication</td>
                    </tr>
                    <tr>
                        <td>Scalability</td>
                        <td>Poor (bottleneck)</td>
                        <td>Good (parallel planning)</td>
                    </tr>
                    <tr>
                        <td>Optimality</td>
                        <td>Easier to optimize globally</td>
                        <td>Often sub-optimal locally</td>
                    </tr>
                </tbody>
            </table>

            <h3>Coordination Strategies</h3>
            <ul>
                <li><strong>Task Allocation:</strong> Assign tasks based on agent capabilities</li>
                <li><strong>Plan Merging:</strong> Combine individually generated plans</li>
                <li><strong>Negotiation:</strong> Resolve conflicts through communication</li>
                <li><strong>Consensus:</strong> Reach agreement on shared variables</li>
            </ul>

            <h3>Applications & Challenges</h3>
            <ul>
                <li><strong>Applications:</strong> Swarm robotics, autonomous vehicles, smart grids, disaster-response drones</li>
                <li><strong>Challenges:</strong> Scalability, partial observability, communication limits, balancing cooperation vs competition</li>
            </ul>

            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>Multi-Agent Planning</strong> mein multiple autonomous agents ek shared environment mein interact karte hain. Agents cooperate bhi kar sakte hain (common goal) ya compete bhi (conflicting goals). <strong>Centralized</strong> approach mein ek master planner sab kuch decide karta hai, <strong>decentralized</strong> mein har agent local plan banata hai aur coordination communication se hoti hai. Task allocation, negotiation, aur consensus common coordination strategies hain. Applications include drone swarms, self-driving cars, aur disaster response. Scalability aur communication overhead major challenges hain.</p>
            </div>

            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>MAP = multiple agents + shared environment + coordination</li>
                    <li>Cooperative vs competitive vs mixed-motive</li>
                    <li>Centralized: global view, poor scalability</li>
                    <li>Decentralized: local plans, scalable, may be sub-optimal</li>
                </ul>
            </div>
        </section>

<!-- =========================================================
     SECTION 16  AI ETHICS
     ========================================================= -->
        <section id="ai-ethics">
            <h2>16. AI Ethics and Society</h2>

            <div class="definition">
                <h4>üìñ AI Ethics</h4>
                <p><strong>AI Ethics</strong> is the branch of applied ethics that studies how AI systems should behave and how their use affects individuals and society.</p>
            </div>

            <h3>Why Ethics in AI?</h3>
            <ul>
                <li>AI influences decisions in employment, credit, policing, healthcare</li>
                <li>Affects social interactions, information access, economic & political systems</li>
                <li>Need to ensure responsible, fair, transparent development & deployment</li>
            </ul>

            <h3>Common Ethical Principles (OECD, EU, UNESCO)</h3>
            <table>
                <thead>
                    <tr>
                        <th>Principle</th>
                        <th>Meaning</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Beneficence</strong></td>
                        <td>AI should promote human well-being</td>
                    </tr>
                    <tr>
                        <td><strong>Non-maleficence</strong></td>
                        <td>Do no harm</td>
                    </tr>
                    <tr>
                        <td><strong>Autonomy</strong></td>
                        <td>Respect human freedom & decision-making</td>
                    </tr>
                    <tr>
                        <td><strong>Justice</strong></td>
                        <td>Fairness, equity, inclusivity</td>
                    </tr>
                    <tr>
                        <td><strong>Explicability</strong></td>
                        <td>Transparency & accountability of AI decisions</td>
                    </tr>
                </tbody>
            </table>

            <h3>Key Ethical Issues</h3>
            <ul>
                <li><strong>Bias & Fairness:</strong> Biased data ‚Üí unfair outcomes (hiring, policing, credit)</li>
                <li><strong>Transparency:</strong> "Black-box" models hard to interpret (deep learning)</li>
                <li><strong>Privacy & Surveillance:</strong> Mass data collection, facial recognition</li>
                <li><strong>Autonomy & Control:</strong> Human oversight in autonomous systems (drones, trading)</li>
                <li><strong>Economic Impact:</strong> Job displacement, wealth concentration</li>
                <li><strong>Disinformation:</strong> Deepfakes, echo chambers, polarization</li>
            </ul>

            <div class="professor-note">
                Professor emphasized that ethics is not just about avoiding bad things‚Äîit's about actively designing systems that promote human well-being. For example, instead of just saying "don't be biased," we should use diverse datasets, conduct algorithmic audits, and involve affected communities in design. The goal is trustworthy AI that augments humans, not replaces them.
            </div>

            <h3>Responsible AI Principles</h3>
            <ul>
                <li><strong>Accountability:</strong> Clear responsibility for AI actions</li>
                <li><strong>Transparency:</strong> Interpretable & auditable decisions</li>
                <li><strong>Fairness:</strong> Avoid discrimination; ensure equity</li>
                <li><strong>Safety & Reliability:</strong> Robust under diverse conditions</li>
                <li><strong>Sustainability:</strong> Consider environmental costs of computation</li>
            </ul>

            <h3>Global Frameworks & Regulations</h3>
            <ul>
                <li><strong>EU AI Act (2024):</strong> Regulates high-risk AI systems</li>
                <li><strong>UNESCO Recommendations (2021):</strong> Global ethical guidelines</li>
                <li><strong>IEEE Ethically Aligned Design:</strong> Engineering standards</li>
            </ul>

            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p><strong>AI Ethics</strong> ensure karta hai ki AI systems human well-being promote kare, harm na kare, fair ho, aur transparent ho. Common principles hain beneficence, non-maleficence, autonomy, justice, aur explicability. Major issues include <strong>bias</strong> (biased data se unfair decisions), <strong>black-box transparency</strong>, <strong>privacy</strong> (mass surveillance), <strong>job displacement</strong>, aur <strong>disinformation</strong> (deepfakes). Solutions: diverse datasets, explainable AI (XAI), privacy-preserving techniques, human-in-the-loop design, aur international regulations jaise EU AI Act. Goal hai <strong>trustworthy AI</strong> jo humanity ko benefit de, harm na pahunchaye.</p>
            </div>

            <div class="practice-questions">
                <h4>üìù Practice Questions</h4>
                
                <div class="question">
                    <p><strong>Q1:</strong> Why is transparency important in AI systems?</p>
                    <div class="answer">
                        Transparency allows users and regulators to understand how decisions are made, enables accountability, helps detect bias, and builds trust. Without it, AI becomes a "black box" that can't be audited or corrected.
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>Q2:</strong> Give an example of AI bias and a possible solution.</p>
                    <div class="answer">
                        Example: Hiring algorithms trained on historical male-dominated data may favor male candidates. Solution: Use diverse and representative training data, perform regular bias audits, and include fairness constraints in model training.
                    </div>
                </div>
            </div>

            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>AI Ethics ensures responsible, fair, transparent AI development</li>
                    <li>Core principles: beneficence, non-maleficence, autonomy, justice, explicability</li>
                    <li>Major concerns: bias, transparency, privacy, job loss, disinformation</li>
                    <li>Solutions: diverse data, XAI, privacy tech, human oversight, regulations</li>
                    <li>Global frameworks emerging (EU AI Act, UNESCO guidelines)</li>
                </ul>
            </div>
        </section>

<!-- =========================================================
     SECTION 17  COMPREHENSIVE MIND MAP
     ========================================================= -->
        <section id="mind-map" class="mind-map">
            <h2>17. Comprehensive Mind Map</h2>
            <div class="mind-map-container">
                <!-- Central Node -->
                <div class="main-topic">Foundation of AI - Week 11</div>

                <!-- Level-1 Sub-topics -->
                <div class="subtopics">
                    <div class="subtopic">
                        <h4>üî¢ Random Variables & Distributions</h4>
                        <ul>
                            <li>Discrete vs Continuous</li>
                            <li>PMF / PDF</li>
                            <li>Expected Value & Variance</li>
                            <li>Bernoulli, Binomial</li>
                            <li>Uniform, Normal (Gaussian)</li>
                        </ul>
                    </div>

                    <div class="subtopic">
                        <h4>üï∏Ô∏è Probabilistic Models</h4>
                        <ul>
                            <li>Bayesian Networks (DAG)</li>
                            <li>Conditional Independence</li>
                            <li>Markov Chains</li>
                            <li>Memoryless Property</li>
                            <li>Steady-State Distribution</li>
                        </ul>
                    </div>

                    <div class="subtopic">
                        <h4>üîç Optimization Techniques</h4>
                        <ul>
                            <li>Hill Climbing</li>
                            <li>Simulated Annealing</li>
                            <li>Linear Programming</li>
                            <li>Constraint Satisfaction</li>
                            <li>Backtracking Search</li>
                        </ul>
                    </div>

                    <div class="subtopic">
                        <h4>üéÆ Game Theory & Agents</h4>
                        <ul>
                            <li>Players, Strategies, Payoffs</li>
                            <li>Dominant Strategy</li>
                            <li>Nash Equilibrium</li>
                            <li>Zero vs Non-zero-sum</li>
                            <li>Multi-Agent Planning</li>
                        </ul>
                    </div>

                    <div class="subtopic">
                        <h4>ü§ñ AI Ethics & Society</h4>
                        <ul>
                            <li>Beneficence & Non-maleficence</li>
                            <li>Fairness & Bias</li>
                            <li>Transparency (XAI)</li>
                            <li>Privacy & Surveillance</li>
                            <li>Global Regulations</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="hinglish-summary">
                <h4>üáÆüá≥ Hinglish Summary</h4>
                <p>Yeh mind map poora Week 11 ka overview deta hai. Center mein <strong>"Foundation of AI"</strong> hai aur 5 main branches hain: (1) Probability & distributions, (2) Probabilistic models (BN, Markov), (3) Optimization algorithms, (4) Game theory & multi-agent systems, (5) AI ethics. Har branch ke sub-topics clearly listed hain jisse aapko yaad rakhne mein easy hoga. Ek glance mein aapko sab kuch visualize ho jata hai aur concepts ke connections bhi clear ho jate hain.</p>
            </div>

            <div class="key-takeaways">
                <h4>üîë Key Takeaways</h4>
                <ul>
                    <li>Mind map provides visual summary of all major topics</li>
                    <li>Five pillars: Probability ‚Üí Models ‚Üí Optimization ‚Üí Games ‚Üí Ethics</li>
                    <li>Use for quick revision before exams or interviews</li>
                    <li>Helps identify connections between concepts</li>
                </ul>
            </div>
        </section>

<!-- =========================================================
     FOOTER
     ========================================================= -->
        <!-- Footer -->
        <div style="text-align: center; margin-top: 50px; padding: 30px; background: #f8f9fa; border-radius: 10px;">
            <p style="font-size: 1.1em; color: #6c757d;">
                <strong>End of Lecture Notes</strong><br>
                Probability and Statistics in AI<br>
            </p>
            
        <p>
            I created this knowledge during my Second semester of BSc in Applied
            AI and Data Science.
          </p>
          <p>~ Armaan Kachhawa</p>
   
        </div>

    </div> <!-- /.container -->
</body>
</html>